Paper ID,Order in Session,CMT ID,Paper Title,Abstract,Author Names,Author Emails,Remote,Demo,AwardNom,Primary Keywords,Secondary Keywords,Notes,,,
,,,,1. Human-Centered Robotics,,,,,,,,,,,
1,1,178,Follow my Advice: Assume-Guarantee Approach to Task Planning with Human in the Loop,"We focus on correct-by-design robot task planning from finite Linear Temporal Logic (LTLf) specifications with a human in the loop. Since provable guarantees are difficult to obtain unconditionally, we take an assume-guarantee perspective. Along with guarantees on the robot's task satisfaction, we compute the weakest sufficient assumptions on the human's behavior. We approach the problem via a stochastic game and leverage algorithmic synthesis of the weakest sufficient assumptions. We turn the assumptions into runtime advice to be communicated to the human. We conducted an online user study and showed that the robot is perceived as safer, more intelligent and more compliant with our approach than a robot giving more frequent advice corresponding to stronger assumptions. In addition, we show that our approach leads to less violations of the specification than not communicating with the participant at all.",Georg Schuppe (KTH)*; Ilaria Torre (KTH); Iolanda Leite (KTH); Jana Tumova (KTH Royal Institute of Technology),schuppe@kth.se; ilariat@kth.se; iolanda@kth.se; tumova@kth.se,In person,,,Formal Methods for Robotics,Human-Robot Interaction; Robot Planning,"Jana Tumova, KTH",,,Advise to humans
2,2,349,Autonomous Justification for Enabling Explainable Decision Support in Human-Robot Teaming,"Justification is an important facet of policy explanation, a process for describing the behavior of an autonomous system. In human-robot collaboration, an autonomous agent can attempt to justify distinctly important decisions by offering explanations as to why those decisions are right or reasonable, leveraging a snapshot of its internal reasoning to do so. Without sufficient insight into a robot's decision-making process, it becomes challenging for users to trust or comply with those important decisions, especially when they are viewed as confusing or contrary to the user's expectations (e.g., when decisions change as new information is introduced to the agent's decision-making process). In this work we characterize the benefits of justification within the context of decision-support during human-robot teaming (i.e., agents giving recommendations to human teammates). We introduce a formal framework using value of information theory to strategically time justifications during periods of misaligned expectations for greater effect. We also characterize four different types of counterfactual justification derived from established explainable AI literature and evaluate them against each other in a human-subjects study involving a collaborative, partially observable search task. Based on our findings, we present takeaways on the effective use of different types of justifications in human-robot teaming scenarios, to improve user compliance and decision-making by strategically influencing human teammate thinking patterns. Finally, we present an augmented reality system incorporating these findings into a real-world decision-support system for human-robot teaming.",Matthew Luebbers (University of Colorado Boulder)*; Aaquib Tabrez (University of Colorado Boulder); Kyler Ruvane (University of Colorado); Bradley Hayes (University of Colorado Boulder),malu5275@colorado.edu; MOHD.TABREZ@COLORADO.EDU; kyler.ruvane@colorado.edu; bradley.hayes@colorado.edu,In person,,,Human-Robot Interaction,,"Bradley Hayes, UC Boulder",,,Justification in human-robot teaming for search
3,3,125,Enabling Team of Teams: A Trust Inference and Propagation (TIP) Model in Multi-Human Multi-Robot Teams,"Trust has been identified as a central factor for effective human-robot teaming. Existing literature on trust modeling predominantly focuses on dyadic human-autonomy teams where one human agent interacts with one robot. There is little, if not no, research on trust modeling in teams consisting of multiple human agents and multiple robotic agents.

To fill this research gap, we present the trust inference and propagation (TIP) model for trust modeling in multi-human multi-robot teams. In a multi-human multi-robot team, we postulate that there exist two types of experiences that a human agent has with a robot: direct and indirect experiences. The TIP model presents a novel mathematical framework that explicitly accounts for both types of experiences. To evaluate the model, we conducted a human-subject experiment with 15 pairs of participants (${N=30}$). Each pair performed a search and detection task with two drones.  Results show that our TIP model successfully captured the underlying trust dynamics and significantly outperformed a baseline model. To the best of our knowledge, the TIP model is the first mathematical framework for computational trust modeling in multi-human multi-robot teams.",Yaohui Guo (University of Michigan); X. Jessie Yang (University of Michigan); Cong Shi (University of Michigan)*,yaohuig@umich.edu; xijyang@umich.edu; shicong@umich.edu,In person,,,Human-Robot Interaction,Cognitive Modeling & Knowledge Representation for Robots,Michigan,,,"Trust multi-robot, multi-human teams: search and detection"
4,4,207,Investigating the Impact of Experience on a User's Ability to Perform Hierarchical Abstraction,"The field of Learning from Demonstration enables end-users, who are not robotics experts, to shape robot behavior. However, using human demonstrations to teach robots to solve long-horizon problems by leveraging the hierarchical structure of the task is still an unsolved problem. Prior work has yet to show that human users can provide sufficient demonstrations in novel domains without showing the demonstrators explicit teaching strategies for each domain. In this work, we investigate whether non-expert demonstrators can generalize robot teaching strategies to provide necessary and sufficient demonstrations to robots zero-shot in novel domains. We find that increasing participant experience with providing demonstrations improves their demonstration's degree of sub-task abstraction (p<.001), teaching efficiency (p<.001), and sub-task redundancy (p<.05) in novel domains, allowing generalization in robot teaching. Our findings demonstrate for the first time that non-expert demonstrators can transfer knowledge from a series of training experiences to novel domains without the need for explicit instruction, such that they can provide necessary and sufficient demonstrations when programming robots to complete task and motion planning problems. ",Nina M Moorman (Georgia Institute of Technology)*; Nakul Gopalan (Arizona State University); Aman Singh (Georgia Institute of Technology); Erin Botti (Georgia Institute of Technology); Mariah Schrum (Georgia Institute of Technology); Chuxuan Yang (Georgia Institute of Technology); Lakshmi Seelam (Georgia Institute of Technology); Matthew Gombolay (Georgia Institute of Technology),ninamoorman@gatech.edu; nakul.gopalan@asu.edu; asingh609@gatech.edu; erin.botti@gatech.edu; mschrum3@gatech.edu; soyang@gatech.edu; lseelam3@gatech.edu; Matthew.Gombolay@cc.gatech.edu,In person,,,Human-Robot Interaction,Robot Learning,"Gombolay, Georgia Tech",,,Non-expert demonstrations for task and motion planning
5,5,323,Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment,"With the rapid growth of computing powers and recent advances in deep learning, we have witnessed impressive demonstrations of novel robot capabilities in research settings. Nonetheless, these learning systems exhibit brittle generalization and require excessive training data for practical tasks. To harness the capabilities of state-of-the-art robot learning models while embracing their imperfections, we present Sirius, a principled framework for humans and robots to collaborate through a division of work. In this framework, partially autonomous robots are tasked with handling a major portion of decision-making where they work reliably; meanwhile, human operators monitor the process and intervene in challenging situations. Such a human-robot team ensures safe deployments in complex tasks. Further, we introduce a new learning algorithm to improve the policy's performance on the data collected from the task executions. The core idea is re-weighing training samples with approximated human trust and optimizing the policies with weighted behavioral cloning. We evaluate Sirius in simulation and on real hardware, showing that Sirius consistently outperforms baselines over a collection of contact-rich manipulation tasks, achieving an 8% boost in simulation and 27% on real hardware than the state-of-the-art methods in policy success rate, with twice faster convergence and 85% memory size reduction. Videos and more details are available at https://ut-austin-rpl.github.io/sirius/",Huihan Liu (UT Austin)*; Soroush Nasiriany (UT Austin); Lance Zhang (UT Austin); Zhiyao Bao (UT Austin); Yuke Zhu (University of Texas - Austin),huihanl@utexas.edu; soroush@cs.utexas.edu; lzhang26@utexas.edu; zhiyaobao@utexas.edu; yukez@cs.utexas.edu,In person,,BEST PAPER,Robot Learning,Grasping & Manipulation,"Yuke Zhu, UT Austin",,,Human-in-the-loop learning in a semi-autonomous system
6,6,242,Robotic Table Tennis: A Case Study into a High Speed Learning System,"We present a deep-dive into a real-world robotic learning system that, in previous work, was shown to be capable of hundreds of table tennis rallies with a human and has the ability to precisely return the ball to desired targets. This system puts together a highly optimized perception subsystem, a high-speed low-latency robot controller, a simulation paradigm that can prevent damage in the real world and also train policies for zero-shot transfer, and automated real world environment resets that enable autonomous training and evaluation on physical robots. We complement a complete system description, including numerous design decisions that are typically not widely disseminated, with a collection of studies that clarify the importance of mitigating various sources of latency, accounting for training and deployment distribution shifts, robustness of the perception system, sensitivity to policy hyper-parameters, and choice of action space. A video demonstrating the components of the system and details of experimental results can be found at https://youtu.be/uFcnWjB42I0.",David B D'Ambrosio (Google Inc)*; Navdeep  Jaitly (Apple); Vikas Sindhwani (Google); Ken Oslund (Google); Peng Xu (Google Inc); Nevena Lazic (DeepMind); Anish Shankar (Google); Tianli Ding (Google); Jonathan Abelian (Google); Erwin Coumans (Nvidia); Gus Kouretas (Google); Thinh Nguyen (Google); Justin Boyd (Google); Atil Iscen (Google); Reza Mahjourian (Waymo); Vincent Vanhoucke (Google); Alex Bewley (Google); Yuheng Kuang (Google); Michael Ahn (Google); Deepali Jain (Google); Satoshi Kataoka (-); Omar E Cortes (FS Studio); Pierre Sermanet (Google); Corey Lynch (Google); Pannag R Sanketi (Google Inc.); Krzysztof Choromanski (Google Brain Robotics); Wenbo Gao (Waymo); Juhana Kangaspunta (Google); Krista Reymann (Google); Grace Vesom (Google); Sherry Q Moore (Google); Avi Singh (Google); Saminda W Abeyruwan (Google Inc); Laura Graesser (Google),ddambro@google.com; ndjaitly@gmail.com; sindhwani@google.com; kenoslund@google.com; pengxu@google.com; nevena@google.com; phinfinity@google.com; tding@google.com; jabelian@google.com; ecoumans@nvidia.com; kkouretas@google.com; nguyenthinh@google.com; justinboyd@google.com; atil@google.com; rezama@google.com; vanhoucke@google.com; bewley@google.com; yuheng@google.com; michaelahn@google.com; jaindeepali@google.com; satok16@gmail.com; ocortes@google.com; sermanet@google.com; coreylynch@google.com; psanketi@google.com; kchoro@google.com; wenbogao@google.com; juhana.kangaspunta@gmail.com; reymann@google.com; vesom@google.com; sherrym@google.com; singhavi@google.com; saminda@google.com; lauragraesser@google.com,In person,,BEST SYSTEM,Robot Learning,"Robot Perception, Sensors & Vision",Google,,,Robot learning in the real world
7,7,255,SAR: Generalization of Physiological Dexterity via Synergistic Action Representation,"Learning effective continuous control policies in high-dimensional systems, including musculoskeletal agents, remains a significant challenge. Over the course of biological evolution, organisms have developed robust mechanisms for overcoming this complexity to learn highly sophisticated strategies for motor control. What accounts for this robust behavioral flexibility? Modular control via muscle synergies, i.e. coordinated muscle co-contractions, is considered to be one putative mechanism that enables organisms to learn muscle control in a simplified and generalizable action space. Drawing inspiration from this evolved motor control strategy, we use a physiologically accurate hand model to investigate whether leveraging a Synergistic Action Representation (SAR) acquired from simpler manipulation tasks improves learning and generalization on more complex tasks. We find that SAR-exploiting policies trained on a complex, 100-object randomized reorientation task significantly outperformed (> 70% success) baseline approaches (< 20% success). Notably, SAR-exploiting policies were also found to zero-shot generalize to thousands of unseen objects with out-of-domain size variations, while policies that did not adopt SAR failed to generalize. SAR also enabled significantly improved transfer learning on real-world objects. Finally, using a robotic manipulation task set and a full-body humanoid locomotion task, we establish the generality of SAR on broader high-dimensional control problems, achieving SOTA performance with an order of magnitude improved sample efficiency. To the best of our knowledge, this investigation is the first of its kind to present an end-to-end pipeline for discovering synergies and using this representation to learn high dimensional continuous control across a wide diversity of tasks.",Cameron H Berg (Meta AI)*; Vittorio Caggiano (Meta AI); Vikash Kumar (Univ. of Washington),cameronberg@meta.com; caggiano@gmail.com; vikashplus@gmail.com,In person,,,Bioinspired Robots,Grasping & Manipulation; Robot Learning,"Kumar, Washington + Meta",,,representations for transfer learning of manipulation given physiological hand model
8,8,35,One Policy to Dress Them All: Learning to Dress People with Diverse Poses and Garments,"Robot-assisted dressing could benefit the lives of many people such as older adults and individuals with disabilities. Despite such potential, robot-assisted dressing remains a challenging task for robotics as it involves complex manipulation of deformable cloth in 3D space. Many prior works aim to solve the robot-assisted dressing task, but they make certain assumptions such as a fixed garment and a fixed arm pose that limit their ability to generalize. In this work, we develop a robot-assisted dressing system that is able to dress different garments on people with diverse poses from partial point cloud observations, based on a learned policy. We show that with proper design of the policy architecture and Q function, reinforcement learning (RL) can be used to learn effective policies with partial point cloud observations that work well for dressing diverse garments. We further leverage policy distillation to combine multiple policies trained on different ranges of human arm poses into a single policy that works over a wide range of different arm poses. We conduct comprehensive real-world evaluations of our system with 510 dressing trials in a human study with 17 participants with different arm poses and dressed garments. Our system is able to dress 86% of the length of the participants' arms on average. Videos can be found on our project webpage: https://sites.google.com/view/one-policy-dress.",Yufei Wang (Carnegie Mellon University)*; Zhanyi Sun (Carnegie Mellon University); Zackory Erickson (Carnegie Mellon University); David Held (CMU),yufeiw2@andrew.cmu.edu; zhanyis@andrew.cmu.edu; zerickso@andrew.cmu.edu; dheld@andrew.cmu.edu,HYBRID,,,"Assistive, Entertainment and Service Robots",Human-Robot Interaction; Robot Learning,"David Held, CMU",,,RL for dressing people
,,,,,,,,,,,,,,,
,,,,2. Learning Manipulation from Demonstration and Teleoperation,,,,,,,,,,,
9,1,17,Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations,"While imitation learning provides us with an efficient toolkit to train robots, learning skills that are robust to environment variations remains a significant challenge. Current approaches address this challenge by relying either on large amounts of demonstrations that span environment variations or on handcrafted reward functions that require state estimates. Both directions are not scalable to fast imitation. In this work, we present Fast Imitation of Skills from Humans (FISH), a new imitation learning approach that can learn robust visual skills with less than a minute of human demonstrations. Given a weak base-policy trained by offline imitation of demonstrations, FISH computes rewards that correspond to the “match” between the robot’s behavior and the demonstrations. These rewards are then used to adaptively update a residual policy that adds on to the base-policy. Across all tasks, FISH requires at most twenty minutes of interactive learning to imitate demonstrations on object configurations that were not seen in the demonstrations. Importantly, FISH is constructed to be versatile, which allows it to be used across robot morphologies (e.g. xArm, Allegro, Stretch) and camera configurations (e.g. third-person, eye-in-hand). Our experimental evaluations on 9 different tasks show that FISH achieves an average success rate of 93%, which is around 3.8× higher than prior state-of-the-art methods.",Siddhant Haldar (New York University)*; Jyothish Pari (NYU); Anant Rai (New York University); Lerrel Pinto (New York University),sh6474@nyu.edu; jp5981@nyu.edu; ar7420@nyu.edu; lerrel@cs.nyu.edu,In person,,BEST STUDENT,Robot Learning,Grasping & Manipulation,"Lerrel Pinto, NYU ",,,Learning from Demonstration
10,2,224,GenAug: Retargeting behaviors to unseen situations via Generative Augmentation,"Robot learning methods have the potential for widespread generalization across tasks, environments, and objects. However, these methods are severely limited by the amount of data that they are provided or are able to collect. Robots in the real world are likely to only be able to collect a small dataset, both in terms of data quantity and diversity. For robot learning to generalize, we must be able to leverage sources of data or priors beyond the robot’s own experience. In this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data, can serve as such a source of data. We show that despite these generative models being trained on largely non-robotics data, they can serve as effective ways to impart priors into the process of robot learning in the
real world in a way that enables widespread generalization. In particular, we show how pre-trained generative models for in- painting can serve as effective tools for semantically meaningful data augmentation. By leveraging these pre-trained models for generating appropriate “functional” data augmentations, we
propose a system GenAug that is able to significantly improve policy generalization. We apply GenAug to tabletop manipulation tasks, showing the ability to retarget behavior to novel scenarios, while only requiring marginal amounts of real-world data. We demonstrate the efficacy of this system on a number of object manipulation problems in the real world, showing a 40% improvement in generalization to novel scenes and objects.",Qiuyu Chen (University of Washington)*; Shosuke C Kiami (University of Washington); Abhishek Gupta (University of Washington); Vikash Kumar (Univ. of Washington),qiuyuchen14@gmail.com; kiami.sho@gmail.com; abhgupta@cs.washington.edu; vikashplus@gmail.com,In person,,BEST SYSTEM,"Robot Perception, Sensors & Vision",Robot Learning,"Gupta, Kumar, U Washington",,,Augmentation of human demonstrations for manipulation
11,3,19,Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets,"Enabling robots to learn novel visuomotor skills in a data-efficient manner remains an unsolved problem with myriad challenges. A popular paradigm for tackling this problem is through leveraging large unlabeled datasets that have many behaviors in them and then adapting a policy to a specific task using a small amount of task-specific human supervision (i.e. interventions or demonstrations). However, how best to leverage the narrow task-specific supervision and balance it with offline data remains an open question. Our key insight in this work is that task-specific data not only provides new data for an agent to train on but can also inform the type of prior data the agent should use for learning. Concretely, we propose a simple approach that uses a small amount of downstream expert interventions or demonstrations to selectively query relevant behaviors from an offline, unlabeled dataset (including many sub-optimal behaviors). The agent is then jointly trained on the expert and queried data. We observe that our method learns to query only the relevant transitions to the task, filtering out sub-optimal or task-irrelevant data. By doing so, it is able to learn more effectively from the mix of task-specific and offline data compared to naively mixing the data or only using the task-specific data. 
Furthermore, we find that our simple querying approach outperforms more complex goal-conditioned methods by 20% across simulated and real robotic manipulation tasks from images. ",Maximilian Du (Stanford); Suraj Nair (Stanford University)*; Dorsa Sadigh (Stanford); Chelsea Finn (Stanford),maximilianjdu@gmail.com; surajn@stanford.edu; dorsa@cs.stanford.edu; cbfinn@cs.stanford.edu,In person,,,Robot Learning,,"Dorsa, Chelsea, Stanford",,,Learning from few demonstrations
12,4,278,Structured World Models from Human Videos,"In this paper, we tackle the problem of learning complex, general behaviors directly in the real world. We propose an approach for robots to efficiently learn manipulation skills using only a handful of real-world interaction trajectories from many different settings. Inspired by the success of learning from large-scale datasets in the fields of computer vision and natural language, our belief is that in order to efficiently learn, a robot must be able to leverage internet-scale, human video data. Humans interact with the world in many interesting ways, which can allow a robot to not only build an understanding of useful actions and affordances but also how these actions affect the world for manipulation. Our approach builds a structured, human-centric action space grounded in visual affordances learned from human videos. Further, we train a world model on human videos and fine-tune on a small amount of robot interaction data without any task supervision. We show that this approach of affordance-space world models enables different robots to learn various manipulation skills in complex settings, in under 30 minutes of interaction. Videos can be found at https://human-world-model.github.io",Russell Mendonca (Carnegie Mellon University)*; Shikhar Bahl (Carnegie Mellon University ); Deepak Pathak (Carnegie Mellon University),rmendonc@andrew.cmu.edu; sbahl2@cs.cmu.edu; dpathak@cs.cmu.edu,In person,,,Robot Learning,"Robot Perception, Sensors & Vision","Deepak, CMU",,,Learning manipulation from human videos and then some interaction
13,5,60,PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection,"Large-scale data is an essential component of machine learning as demonstrated in recent advances in natural language processing and computer vision research. However, collecting large-scale robotic data is much more expensive and slower as each operator can control only a single robot at a time. To make this costly data collection process efficient and scalable, we propose Policy Assisted TeleOperation (PATO), a system which automates part of the demonstration collection process using a learned assistive policy. PATO autonomously executes repetitive behaviors in data collection and asks for human input only when it is uncertain about which subtask or behavior to execute. We conduct teleoperation user studies both with a real robot and a simulated robot fleet and demonstrate that our assisted teleoperation system reduces human operators' mental load while improving data collection efficiency. Further, it enables a single operator to control multiple robots in parallel, which is a first step towards scalable robotic data collection. For code and video results, see https://clvrai.com/pato","Shivin Dass (University of Southern California); Karl Pertsch (University of Southern California)*; Hejia Zhang (University of Southern California); Youngwoon Lee (University of California, Berkeley); Joseph J Lim (University of Southern California); Stefanos Nikolaidis (University of Southern California)",sdass@usc.edu; pertsch@usc.edu; hejiazha@usc.edu; youngwoon@berkeley.edu; joe.lim@kaist.ac.kr; nikolaid@usc.edu,In person,,,Robot Learning,,"Stefanos Nikolaidis, USC",,,Teleoperation for demonstration
14,6,168,To the Noise and Back: Diffusion for Shared Autonomy,"Shared autonomy is an operational concept in which a user and an autonomous agent collaboratively control a robotic system. It provides a number of advantages over the extremes of full-teleoperation and full-autonomy in many settings. Traditional approaches to shared autonomy rely on knowledge of the environment dynamics, a discrete space of user goals that is known a priori, or knowledge of the user's policy -- assumptions that are unrealistic in many domains.
Recent works relax some of these assumptions by formulating shared autonomy with model-free deep reinforcement learning (RL).
In particular, they no longer need knowledge of the goal space (e.g., that the goals are discrete or constrained) or environment dynamics. However, they need knowledge of a task-specific reward function to train the policy. Unfortunately, such reward specification can be a difficult and brittle process. On top of that, the formulations inherently rely on human-in-the-loop training, and that necessitates them to prepare a policy that mimics users' behavior.
In this paper, we present a new approach to shared autonomy that employs a modulation of the forward and reverse diffusion process of diffusion models. Our approach does not assume known environment dynamics or the space of user goals, and in contrast to previous work, it does not require any reward feedback, nor does it require access to the user's policy during training. Instead, our framework learns a distribution over a space of desired behaviors. It then employs a diffusion model to translate the user's actions to a sample from this distribution. Crucially, we show that it is possible to carry out this process in a manner that preserves the user's control authority. We evaluate our framework on a series of challenging continuous control tasks, and analyze its ability to effectively correct user actions while maintaining their autonomy.","Takuma Yoneda (Toyota Technological Institute at Chicago)*; Luzhe Sun (University of Chicago, Computer Science Department ); Ge Yang (University of Chicago); Bradly  C Stadie (Vector Institute); Matthew R Walter (Toyota Technological Institute at Chicago)",takuma@ttic.edu; luzhesun@uchicago.edu; ge.ike.yang@gmail.com; stadiebradly@gmail.com; mwalter@ttic.edu,In person,,,Robot Learning,Human-Robot Interaction; Robot Modeling & Simulation,"Walter, TTI",,,RL for shared autonomy in teleoperation
15,7,51,AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System,"Vision-based teleoperation offers the possibility to endow robots with human-level intelligence to physically interact with the environment, while only requiring low-cost camera sensors. However, current vision-based teleoperation systems are designed and engineered towards a particular robot model and deploy environment, which scales poorly as the pool of the robot models expanded and the variety of the operating environment increases. In this paper, we propose AnyTeleop, a unified and general teleoperation system to support multiple different arms, hands, realities, and camera configurations within a single system. Although being designed to provide great flexibility to the choice of simulators and real hardware, our system can still achieve great performance. For real-world experiments, AnyTeleop
can outperform a previous system that was designed for the specific robot hardware with a higher success rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to better imitation learning performance, compared with a previous system that is particularly designed for that simulator.","Yuzhe Qin (University of California San Diego)*; Wei Yang (NVIDIA); Binghao Huang (University of California, San Diego); Karl Van Wyk (NVIDIA); Hao Su (UCSD); Xiaolong Wang (UCSD); Yu-Wei Chao (NVIDIA); Dieter Fox (NVIDIA Research / University of Washington)",y1qin@eng.ucsd.edu; platero.yang@gmail.com; qazploki@163.com; kvanwyk@nvidia.com; haosu@eng.ucsd.edu; xiw012@ucsd.edu; ywchao@umich.edu; fox@cs.washington.edu,In person,,,Grasping & Manipulation,Human-Robot Interaction; Robot Learning,"Xiaolong Wang, UCSD - Dieter Fox, NVIDIA",,,Vision system for teleoperation
16,8,290,Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware,"Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: the error of the policy can compound over time, drifting out of the training distribution. To address this challenge, we develop a simple yet novel algorithm Action Chunking with Transformers (ACT) which reduces the effective horizon by predicting actions in chunks. This allows us to learn difficult tasks such as opening a translucent condiment cup and slotting a battery with 80-90% success, with only 10 minutes worth of demonstration data. Project website: https://tonyzhaozh.github.io/aloha/",Tony Z. Zhao (Stanford University)*; Vikash Kumar (Univ. of Washington); Sergey Levine (UC Berkeley); Chelsea Finn (Stanford),tonyzhao@stanford.edu; vikashplus@gmail.com; svlevine@eecs.berkeley.edu; cbfinn@cs.stanford.edu,HYBRID,,,Robot Learning,Grasping & Manipulation,"Kumar, Washington + Finn, Levine, Berkeley",,,Low cost teleoperation for imitation learning
,,,,,,,,,,,,,,,
,,,,3. Self-supervised and Reinrforcement Learning for Manipulation,,,,,,,,,,,
17,1,22,Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction,"We introduce a novel robotic system for improving unseen object instance segmentation in the real world by leveraging long-term robot interaction with objects. Previous approaches either grasp or push an object and then obtain the segmentation mask of the grasped or pushed object after one action. Instead, our system defers the decision on segmenting objects after a sequence of robot pushing actions. By applying multi-object tracking and video object segmentation on the images collected via robot pushing, our system can generate segmentation masks of all the objects in these images in a self-supervised way. These include images where objects are very close to each other, and segmentation errors usually occur on these images for existing object segmentation networks. We demonstrate the usefulness of our system by fine-tuning segmentation networks trained on synthetic data with real-world data collected by our system. We show that, after fine-tuning, the segmentation accuracy of the networks is significantly improved both in the same domain and across different domains. In addition, we verify that the fine-tuned networks improve top-down robotic grasping of unseen objects in the real world.",Yangxiao Lu (University of Texas at Dallas)*; Ninad A Khargonkar (The University of Texas at Dallas); Zesheng Xu (The University of Texas at Dallas); Charles Averill (University of Texas at Dallas); Kamalesh Palanisamy (The University of Texas at Dallas); Kaiyu Hang (Rice University); Yunhui Guo (University of Texas at Dallas); Nicholas Ruozzi (UT Dallas); Yu Xiang (The University of Texas at Dallas),yangxiao.lu@utdallas.edu; NinadArun.Khargonkar@UTDallas.edu; Zesheng.Xu@utdallas.edu; charles@utdallas.edu; Kamalesh.Palanisamy@utdallas.edu; kaiyu.hang@rice.edu; Yunhui.Guo@UTDallas.edu; nicholas.ruozzi@utdallas.edu; yu.xiang@utdallas.edu,In person,,,"Robot Perception, Sensors & Vision",Robot Learning,UT Dallas,,,Self-supervised segmentation labeling via manipulation and tracking
18,2,298,Self-Supervised Visuo-Tactile Pretraining to Locate and Follow Garment Features,"Humans make extensive use of vision and touch as complementary senses, with vision providing global information about the scene and touch measuring local information during manipulation without suffering from occlusions. While prior work demonstrates the efficacy of tactile sensing for precise manipulation of deformables, they typically rely on supervised, human-labeled datasets. We propose Self-Supervised Visuo-Tactile Pretraining (SSVTP), a framework for learning multi-task visuo-tactile representations in a self-supervised manner through cross-modal supervision. We design a mechanism that enables a robot to autonomously collect precisely spatially-aligned visual and tactile image pairs, then train visual and tactile encoders to embed these pairs into a shared latent space using cross- modal contrastive loss. We apply this latent space to downstream perception and control of deformable garments on flat surfaces, and evaluate the flexibility of the learned representations without fine-tuning on 5 tasks: feature classification, contact localization, anomaly detection, feature search from a visual query (e.g., garment feature localization under occlusion), and edge following along cloth edges. The pretrained representations achieve a 73-100% success rate on these 5 tasks.",Justin Kerr (Berkeley)*; Huang Huang (UC Berkeley); Albert Wilcox (UC Berkeley); Ryan I Hoque (UC Berkeley); Jeffrey Ichnowski (Carnegie Mellon University); Roberto Calandra (TU Dresden); Ken Goldberg (UC Berkeley),justin_kerr@berkeley.edu; hh19971229@berkeley.edu; albertwilcox@berkeley.edu; ryanhoque@berkeley.edu; jeffi@cmu.edu; dr.rcalandra@gmail.com; goldberg@berkeley.edu,In person,,,Robot Learning,"Grasping & Manipulation; Robot Perception, Sensors & Vision","Gordberg, Berkeley",,,Visuo-tactile self-supervised learning for deformable manipulation
19,3,137,Pre-Training for Robots: Offline RL Enables Learning New Tasks in a Handful of Trials,"Progress in deep learning highlights the tremendous potential of utilizing diverse datasets for attaining effective generalization and makes it enticing to consider leveraging broad datasets for attaining robust generalization in robotic learning as well. However, in practice we often want to learn a new skill in a new environment that is unlikely to be contained in the prior data. Therefore we ask: how can we leverage existing diverse offline datasets in combination with small amounts of task-specific data to solve new tasks, while still enjoying the generalization benefits of training on large amounts of data? In this paper, we demonstrate that end-to-end offline RL can be an effective approach for doing this, without the need for any representation learning or vision-based pre-training. We present pre-training for robots (PTR), a framework based on offline RL that attempts to effectively learn new tasks by combining pre-training on existing robotic datasets with rapid fine-tuning on a new task, with as a few as 10 demonstrations. PTR utilizes an existing offline RL method, conservative Q-learning (CQL), but extends it to include several crucial design decisions that enable PTR to actually work and outperform a variety of prior methods. To our knowledge, PTR is the first RL method that succeeds at learning new tasks in a new domain on a real WidowX robot with as few as 10 task demonstrations, by effectively leveraging an existing dataset of diverse multi-task robot data collected in a variety of toy kitchens. We also demonstrate that the PTR approach can enable effective autonomous fine-tuning and improvement in a handful of trials, without needing any demonstrations. An accompanying overview video can be found at this anonymous URL: ",Aviral Kumar (UC Berkeley)*; Anikait Singh (Berkeley); Frederik D Ebert (UC Berkeley); Mitsuhiko Nakamoto (UC Berkeley); Yanlai Yang (New York University); Chelsea Finn (Stanford); Sergey Levine (UC Berkeley),aviralk@berkeley.edu; asap7772@berkeley.edu; febert@berkeley.edu; nakamoto@eecs.berkeley.edu; yy2694@nyu.edu; cbfinn@cs.stanford.edu; svlevine@eecs.berkeley.edu,In person,,,Robot Learning,,"Levine, Berkeley + Chelsea, Stanford",,,RL for new robot tasks
20,4,330,Sampling-based Exploration for Reinforcement Learning of Dexterous Manipulation,"In this paper, we present a novel method for achieving dexterous manipulation of complex objects, while simultaneously securing the object without the use of passive support surfaces. We posit that a key difficulty for training such policies in a Reinforcement Learning framework is the difficulty of exploring the problem state space, as the accessible regions of this space form a complex structure along manifolds of a high-dimensional space. To address this challenge, we use two versions of the non-holonomic Rapidly-Exploring Random Trees algorithm; one version is more general, but requires explicit use of the environment’s transition function, while the second version uses manipulation-specific kinematic constraints to attain better sample efficiency. In both cases, we use states found via sampling-based exploration to generate reset distributions that enable training control policies under full dynamic constraints via model-free Reinforcement Learning. We show that these policies are effective at manipulation problems of higher difficulty than previously shown, and also transfer effectively to real robots.",Gagan Khandate (Columbia University)*; Siqi Shang (Columbia University); Eric T Chang (Columbia University); Tristan L Saidi (Columbia University); Johnson Adams (Columbia University); Matei Ciocarlie (Columbia University),gagank@cs.columbia.edu; siqi.shang@columbia.edu; eric.chang@columbia.edu; tls2160@columbia.edu; jp4334@columbia.edu; matei.ciocarlie@columbia.edu,In person,,,Robot Learning,Grasping & Manipulation,"Matei Ciocarlie, Columbia",,,RL for dexterous manipulation - sim2real
21,5,378,Cherry-Picking with Reinforcement Learning,"Grasping small objects surrounded by unstable or non-rigid material plays a crucial role in applications such as surgery, harvesting, construction, disaster recovery, and assisted feeding. This task is especially difficult when fine manipulation is required in the presence of sensor noise and perception errors; errors inevitably trigger dynamic motion,  which is challenging to model precisely. Circumventing the difficulty to build accurate models for contacts and dynamics, data-driven methods like reinforcement learning (RL) can optimize task performance via trial and error, reducing the need for accurate models of contacts and dynamics. Applying RL methods to real robots, however, has been hindered by factors such as prohibitively high sample complexity or the high training infrastructure cost for providing resets on hardware. This work presents CherryBot, an RL system that uses chopsticks for fine manipulation that surpasses human reactiveness for some dynamic grasping tasks. By integrating imprecise simulators, suboptimal demonstrations and external state estimation, we study how to make a real-world robot learning system sample efficient and general while reducing the human effort required for supervision. Our system shows continual improvement through 30 minutes of real-world interaction:  through reactive retry, it achieves an almost 100% success rate on the demanding task of using chopsticks to grasp small objects swinging in the air. We demonstrate the reactiveness, robustness and generalizability of CherryBot to varying object shapes and dynamics (e.g., external disturbances like wind and human perturbations). Videos are available at https://goodcherrybot.github.io/.",Yunchu Zhang (CMU); Liyiming Ke (University of Washington)*; Abhay Deshpande (University of Washington); Abhishek Gupta (University of Washington); Siddhartha Srinivasa (University of Washington),yunchuz@alumni.cmu.edu; kayke@cs.washington.edu; abhayd@uw.edu; abhgupta@cs.washington.edu; siddh@cs.washington.edu,In person,,,Grasping & Manipulation,Robot Learning,"Sidd Srinivasa, U Wash + Gupta",,,RL for some challening manipulation task
22,6,271,Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators,"We describe a system for deep reinforcement learning of robotic manipulation skills applied to a large-scale real-world task: sorting recyclables and trash in office buildings. Real-world deployment of deep RL policies requires not only effective training algorithms, but the ability to bootstrap real-world training and enable broad generalization. To this end, our system combines scalable deep RL from real-world data with bootstrapping from training in simulation, and incorporates auxiliary inputs from existing computer vision systems as a way to boost generalization to novel objects, while retaining the benefits of end-to-end training. We analyze the tradeoffs of different design decisions in our system, and present a large-scale empirical validation that includes training on real-world data gathered over the course of 24
months of experimentation, across a fleet of 23 robots in three office buildings, with a total training set of 9527 hours of robotic experience. Our final validation also consists of 4800 evaluation trials across 240 waste station configurations, in order to evaluate in detail the impact of the design decisions in our system, the scaling effects of including more real-world data, and the performance of the method on novel objects.","Alexander Herzog (Google X)*; Kanishka Rao (Google); Karol Hausman (Google Brain); Yao Lu (Google Research); Paul Wohlhart (Google); Mengyuan Yan (Google); Jessica Lin (Everyday Robots); Montserrat Gonzalez Arenas (Google); Ted Xiao (Google); Daniel Kappler (Google X); Daniel Ho (Google); Jarek Rettinghouse (Everyday Robots	); Yevgen Chebotar (Google); Kuang-Huei Lee (Google); Keerthana Gopalakrishnan (Google); Ryan Julian (Google); Adrian Li (Wayve); Chuyuan Fu (Everyday Robots	); Bob Wei (Everyday Robots); Sangeetha Ramesh (Everyday Robots	); Khem Holden (Google); Kim Kleiven (Everyday Robots); David J Rendleman (Google); Sean Kirmani (Everyday Robots); Jeffrey Bingham (Everyday Robots); Jonathan Weisz (Everyday Robots); Ying Xu (Everyday Robots); Wenlong Lu (Everyday Robots); Matthew Bennice (Everyday Robots); Cody Fong (Everyday Robots	); David Do (Everyday Robots); Jessica Lam (Everyday Robots); Yunfei Bai (X); Benjie Holson (X, Inc. (Google)); Michael Quinlan (X, Inc. (Google)); Noah Brown (Google); Mrinal Kalakrishnan (X); Julian Ibarz (Google); Peter Pastor (X); Sergey Levine (Google)",alexanderherzog001@googlemail.com; kanishkarao@google.com; karolhausman@google.com; yaolug@google.com; wohlhart@google.com; mengyuany@google.com; jiayilin@google.com; montse@google.com; tedxiao@google.com; kappler@google.com; danielho@google.com; jarekr@google.com; chebotar@google.com; leekh@google.com; keerthanapg@google.com; rjulian@google.com; adrlhli@gmail.com; fuchuyuan@google.com; bwei@google.com; saramesh@google.com; khemh@google.com; kkleiven@google.com; drendleman@google.com; skirmani@google.com; jeffbingham@google.com; jonweisz@google.com; xuyng@google.com; wenlongl@google.com; mbennice@google.com; codyfong@google.com; davdo@google.com; jessicala@google.com; yunfeibai@google.com; bmholson@gmail.com; michael.quinlan@gmail.com; noahbrown@google.com; kalakris@google.com; julianibarz@google.com; peterpastor@google.com; slevine@google.com,In person,,,Robot Learning,Grasping & Manipulation; Multi-Robot & Networked Systems,Google,,,Deep RL for robot manipulation
23,7,158,Demonstrating Large-Scale Package Manipulation via Learned Metrics of Pick Success,"Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to workforce fluctuations. The past few years have seen increased interest in automating such repeated tasks but mostly in controlled settings. Tasks such as picking objects from unstructured, cluttered piles have only recently become robust enough for large-scale deployment with minimal human intervention.

This paper demonstrates a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which utilizes a pick success predictor trained on real production data. Specifically, the system was trained on over 394K picks. It is used for singulating up to 5~million packages per day and has manipulated over 200~million packages during this paper's evaluation period.

The developed learned pick quality measure ranks various pick alternatives in real-time and prioritizes the most promising ones for execution. The pick success predictor aims to estimate from prior experience the success probability of a desired pick by the deployed industrial robotic arms in cluttered scenes containing deformable and rigid objects with partially known properties. It is a shallow machine learning model, which allows us to evaluate which features are most important for the prediction. An online pick ranker leverages the learned success predictor to prioritize the most promising picks for the robotic arm, which are then assessed for collision avoidance. This learned ranking process is demonstrated to overcome the limitations and outperform the performance of manually engineered and heuristic alternatives.

To the best of the authors' knowledge, this paper presents the first large-scale deployment of learned pick quality estimation methods in a real production system.",Shuai Li (Amazon Robotics); Azarakhsh Keipour (Amazon Robotics)*; Kevin Jamieson (U Washington); Nicolas Hudson (Amazon Robotics); Charles Swan (Amazon Robotics); Kostas Bekris (Rutgers University),shuailirpi@gmail.com; keipour@gmail.com; jamieson@cs.washington.edu; nicolashenryhudson@gmail.com; cswan@amazon.com; kostas.bekris@cs.rutgers.edu,In person,DEMO,,Grasping & Manipulation,"Assembly, Logistics and Manufacturing; Robot Learning","Kostas, Rutgers + Amazon",,,Learning manipulation in production
24,8,163,Demonstrating Large Language Models on Robots,"Robots may benefit from large language models (LLMs), which have demonstrated strong reasoning capabilities across various domains. This demonstration includes several systems based on recent methods that integrate LLMs on robots: SayCan, Socratic Models, Inner Monologue, and Code as Policies. While each algorithm highlights a different mode of grounding, they all share a common system-level structure in that they use LLMs to take as input natural language instructions and generate robot plans in the form of step-by-step procedures or code. This structure provides several practical perks for demonstration in that (i) we can use existing video chat interfaces to instruct the robot by typing commands and broadcasting its movements in action via video streaming, (ii) one can seamlessly switch between interfaces that communicate with different robots, and (iii) this can all be done remotely on a laptop, where the robots on real hardware can be held on standby in the lab ready to run on command. Our tentative plan is to show at least one system running on real hardware remotely -- Inner Monologue or Code as Policies, and solicit task instructions from a live audience. Time-permitting we may also demonstrate the other systems available to run on real hardware. Otherwise, we will present recorded videos of past runs. We will link to open-source code, and conclude with a discussion of open research questions in the area.",Andy Zeng (Google)*; Brian Ichter (Google Brain); Fei Xia (Google Inc); Ted Xiao (Google); Vikas Sindhwani (Google),andyzeng@google.com; ichter@google.com; xiafei@google.com; tedxiao@google.com; sindhwani@google.com,In person,DEMO,BEST DEMO,Robot Learning,Human-Robot Interaction,Google,,,LLMs for Robots
,,,,,,,,,,,,,,,
,,,,4. Large Data and Vision-Language Models for Robotics,,,,,,,,,,,
25,1,6,Diffusion Policy: Visuomotor Policy Learning via Action Diffusion,"This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%. Diffusion Policy learns the score function of the action distribution and optimizes with respect to this gradient field iteratively during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details will be publicly available.",Cheng Chi (Columbia University)*; Siyuan Feng (Toyota Research Institute); Yilun Du (MIT); Zhenjia Xu (Columbia University); Eric Cousineau (Toyota Research Institute); Benjamin CM Burchfiel (Toyota Research Institute); Shuran Song (Columbia University),cc4617@columbia.edu; siyuan.feng@tri.global; yilundu@mit.edu; xuzhenjia@cs.columbia.edu; eric.cousineau@tri.global; ben.burchfiel@tri.global; shurans@cs.columbia.edu,In person,,BEST STUDENT,Robot Learning,Grasping & Manipulation,"Shuran Song, Columbia + Toyota RI",,,Generative models for visuomotor policies
26,2,109,RT-1: Robotics Transformer for Real-World Control at Scale,"By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks.","Anthony Brohan (Google Research); Noah Brown (Google Research); Justice Carbajal (Google Research); Yevgen Chebotar (Google); Joseph Dabis (Google Research	); Chelsea Finn (Google Brain); Keerthana Gopalakrishnan (Google); Karol Hausman (Google Brain)*; Alexander Herzog ([X]); Jasmine Hsu (Google); Julian Ibarz (Google); Brian Ichter (Google Brain); Alex Irpan (Google); Tomas Jackson (Google Research); Sally Jesmonth (Google Research); Nikhil Joshi (Google Research); Ryan Julian (Google); Dmitry Kalashnikov (Google Inc.); Yuheng Kuang (Google Research); Isabel Leal (Google Research	); Kuang-Huei Lee (Google); Sergey Levine (Google); Yao Lu (Google Research); Utsav Malla (Google Research	); Deeksha Manjunath (Google Research); Igor Mordatch (Google); Ofir Nachum (Google); Carolina Parada (Google); Jodilyn  Peralta (Google); Emily Perez (Google); Karl Pertsch (Google); Jornell  Quiambao (Google); Kanishka Rao (Google); Michael S Ryoo (Google; Stony Brook University); Grecia  Salazar (Google); Pannag R Sanketi (Google Inc.); Kevin  Sayed (Google); Jaspiar  Singh (Google); Sumedh  Sontakke (Google); Austin  Stone (Google); Clayton  Tan (Google); Huong  Tran (Google); Vincent Vanhoucke (Google); Steve  Vega (Google); Quan H Vuong (Google); Fei Xia (Google Inc); Ted Xiao (Google); Peng Xu (Google Inc); Sichun Xu (Google); Tianhe Yu (Google Brain); Brianna  Zitkovich (Google)",brohan@google.com; noahbrown@google.com; jucarbajal@google.com; chebotar@google.com; dabis@google.com; chelseaf@google.com; keerthanapg@google.com; karolhausman@google.com; alexherzog@x.team; hellojas@google.com; julianibarz@google.com; ichter@google.com; alexirpan@google.com; tomasjackson@google.com; sallyjesmonth@google.com; nikhiljoshi@google.com; rjulian@google.com; dkalashnikov@google.com; yuheng@google.com; isabelleal@google.com; leekh@google.com; slevine@google.com; yaolug@google.com; umalla@google.com; eemd@google.com; imordatch@google.com; ofirnachum@gmail.com; carolinap@google.com; jodilyn@google.com; emilyperez@google.com; kpertsch@google.com; jornell@google.com; kanishkarao@google.com; mryoo@google.com; grecias@google.com; psanketi@google.com; ksayed@google.com; jaspiar@google.com; sasontakke@google.com; austinstone@google.com; claytontan@google.com; huongtt@google.com; vanhoucke@google.com; stevevega@google.com; quanhovuong@google.com; xiafei@google.com; tedxiao@google.com; pengxu@google.com; sicxu@google.com; tianheyu@google.com; zitkovich@google.com,In person,,,Robot Learning,Grasping & Manipulation,Google ,,,Large Robotics Model
27,3,129,Scaling Robot Learning with Semantically Imagined Experience,"Recent advances in robot learning have shown promise in enabling robots to perform a variety of manipulation tasks and generalize to novel scenarios. 
One of the key contributing factors to this progress is the scale of robot data used to train the models. To obtain large-scale datasets, prior approaches have relied on either demonstrations requiring high human involvement or engineering-heavy autonomous data collection schemes, both of which being challenging in scaling up the space of new tasks and skills needed for building generalist robots. 
To mitigate this issue, we propose to take an alternative route and leverage text-to-image foundation models widely used in computer vision and natural language processing to obtain meaningful data for robot learning without requiring additional robot data. Specifically, we make use of the state of the art text-to-image diffusion models and perform aggressive data augmentation on top of our existing robotic manipulation datasets via inpainting of various unseen objects for manipulation, backgrounds, and distractors with pure text guidance. Through extensive real-world experiments, we show that manipulation policies trained on the augmented data are able to solve completely unseen tasks with new objects and can behave more robustly w.r.t. novel distractors. In addition, we also find that we can improve the robustness and generalization of high-level robot learning tasks such as success detection through training with the diffusion-based data augmentation.",Tianhe Yu (Google Brain)*; Ted Xiao (Google); Jonathan Tompson (Google); Austin Stone (Google Inc); Su Wang (Google AI Language); Anthony Brohan (Google Research); Jaspiar Singh (Google); Clayton Tan (Google); Dee M (Google); Jodilyn Peralta (Google); Karol Hausman (Google Brain); Brian Ichter (Google Brain); Fei Xia (Google Inc),tianheyu@google.com; tedxiao@google.com; tompson@google.com; austinstone@google.com; wangsu@google.com; brohan@google.com; jaspiar@google.com; claytontan@google.com; deemd@google.com; jodilyn@google.com; karolhausman@google.com; ichter@google.com; xiafei@google.com,In person,,,Robot Learning,Grasping & Manipulation,Google,,,text-to-image models for robot learning 
28,4,245,Goal-Conditioned Imitation Learning using Score-based Diffusion Policies,"We propose a new policy representation based on score-based diffusion models (SDMs). We apply our new policy representation in the domain of Goal-Conditioned Imitation Learning (GCIL) to learn general-purpose goal-specified policies from large uncurated datasets without rewards. Our new goal-conditioned policy architecture ""BEhavior generation with ScOre-based Diffusion Policies"" (BESO) leverages a generative, score-based diffusion model as its policy. BESO decouples the learning of the score model from the inference sampling process, and, hence allows for fast sampling strategies to generate goal-specified behavior in just 3 inference steps, compared to 30+ inference steps of other diffusion based policies. Furthermore, BESO is highly expressive and can effectively capture multi-modality present in the solution space of the play data. Unlike previous methods such as Latent Plans or C-Bet, BESO does not rely on complex hierarchical policies or additional clustering for effective goal-conditioned behavior learning. Finally, we show how BESO can even be used to learn a goal-independent policy from play-data using classifier-free guidance. To the best of our knowledge this is the first work that a) represents a behavior policy based on such a decoupled SDM  b) learns an SDM based policy in the domain of GCIL and c) provides a way to simultaneously learn a goal-dependent and a goal-independent policy from play-data. We evaluate BESO through detailed simulation and show that it consistently outperforms several state-of-the-art goal-conditioned imitation learning methods on challenging benchmarks. We additionally provide extensive ablation studies and experiments to demonstrate the effectiveness of our method for goal-conditioned behavior generation. Demonstrations and Code are available at https://intuitive-robots.github.io/beso-website.",Moritz Reuss (Karlsruhe Institute of Technology (KIT))*; Maximilian Li (Karlsruher Institute of Technology); Xiaogang Jia (Karlsruhe Institute of Technology); Rudolf Lioutikov (Karlsruhe Institute of Technology),moritz.reuss@kit.edu; maximilian.li@kit.edu; xiaogang.jia@partner.kit.edu; lioutikov@kit.edu,In person,,,Robot Learning,,Karlsruhe,,,Diffusion model for immitation learning
29,5,145,Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models,"Robotic manipulation policies that follow natural language instructions are typically trained from corpora of robot-language data that were either collected with specific tasks in mind or expensively relabeled by humans with varied language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? For example, if the original annotations contained simple task descriptions such as ""pick up the apple"", a pretrained VLM-based labeler could significantly expand the number of semantic concepts available in the data and introduce spatial concepts such as ""the apple on the right side of the table"" or alternative phrasings such as ""the red colored fruit"". To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabeled demonstration data and then train language-conditioned policies on the augmented datasets. This method enables cheaper acquisition of useful language descriptions compared to expensive human labels, allowing for more efficient label coverage of large-scale datasets. We apply DIAL to a challenging real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations do not contain crowd-sourced language annotations. Through a large-scale study of over 1,300 real world evaluations, we find that DIAL enables imitation learning policies to acquire new capabilities and generalize to 60 novel instructions unseen in the original dataset.",Ted Xiao (Google)*; Harris Chan (University of Toronto); Pierre Sermanet (Google); Ayzaan Wahid (Google); Anthony Brohan (Google Research); Karol Hausman (Google Brain); Sergey Levine (Google); Jonathan Tompson (Google),tedxiao@google.com; hchan@cs.toronto.edu; sermanet@google.com; ayzaan@google.com; brohan@google.com; karolhausman@google.com; slevine@google.com; tompson@google.com,In person,,,Robot Learning,Grasping & Manipulation,"Levine, Google",,,VLMs for robot learning
30,6,71,Energy-based Models are Zero-Shot Planners for Compositional Scene Rearrangement,"Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene-rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. 
We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then re-locate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model can execute highly compositional instructions zero-shot in simulation and in the real world. It outperforms language-to-action reactive policies and Large Language Model planners by a large margin, especially for long instructions that involve compositions of multiple spatial concepts. Simulation and real-world robot execution videos, as well as our code and datasets are publicly available on our website: https://ebmplanner.github.io.",Nikolaos Gkanatsios (Carnegie Mellon University)*; Ayush Jain (Carnegie Mellon University); Zhou Xian (Carnegie Mellon University); Yunchu Zhang (CMU); Christopher G Atkeson (CMU Robotics Institute); Katerina Fragkiadaki (Carnegie Mellon University),ngkanats@andrew.cmu.edu; ayushj2@andrew.cmu.edu; zhouxian@cmu.edu; yunchuz@alumni.cmu.edu; cga@andrew.cmu.edu; katef@cs.cmu.edu,In person,,,Robot Learning,Robot Planning,"Atkenson, Fragkiadaki, CMU",,,Mapping language to energy functions over object arrangements 
31,7,45,StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects,"Robots operating in human environments must be able to rearrange objects into semantically-meaningful configurations, even if these objects are previously unseen. In this work, we focus on the problem of building physically-valid structures without step-by-step instructions. We propose StructDiffusion, which combines a diffusion model and an object-centric transformer to construct structures given partial-view point clouds and high-level language goals, such as ""set the table"". Our method can perform multiple challenging language-conditioned multi-step 3D planning tasks using one model. StructDiffusion even improves the success rate of assembling physically-valid structures out of unseen objects by on average 16% over an existing multi-modal transformer model trained on specific structures. We show experiments on held-out objects in both simulation and on real-world rearrangement tasks. Importantly, we show how integrating both a diffusion model and a collision-discriminator model allows for improved generalization over other methods when rearranging previously-unseen objects.",Weiyu Liu (Georgia Institute of Technology)*; Yilun Du (MIT); Tucker Hermans (University of Utah); Sonia Chernova (Georgia Institute of Technology); Chris Paxton (Meta AI),wliu88@gatech.edu; yilundu@mit.edu; tucker.hermans@utah.edu; chernova@cc.gatech.edu; cpaxton@fb.com,In person,,,Robot Learning,Grasping & Manipulation,"Hermans, Utah + Chernova, GaTech",,,Diffucion language model for rearrangement
32,8,233,Language-Driven Representation Learning for Robotics,"Recent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks. Leveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control. But, robot learning encompasses a diverse set of problems beyond control including grasp affordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration, amongst others. First, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of high-level semantics, while contrastive learning approaches capture the opposite. We then introduce Voltron, a framework for language-driven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics. We also construct a new evaluation suite spanning five distinct robot learning problems – a unified platform for holistically evaluating visual representations for robotics. Through comprehensive, controlled experiments across all five problems, we find that Voltron’s language-driven representations outperform the prior state-of-the-art, especially on targeted problems requiring higher-level features.",Siddharth Karamcheti (Stanford University)*; Suraj Nair (Stanford University); Annie S Chen (Stanford University); Thomas Kollar (TRI); Chelsea Finn (Stanford); Dorsa Sadigh (Stanford); Percy Liang (Stanford University),skaramcheti@cs.stanford.edu; surajn@stanford.edu; asc8@stanford.edu; thomas.kollar@tri.global; cbfinn@cs.stanford.edu; dorsa@cs.stanford.edu; pliang@cs.stanford.edu,In person,,BEST PAPER,Robot Learning,,"Dorsa, Chelsea, Stanford",,,Language-driven representations for robotics
,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,
,,,,5. Simulation and Sim2Real for RL Transfer,,,,,,,,,,,
33,1,240,Local object crop collision network for efficient simulation of non-convex objects in GPU-based simulators,"Our goal is to develop an efficient contact detection algorithm for large-scale GPU-based simulation of non-convex objects. Current GPU-based simulators such as IsaacGym [16] and Brax [11] must trade-off speed with fidelity, generality, or both when simulating non-convex objects. Their main issue lies in contact detection (CD): existing CD algorithms, such as Gilbert–Johnson–Keerthi (GJK), must trade off their computational speed with accuracy which becomes expensive as the number of collisions among non-convex objects increases. We propose a data-driven approach for CD, whose accuracy depends only on the quality and quantity of offline dataset rather than online computation time. Unlike GJK, our method inherently has a uniform computational flow, which facilitates efficient GPU usage based on advanced compilers such as XLA (Accelerated Linear Algebra) [2]. Further, we offer a data-efficient solution by learning the patterns of colliding local crop object shapes, rather than global object shapes which are harder to learn. We demonstrate our approach improves the efficiency of existing CD methods by a factor of 5-10 for nonconvex objects with comparable accuracy. Using the previous work on contact resolution for a neural-network-based contact detector [23], we integrate our CD algorithm into the open-source GPU-based simulator, Brax, and show that we can improve the efficiency over IsaacGym and generality over standard Brax. We highly recommend the videos of our simulator included in the supplementary materials. (https://sites.google.com/view/locc-rss2023/home)",Dongwon Son (KAIST)*; Beomjoon Kim (MIT),dongwon.son@kaist.ac.kr; beomjoon.kim@kaist.ac.kr,In person,,,Robot Modeling & Simulation,,"Beomjoon, KAIST",,,Data-driven contact detection and simulator
34,2,85,GranularGym: High Performance Simulation for Robotic Tasks with Granular Materials,"Granular materials are of critical interest to many robotic tasks in planetary science, construction, and manufacturing. However, the dynamics of granular materials are complex and often computationally very expensive to simulate. We propose a set of methodologies and a system for the fast simulation of granular materials on Graphics Processing Units (GPUs), and show that this simulation is fast enough for basic training with Reinforcement Learning algorithms, which currently require many dynamics samples to achieve acceptable performance. Our method models granular material dynamics using implicit timestepping methods for multibody rigid contacts, as well as algorithmic techniques for efficient parallel collision detection between pairs of particles and between particle and arbitrarily shaped rigid bodies, and programming techniques for minimizing warp divergence on Single-Instruction, Multiple-Thread (SIMT) chip architectures. We showcase our simulation system on several environments targeted toward robotic tasks, and release our simulator as an open-source tool.",David R Millard (University of Southern California)*; Daniel Pastor (Jet Propulsion Laboratory); Joseph Bowkett (JPL); Paul Backes (Jet Propulsion Laboratory); Gaurav S Sukhatme (University of Southern California; Amazon),dmillard@usc.edu; daniel.pastor.moreno@jpl.nasa.gov; bowkett@jpl.nasa.gov; paul.g.backes@jpl.nasa.gov; gaurav@usc.edu,In person,,,Robot Modeling & Simulation,,"Gaurav Sukhatme, USC + JPL",,,Modeling granular material for RL
35,3,205,Beyond Flat GelSight Sensors: Simulation of Optical Tactile Sensors of Complex Morphologies for Sim2Real Learning,"Recently, several morphologies, each with its advantages, have been proposed for the GelSight high-resolution tactile sensors. However, existing simulation methods are limited to flat-surface sensors, which prevents its usage with the newer sensors of non-flat morphologies in Sim2Real experiments. In this paper, we extend a previously proposed GelSight simulation method, which was developed for flat-surface sensors, and propose a novel method for curved sensors. In particular, we address the simulation of light rays travelling through a curved tactile membrane in the form of geodesic paths. The method is validated by simulating the finger-shaped GelTip sensor and comparing the generated synthetic tactile images against the corresponding real images. Our extensive experiments show that combining the illumination generated from the geodesic paths, with a background image from the real sensor, produces the best results when compared to the lighting generated by direct linear paths in the same conditions. As the method is parameterized by the sensor mesh, it can be applied in principle to simulate a tactile sensor of any morphology.  The proposed method not only unlocks simulating existing optical tactile sensors of complex morphologies, but also enables experimenting with sensors of novel morphologies, before the fabrication of the real sensor.
Project website: https://danfergo.github.io/geltip-sim",Daniel Fernandes Gomes (Kings College London)*; Shan Luo (King's College London); Paolo Paoletti (University of Liverpool),danfergo@gmail.com; shan.luo@kcl.ac.uk; P.Paoletti@liverpool.ac.uk,In person,,,"Robot Perception, Sensors & Vision",Robot Modeling & Simulation,UK,,,Simulation of tactile sensors
36,4,68,Rotating without Seeing: Towards In-hand Dexterity through Touch,"Tactile information plays a critical role in human dexterity. It reveals useful contact information that may not be inferred directly from vision. In fact, humans can even perform in-hand dexterous manipulation without using vision. Can we enable the same ability for the multi-finger robot hand? In this paper, we propose to perform in-hand object rotation using only touching without seeing the object. Instead of relying on precise tactile sensing in a small region, we introduce a new system design using dense binary force sensors (touch or no touch) overlaying one side of the whole robot hand (palm, finger links, fingertips). Such a design is low-cost, giving a larger coverage of the object, and minimizing the Sim2Real gap at the same time. We train an in-hand rotation policy using Reinforcement Learning on diverse objects in simulation. Relying on touch-only sensing, we can directly deploy the policy in a real robot hand and rotate novel objects that are not presented in training. Extensive ablations are performed on how tactile information help in-hand manipulation. ","Zhao-Heng Yin (HKUST)*; Binghao Huang (University of California, San Diego); Yuzhe Qin (University of California San Diego); Qifeng Chen (HKUST); Xiaolong Wang (UCSD)",zhaohengyin@gmail.com; qazploki@163.com; y1qin@eng.ucsd.edu; cqf@ust.hk; xiw012@ucsd.edu,In person,,,Grasping & Manipulation,Robot Learning,"Qifeng Chen, Hong Kong- Xiaolong, UCSD",,,Dexterous manipulation via tactile-based RL training in simulation
37,5,202,DexPBT: Scaling up Dexterous Manipulation for Hand-Arm Systems with Population Based Training,"In this work, we propose algorithms and methods that enable learning dexterous object manipulation using simulated one- or two-armed robots equipped with multi-fingered hand end-effectors. Using a parallel GPU-accelerated physics simulator (Isaac Gym), we implement challenging tasks for these robots, including regrasping, grasp-and-throw, and object reorientation. To solve these problems we introduce a decentralized Population-Based Training (PBT) algorithm that allows us to massively amplify the exploration capabilities of deep reinforcement learning. We find that this method significantly outperforms regular end-to-end learning and is able to discover robust control policies in challenging tasks. Video demonstrations of learned behaviors and the code can be found at https://sites.google.com/view/dexpbt",Aleksei Petrenko (University of Southern California)*; Arthur Allshire (University of Toronto); Gavriel State (NVIDIA); Ankur Handa (NVIDIA); Viktor Makoviychuk (NVIDIA),apetrenko1991@gmail.com; arthur@allshire.org; gstate@nvidia.com; ahanda@nvidia.com; vmakoviychuk@nvidia.com,In person,,,Robot Learning,Grasping & Manipulation; Robot Modeling & Simulation,"Ankur, NVIDIA",,,Learning dexterous manipulation via simulation
38,6,387,Hindsight States: Blending Sim & Real Task Elements for Efficient Reinforcement Learning,"Reinforcement learning has shown great potential in solving complex tasks when large amounts of data can be generated with little effort. In robotics, one approach to generate training data builds on simulations or models. However, for many tasks, such as with complex soft robots, devising such models is substantially more challenging. Recent successes in soft robotics indicate that employing complex robots can lead to performance boosts. Here, we leverage the imbalance in complexity of the dynamics to learn more sample-efficiently. We (i) abstract the task into distinct components, (ii) off-load the simple dynamics parts into the simulation, and (iii) multiply these virtual parts to generate more data in hindsight. Our new method, Hindsight States (HiS), uses this data and selects the most useful transitions for training. It can be used with an arbitrary off-policy algorithm. 
We validate our method on several challenging simulated tasks and demonstrate that it improves learning both on its own and when combined with an existing hindsight algorithm, Hindsight Experience Replay (HER). Finally, we evaluate HiS on a physical system and show that it boosts performance on a complex table tennis task with a muscular robot.","Simon Guist (Max Planck Institute for Intelligent Systems)*; Jan Schneider (MPI for Intelligent Systems, Tübingen); Vincent Berenz (Max Planck Institute for Intelligent Systems); Alexander Dittrich (MPI for Intelligent Systems); Bernhard Schölkopf (MPI for Intelligent Systems, Tübingen); Dieter Büchler (MPI for Intelligent Systems, Tübingen)",simon.guist@gmail.com; Jan.Schneider@tuebingen.mpg.de; vberenz@tuebingen.mpg.de; alexander.dittrich@tuebingen.mpg.de; bs@tuebingen.mpg.de; dieter.buechler@tuebingen.mpg.de,In person,,,Robot Learning,Robot Modeling & Simulation; Soft Robots,Max Planck,,,Improving RL traning via simulation
39,7,391,IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality,"Robotic assembly is a longstanding challenge, requiring contact-rich interaction and high precision and accuracy. Many applications also require adaptivity to diverse parts, poses, and environments, as well as low cycle times. In other areas of robotics, simulation is a powerful tool to develop algorithms, generate datasets, and train agents. However, simulation has had a more limited impact on assembly. We present IndustReal, a set of algorithms, systems, and tools that solve assembly tasks in simulation with reinforcement learning (RL) and successfully achieve policy transfer to the real world. Specifically, we propose 1) simulation-aware policy updates, 2) signed-distance-field rewards, and 3) sampling-based curricula for robotic RL agents. We use these algorithms to enable robots to solve contact-rich pick, place, and insertion tasks in simulation. We then propose 4) a policy-level action integrator to minimize error at policy deployment time. We build and demonstrate a real-world robotic assembly system that uses the trained policies and action integrator to achieve repeatable performance in the real world. Finally, we present hardware and software tools that allow other researchers to reproduce our system and results. For videos and additional details, please see our project website at https://sites.google.com/nvidia.com/industreal.",Bingjie Tang (University of Southern California)*; Michael A Lin (Stanford University); Iretiayo A Akinola (NVIDIA Research); Ankur Handa (NVIDIA); Gaurav S Sukhatme (University of Southern California; Amazon); Fabio Ramos (NVIDIA); Dieter Fox (NVIDIA); Yashraj S Narang (NVIDIA),bingjiet@usc.edu; mlinyang@stanford.edu; iakinola@nvidia.com; ahanda@nvidia.com; gaurav@usc.edu; ftozetoramos@nvidia.com; dieterf@nvidia.com; ynarang@nvidia.com,In person,,,"Assembly, Logistics and Manufacturing",Robot Learning; Robot Modeling & Simulation,"Sukhatme, USC + Ramos, Fox, NVIDIA",,,Learning assembly via RL in simulation
40,8,160,SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering,"Model-based reinforcement learning (MBRL) is recognized with the potential to be significantly more sample efficient than model-free RL. How an accurate model can be developed automatically and efficiently from raw sensory inputs (such as images), especially for complex environments and tasks, is a challenging problem that hinders the broad application of MBRL in the real world. In this work, we propose a sensing-aware model-based reinforcement learning system called SAM-RL. Leveraging the differentiable physics-based simulation and rendering, SAM-RL automatically updates the model by comparing rendered images with real raw images and produces the policy efficiently. With the sensing-aware learning pipeline, SAM-RL allows a robot to select an informative viewpoint to monitor the task process. We apply our framework to real world experiments for accomplishing three manipulation tasks: robotic assembly, tool manipulation, and deformable object manipulation. We demonstrate the effectiveness of SAM-RL via extensive experiments. Videos are available on our project webpage at https://sites.google.com/view/rss-sam-rl.","Jun Lv (Shanghai Jiao Tong University)*; Yunhai Feng (University of California, San Diego ); Cheng Zhang (Meta Reality Labs Research); Shuang Zhao (University of California, Irvine); Lin Shao (National University of Singapore); Cewu Lu (Shanghai Jiao Tong University)",LyuJune_SJTU@sjtu.edu.cn; fyhMer@gmail.com; zhangchengee@gmail.com; shz@ics.uci.edu; linshao@nus.edu.sg; lucewu@sjtu.edu.cn,In person,,BEST SYSTEM,Robot Learning,"Cognitive Modeling & Knowledge Representation for Robots; Robot Perception, Sensors & Vision","Cewu Lu, Shanghai Jiao Tong",,,RL via differentiable physics-based simulation
,,,,,,,,,,,,,,,
,,,,6. Grasping and Manipulation,,,,,,,,,,,
41,1,26,FurnitureBench: Reproducible Real-World Benchmark for Long-Horizon Complex Manipulation,"Reinforcement learning (RL), imitation learning (IL), and task and motion planning (TAMP) have demonstrated impressive performance across various robotic manipulation tasks. However, these approaches have been limited to learning simple behaviors in current real-world manipulation benchmarks, such as pushing or pick-and-place. To enable more complex, long-horizon behaviors of an autonomous robot, we propose to focus on real-world furniture assembly, a complex, long-horizon robot manipulation task that requires addressing many current robotic manipulation challenges to solve. We present FurnitureBench, a reproducible real-world furniture assembly benchmark aimed at providing a low barrier for entry and being easily reproducible, so that researchers across the world can reliably test their algorithms and compare them against prior work. For ease of use, we provide 200+ hours of pre-collected data (5000+ demonstrations), 3D printable furniture models, a robotic environment setup guide, and systematic task initialization. Furthermore, we provide FurnitureSim, a fast and realistic simulator of FurnitureBench. We benchmark the performance of offline RL and IL algorithms on our assembly tasks and demonstrate the need to improve such algorithms to be able to solve our tasks in the real world, providing ample opportunities for future research.","Minho Heo (Korea Advanced Institute of Science & Technology); Youngwoon Lee (University of California, Berkeley)*; Doohyun Lee (Korea Advanced Institute of Science & Technology); Joseph J Lim (University of Southern California)",minho.heo@kaist.ac.kr; youngwoon@berkeley.edu; doohyun@kaist.ac.kr; joe.lim@kaist.ac.kr,In person,,BEST SYSTEM,Robot Learning,Grasping & Manipulation,"Joseph Lim, KAIST",,,Benchmark for Manipulation
42,2,2,Learning-Free Grasping of Unknown Objects Using Hidden Superquadrics,"Robotic grasping is an essential and fundamental task and has been studied extensively over the past several decades. Traditional work analyzes physical models of the objects and computes force-closure grasps. Such methods require pre-knowledge of the complete 3D model of an object, which can be hard to obtain. Recently with significant progress in machine learning, data-driven methods have dominated the area. Although impressive improvements have been achieved, those methods require a vast amount of training data and suffer from limited generalizability. In this paper, we propose a novel two-stage approach to predicting and synthesizing grasping poses directly from the point cloud of an object without database knowledge or learning. Firstly, multiple superquadrics are recovered at different positions within the object, representing the local geometric features of the object surface. Subsequently, our algorithm exploits the tri-symmetry feature of superquadrics and synthesizes a list of antipodal grasps from each recovered superquadric. An evaluation model is designed to assess and quantify the quality of each grasp candidate. The grasp candidate with the highest score is then selected as the final grasping pose. We conduct experiments on isolated and packed scenes to corroborate the effectiveness of our method. The results indicate that our method demonstrates competitive performance compared with the state-of-the-art without the need for either a full model or prior training.",Yuwei Wu (National University of Singapore)*; Weixiao Liu (Johns Hopkins University); zhiyang liu (national university of singapore); Gregory S Chirikjian (National University of Singapore),yw.wu@nus.edu.sg; wliu72@jhu.edu; zhiyang@u.nus.edu; mpegre@nus.edu.sg,In person,,,Grasping & Manipulation,"Robot Learning; Robot Perception, Sensors & Vision",Gregory S Chirikjian ,,,Grasping from models - no learning
43,3,376,Uncertain Pose Estimation during Contact Tasks using Differentiable Contact Features,"For many robotic manipulation and contact tasks, it is crucial to accurately estimate uncertain object poses, for which certain geometry and sensor information are fused in some optimal fashion. Previous results for this problem primarily adopt sampling-based or end-to-end learning methods, which yet often suffer from the issues of efficiency and generalizability.
In this paper, we propose a novel differentiable framework for this uncertain pose estimation during contact, so that it can be solved in an efficient and accurate manner with gradient-based solver. 
To achieve this, we introduce a new geometric definition that is highly adaptable and capable of providing differentiable contact features. 
Then we approach the problem from a bi-level perspective and utilize the gradient of these contact features along with differentiable optimization to efficiently solve for the uncertain pose.
Several scenarios are implemented to demonstrate how the proposed framework can improve existing methods.",Dongjun Lee (Seoul National University)*; Jeongmin Lee (Seoul National University); Minji Lee (Seoul National University),djlee@snu.ac.kr; ljmlgh@snu.ac.kr; mingg8@snu.ac.kr,In person,,,Grasping & Manipulation,Robot Modeling & Simulation,Seoul National University,,,Optimization over contact with a differentialble process
44,4,309,Simultaneous Trajectory Optimization and Contact Selection for Multi-Modal Manipulation Planning,"Complex dexterous manipulations require switching between prehensile and non-prehensile grasps, and sliding and pivoting the object against the environment. This paper presents a manipulation planner that is able to reason about diverse changes of contacts to discover such plans. It implements a hybrid approach that performs contact-implicit trajectory optimization for pivoting and sliding manipulation primitives and sampling-based planning to change between manipulation primitives and target object poses. The optimization method, simultaneous trajectory optimization and contact selection (STOCS), introduces an infinite programming framework to dynamically select from contact points and support forces between the object and environment during a manipulation primitive. To sequence manipulation primitives, a sampling-based tree-growing planner uses STOCS to construct a manipulation tree.
We show that by using a powerful trajectory optimizer, the proposed planner can discover multi-modal manipulation trajectories involving grasping, sliding, and pivoting within a few dozen samples. The resulting trajectories are verified to enable a 6 DoF manipulator to manipulate physical objects successfully.",Mengchao Zhang (University of Illinois at Urbana-Champaign)*; Devesh K Jha (MERL); Arvind U Raghunathan (MERL); Kris Hauser (University of Illinois),mz17@illinois.edu; jha@merl.com; raghunathan@merl.com; kkhauser@illinois.edu,In person,,,Grasping & Manipulation,Robot Planning,"Kris Hauser, UIUC + MERL",,,Planning for multi-modal manipulation
45,5,239,Precise Object Sliding with Top Contact via Asymmetric Dual Limit Surfaces,"In this paper, we discuss the mechanics and planning algorithms to slide an object on a horizontal planar surface via frictional patch contact made with its top surface. Here, we propose an asymmetric dual limit surface model to determine slip boundary conditions for both the top and bottom contact. With this model, we obtain a range of twists that can keep the object in sticking contact with the robot end-effector while slipping on the supporting plane. Based on these constraints, we derive a planning algorithm to slide objects with only top contact to arbitrary goal poses without slippage between end effector and the object. We validate the proposed model empirically and demonstrate its predictive accuracy on a variety of object geometries and motions. We also evaluate the planning algorithm over a variety of objects and goals demonstrate an orientation error improvement of 90% when compared to methods naive to linear path planners.",Xili Yi (University of Michigan)*; Nima Fazeli (Michigan),yixili@umich.edu; nfz@umich.edu,In person,,,Grasping & Manipulation,Robot Planning,"Nima Fazeli, Michigan",,,Modeling and planning to slide objects
46,6,7,RoboNinja: Learning an Adaptive Cutting Policy for Multi-Material Objects,"We introduce RoboNinja, a learning-based cutting system for multi-material objects (i.e., soft objects with rigid cores such as avocados or mangos). In contrast to prior works using open-loop cutting actions to cut through single-material objects (e.g., slicing a cucumber), RoboNinja aims to remove the soft part of an object while preserving the rigid core, thereby maximizing the yield. To achieve this, our system closes the perception-action loop by utilizing an interactive state estimator and an adaptive cutting policy.  The system first employs sparse collision information to iteratively estimate the position and geometry of an object's core and then generates closed-loop cutting actions based on the estimated state and a tolerance value. The ""adaptiveness"" of the policy is achieved through the tolerance value, which modulates the policy's conservativeness when encountering collisions, maintaining an adaptive safety distance from the estimated core. Learning such cutting skills directly on a real-world robot is challenging. Yet, existing simulators are limited in simulating multi-material objects or computing the energy consumption during the cutting process. To address this issue, we develop a differentiable cutting simulator that supports multi-material coupling and allows for the generation of optimized trajectories as demonstrations for policy learning. Furthermore, by using a low-cost force sensor to capture collision feedback, we were able to successfully deploy the learned model in real-world scenarios, including objects with diverse core geometries and soft materials. ",Zhenjia Xu (Columbia University)*; Zhou Xian (Carnegie Mellon University); Xingyu Lin (UC Berkeley); Cheng Chi (Columbia University); Zhiao Huang (University of California San Diego); Chuang Gan (MIT-IBM Watson AI Lab); Shuran Song (Columbia University),xuzhenjia@cs.columbia.edu; zhouxian@cmu.edu; xingyu@berkeley.edu; cc4617@columbia.edu; z2huang@eng.ucsd.edu; ganchuang1990@gmail.com; shurans@cs.columbia.edu,In person,,,Grasping & Manipulation,"Robot Perception, Sensors & Vision","Shuran Song, Columbia",,,Learning in simulation to cut
47,7,24,Dynamic-Resolution Model Learning for Object Pile Manipulation,"Dynamics models learned from visual observations have shown to be effective in various robotic manipulation tasks. One of the key questions for learning such dynamics models is what scene representation to use. Prior works typically assume representation at a fixed dimension or resolution, which may be inefficient for simple tasks and ineffective for more complicated tasks. In this work, we investigate how to learn dynamic and adaptive representations at different levels of abstraction to achieve the optimal trade-off between efficiency and effectiveness. Specifically, we construct dynamic-resolution particle representations of the environment and learn a unified dynamics model using graph neural networks (GNNs) that allows continuous selection of the abstraction level. During test time, the agent can adaptively determine the optimal resolution at each model-predictive control (MPC) step. We evaluate our method in object pile manipulation, a task we commonly encounter in cooking, agriculture, manufacturing, and pharmaceutical applications. Through comprehensive evaluations both in the simulation and the real world, we show that our method achieves significantly better performance than state-of-the-art fixed-resolution baselines at the gathering, sorting, and redistribution of granular object piles made with various instances like coffee beans, almonds, corn, etc.",Yixuan Wang (University of Illinois at Urbana-Champaign); Yunzhu Li (Stanford University & University of Illinois at Urbana-Champaign)*; Katherine Driggs-Campbell (University of Illinois at Urbana-Champaign); Li Fei-Fei (Stanford University); Jiajun Wu (Stanford University),yixuan22@illinois.edu; liyunzhu@stanford.edu; krdc@illinois.edu; feifeili@cs.stanford.edu; jiajunwu@cs.stanford.edu,In person,,,Grasping & Manipulation,Robot Learning,"Katie, UIUC, Fei-Fei Li, Stanford",,,Learning representations for pile manipulation
48,8,318,Few-shot Adaptation for Manipulating Granular Materials Under Domain Shift,"Autonomous lander missions on extraterrestrial bodies will need to sample granular material while coping with domain shift, no matter how well a sampling strategy is tuned on Earth. This paper proposes an adaptive scooping strategy that uses deep Gaussian process method trained with meta-learning to learn on-line from very limited experience on the target terrains. It introduces a novel meta-training approach, Deep Meta-Learning with Controlled Deployment Gaps (CoDeGa), that explicitly trains the deep kernel to predict scooping volume robustly under large domain shifts. Employed in a Bayesian Optimization sequential decision-making framework, the proposed method allows the robot to use vision and very little on-line experience to achieve high-quality scooping actions on out-of-distribution terrains, significantly outperforming non-adaptive methods proposed in the excavation literature as well as other state-of-the-art meta-learning methods. Moreover, a dataset of 6,700 executed scoops collected on a diverse set of materials, terrain topography, and compositions is made available for future research in granular material manipulation and meta-learning.",Yifan Zhu (University of Illinois at Urbana-Champaign)*; Pranay Thangeda (University of Illinois Urbana-Champaign); Melkior Ornik (University of Illinois at Urbana-Champaign); Kris Hauser (University of Illinois),yifanzhu95@gmail.com; pranayt2@illinois.edu; mornik@illinois.edu; kkhauser@illinois.edu,In person,,,Field Robotics,Grasping & Manipulation,"Kris Hauser, UIUC",,,"Scooping granular material, adaptation of learned strategies"
,,,,,,,,,,,,,,,
,,,,7. Mobile Manipulation and Locomotion,,,,,,,,,,,
49,1,260,Causal Policy Gradient for Whole-Body Mobile Manipulation,"Developing the next generation of household robot helpers requires combining locomotion and interaction capabilities, which is generally referred to as mobile manipulation (MoMa). MoMa tasks are difficult due to the large action space of the robot and the common multi-objective nature of the task, e.g., efficiently reaching a goal while avoiding obstacles. Current approaches often segregate tasks into navigation without manipulation and stationary manipulation without locomotion
by manually matching parts of the action space to MoMa sub-objectives (e.g. base actions for locomotion objectives and arm actions for manipulation). This solution prevents simultaneous combinations of locomotion and interaction degrees of freedom and requires human domain knowledge for both partitioning the action space and matching the action parts to the sub-objectives. In this paper, we introduce Causal MoMa, a new framework to train policies for typical MoMa tasks that makes use of the most favorable subspace of the robot’s action space to address each sub-objective. Causal MoMa automatically discovers the causal dependencies between actions and terms of the reward function and exploits these dependencies in a causal policy learning procedure that reduces gradient variance compared to previous state-of-the-art policy gradient algorithms, improving convergence and results. We evaluate the performance of Causal MoMa on three types of simulated robots across different MoMa tasks and demonstrate success in transferring the policies trained in simulation directly to a real robot, where our agent is able to follow moving goals and react to dynamic obstacles while simultaneously and synergistically controlling the whole-body: base, arm, and head. More information at https://sites.google.com/view/causal-moma",Jiaheng Hu (UT Austin)*; Peter Stone (University of Texas at Austin and Sony AI); Roberto Martín-Martín (University of Texas at Austin),jiahengh@utexas.edu; pstone@cs.utexas.edu; robertomm@cs.utexas.edu,In person,,,Robot Learning,"Assistive, Entertainment and Service Robots","Peter Stone, Martin-Martin, UT Austin",,,Learning for mobile manipulation
50,2,262,Centralized Model Predictive Control for Collaborative Loco-Manipulation,"In this work, we extend the model predictive control methods developed in the legged robotics literature to collaborative loco-manipulation settings. The systems we study entail a payload collectively carried by multiple quadruped robots equipped with a mechanical arm. We use a direct multiple shooting method to solve the resulting high-dimensional, optimal control problems for trajectories of ground reaction forces, manipulation wrenches, and stepping locations. To capture the dominant dynamics of the system, we model each agent and the shared payload as single rigid bodies. We demonstrate the versatility of our framework in a series of simulation experiments involving collaborative manipulation over challenging terrains.",Flavio De Vincenti (ETH Zurich)*; Stelian Coros (ETH Zurich),flavio.devincenti@inf.ethz.ch; scoros@inf.ethz.ch,In person,,,Robot Planning,Humanoid & Walking Robots; Multi-Robot & Networked Systems,"Stelian Coros, ETH",,,MPC for locomotion+manipulation
51,3,333,Learning and Adapting Agile Locomotion Skills by Transferring Experience,"Legged robots have enormous potential in their range of capabilities, from navigating unstructured terrains to high-speed running. However, these capabilities bring with them difficult control problems, and designing controllers for highly agile dynamic motions remains a substantial challenge for roboticists. Reinforcement learning (RL) offers a promising data-driven approach for automatically training such controllers. However, exploration in these high-dimensional, underactuated systems remains a significant hurdle for enabling legged robots to learn performant, naturalistic, and versatile agility skills. We propose a framework for training complex robotic skills by transferring experience from existing controllers to jumpstart learning new tasks. To leverage controllers we can acquire in practice, we design this framework to be flexible in terms of their source---that is, the controllers may have been optimized for a different objective under different dynamics, or may require different knowledge of the surroundings---and thus may be highly suboptimal for the target task. We show that our method enables learning complex agile jumping behaviors, navigating to goal locations while walking on hind legs, and adapting to new environments. We also demonstrate that the agile behaviors learned in this way are graceful and safe enough to deploy in the real world.","Laura M Smith (UC Berkeley)*; J. Chase Kew (Google Brain); Tianyu Li (Facebook AI Research); Linda Luu (Google); Xue Bin Peng (""University of California, Berkeley""); Sehoon Ha (Georgia Institute of Technology); Jie Tan (Google); Sergey Levine (UC Berkeley)",smithlaura@berkeley.edu; jkew@google.com; tli471@gatech.edu; luulinda@google.com; jasonpeng142@hotmail.com; sehoonha@gatech.edu; jietan@google.com; svlevine@eecs.berkeley.edu,In person,,BEST PAPER,Robot Learning,Humanoid & Walking Robots,"Levine, Berkely + Sehoon, GT + Facebook",,,Transfer of skills for locomotion
52,4,269,Robust and Versatile Bipedal Jumping Control through Reinforcement Learning,"This work aims to push the limits of agility for bipedal robots by enabling a torque-controlled bipedal robot to perform robust and versatile dynamic jumps in the real world. We present a reinforcement learning framework for training a robot to accomplish a large variety of jumping tasks, such as jumping to different locations and directions. To improve performance on these challenging tasks, we develop a new policy structure that encodes the robot’s long-term input/output (I/O) history while also providing direct access to a short-term I/O history. In order to train a versatile jumping policy, we utilize a multi-stage training scheme that includes different training stages for different objectives. After multi-stage training, the policy can be directly transferred to a real bipedal Cassie robot. Training on different tasks and exploring more diverse scenarios lead to highly robust policies that can exploit the diverse set of learned maneuvers to recover from perturbations or poor landings during real-world deployment. Such robustness in the proposed policy enables Cassie to succeed in completing a variety of challenging jump tasks in the real world, such as standing long jumps, jumping onto elevated platforms, and multi-axes jumps.","Zhongyu Li (UC Berkeley)*; Xue Bin Peng (Simon Fraser University); Pieter Abbeel (UC Berkeley); Sergey Levine (UC Berkeley); Glen Berseth (Université de Montréal, Mila); Koushil Sreenath (Berkeley)",zhongyu_li@berkeley.edu; xbpeng@sfu.ca; pabbeel@berkeley.edu; svlevine@eecs.berkeley.edu; glen.berseth@mila.quebec; koushils@berkeley.edu,In person,,,Humanoid & Walking Robots,Robot Learning,"Levine, Sreenath, Berkeley",,,Jumping for Cassie robot
53,5,148,On discrete symmetries of robotics systems: A group-theoretic and data-driven analysis,"We present a comprehensive study on discrete morphological symmetries of dynamical systems, which are commonly observed in biological and artificial locomoting systems, such as legged, swimming, and flying animals/robots/virtual characters. These symmetries arise from the presence of one or more planes/axis of symmetry in the system's morphology, resulting in harmonious duplication and distribution of body parts. Significantly, we characterize how morphological symmetries extend to symmetries in the system's dynamics, optimal control policies, and in all proprioceptive and exteroceptive measurements related to the system's dynamics evolution. In the context of data-driven methods, symmetry represents an inductive bias that justifies the use of data augmentation or symmetric function approximators. To tackle this, we present a theoretical and practical framework for identifying the system's morphological symmetry group $\G$ and characterizing the symmetries in proprioceptive and exteroceptive data measurements. We then exploit these symmetries using data augmentation and $\G$-equivariant neural networks. Our experiments on both synthetic and real-world applications provide empirical evidence of the advantageous outcomes resulting from the exploitation of these symmetries, including improved sample efficiency, enhanced generalization, and reduction of trainable parameters.","Daniel F Ordonez-Apraez (Italian Institut of Technology )*; Martin, Mario (); Antonio Agudo (Institut de Robotica i Informatica Industrial, CSIC-UPC); Francesc Moreno (IRI)",daniel.ordonez@iit.it; mmartin@cs.upc.edu; aagudo@iri.upc.edu; fmoreno@iri.upc.edu,In person,,,Formal Methods for Robotics,Robot Learning,"Moreno, IRI, Spain",,,symmetries in locomotion dynamics
54,6,181,Fast Traversability Estimation for Wild Visual Navigation,"Natural environments such as forests and grasslands are challenging for robotic navigation because of the false perception of rigid obstacles from high grass, twigs, or bushes. In this work, we propose Wild Visual Navigation (WVN), an online self-supervised learning system for traversability estimation which uses only vision. The system is able to continuously adapt from a short human demonstration in the field. It leverages high-dimensional features from self-supervised visual transformer models, with an online scheme for supervision generation that runs in real-time on the robot. We demonstrate the advantages of our approach with experiments and ablation studies in challenging environments in forests, parks, and grasslands. Our system is able to bootstrap the traversable terrain segmentation in less than 5 min of in-field training time, enabling the robot to navigate in complex outdoor terrains - negotiating obstacles in high grass as well as a 1.4 km footpath following. While our experiments were executed with a quadruped robot, ANYmal, the approach presented can generalize to any ground robot. Project page: https://bit.ly/3M6nMHH",Jonas Frey (ETH Zurich); Matias Mattamala (University of Oxford)*; Nived Chebrolu (University of Oxford); Cesar Cadena (ETH Zurich); Maurice Fallon (University of Oxford); Marco Hutter (ETHZ),jonfrey@ethz.ch; matias@robots.ox.ac.uk; nived@robots.ox.ac.uk; cesarc@ethz.ch; mfallon@robots.ox.ac.uk; mahutter@ethz.ch,In person,,,Autonomous Vehicle Navigation & Mobile Robots,Field Robotics; Robot Learning,"Marco Hutter, ETH + Maurice, Oxford",,,learning-based visual navigation for field robotics
55,7,265,Demonstrating Mobile Manipulation in the Wild: A Metrics-Driven Approach,"We present our general-purpose mobile manipulation system consisting of a custom robot platform and key algorithms spanning perception and planning. To extensively test the system in the wild and benchmark its performance, we choose a grocery shopping scenario in an actual, unmodified grocery store. We derive key performance metrics from detailed robot log data collected during six week-long field tests, spread across 18 months. These objective metrics, gained from complex yet repeatable tests, drive the direction of our research efforts and let us continuously improve our system’s performance. We find that thorough end-to-end system-level testing of a complex mobile manipulation system can serve as a reality-check for state-of-the-art methods in robotics. This effectively grounds robotics research efforts in real world needs and challenges, which we deem highly useful for the advancement of the field. To this end, we share our key insights and takeaways to inspire and accelerate similar system-level research projects.",Max Bajracharya (Toyota Research Institute); James Borders (Toyota Research Institute); Richard Cheng (Toyota Research Institute); Dan Helmick (TRI); Lukas Kaul (Toyota Research Institute)*; Dan Kruse (Toyota Research Institute); John Leichty (Toyota Research Institute); Jeremy Ma (); Carolyn Matl (Toyota Research Institute); Frank Michel (Toyota Research Institute); Chavdar Papazov (TRI); Josh Petersen (Amazon); Krishna Shankar (Apple); Mark Tjersland (Toyota Research Institute),max.bajracharya@tri.global; james.borders@tri.global; richard.cheng@tri.global; dan.helmick@tri.global; lukas.kaul@tri.global; dan.kruse@tri.global; john.leichty@tri.global; jeremy.ma@gmail.com; carolyn.matl@tri.global; frank.michel@tri.global; chavdar.papazov@tri.global; joshpetersen1231@gmail.com; krishnashankar@gmail.com; mark.tjersland@tri.global,In person,DEMO,BEST DEMO,Field Robotics,"Grasping & Manipulation; Robot Perception, Sensors & Vision",TRI,,,Real-world study of mobile manipulation deployment
56,8,195,Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,"Deep reinforcement learning is a promising approach to learning policies in unstructured environments. Due to its sample inefficiency, though, deep RL applications have primarily focused on simulated environments. In this work, we demonstrate that the recent advancements in machine learning algorithms and libraries combined with careful MDP formulation lead to learning quadruped locomotion in only 20 minutes in the real world. We evaluate our approach on several indoor and outdoor terrains that are known to be challenging for classical, model-based controllers and observe that the robot consistently learns a walking gait on all of these terrains. Finally, we evaluate our design decisions in a simulated environment.",Ilya Kostrikov (UC Berkeley)*; Laura M Smith (UC Berkeley); Sergey Levine (UC Berkeley),kostrikov@berkeley.edu; smithlaura@berkeley.edu; svlevine@eecs.berkeley.edu,In person,DEMO,,Robot Learning,,"Levine, Berkeley",,,Real-world RL for walking
,,,,,,,,,,,,,,,
,,,,8. Robot Planning,,,,,,,,,,,
57,1,208,Non-Euclidean Motion Planning with Graphs of Geodesically-Convex Sets,"Computing optimal, collision-free trajectories for high-dimensional systems is a challenging problem. Sampling-based planners struggle with the dimensionality, whereas trajectory optimizers may get stuck in local minima due to inherent nonconvexities in the optimization landscape. The use of mixed-integer programming to encapsulate these nonconvexities and find globally optimal trajectories has recently shown great promise, thanks in part to tight convex relaxations and efficient approximation strategies that greatly reduce runtimes. These approaches were previously limited to Euclidean configuration spaces, precluding their use with mobile bases or continuous revolute joints. In this paper, we handle such scenarios by modeling configuration spaces as Riemannian manifolds, and we describe a reduction procedure for the zero-curvature case to a mixed-integer convex optimization problem. We demonstrate our results on various robot platforms, including producing efficient collision-free trajectories for a PR2 bimanual mobile manipulator.",Thomas B Cohn (Massachusetts Institute of Technology)*; Mark Petersen (Harvard University); Max Simchowitz (MIT); Russ Tedrake (MIT),tcohn@mit.edu; markpetersen@g.harvard.edu; msimchow@mit.edu; russt@mit.edu,In person,,BEST PAPER,Robot Planning,,"Tedrake, MIT",,,mixed-integer convex optimization for planning in non-euclidean spaces
58,2,337,Convex Geometric Motion Planning on Lie Groups via Moment Relaxation,"This paper reports a novel result: with proper robot models on matrix Lie groups, one can formulate the kinodynamic motion planning problem for rigid body systems as \emph{exact} polynomial optimization problems that can be relaxed as semidefinite programming (SDP). Due to the nonlinear rigid body dynamics, the motion planning problem for rigid body systems is nonconvex. Existing global optimization-based methods do not properly deal with the configuration space of the 3D rigid body; thus, they do not scale well to long-horizon planning problems. We use Lie groups as the configuration space in our formulation and apply the variational integrator to formulate the forced rigid body systems as quadratic polynomials. Then we leverage Lasserre's hierarchy to obtain the globally optimal solution via SDP. By constructing the motion planning problem in a sparse manner, the results show that the proposed algorithm has \emph{linear} complexity with respect to the planning horizon. This paper demonstrates the proposed method can provide rank-one optimal solutions at relaxation order two for most of the testing cases of 1) 3D drone landing using the full dynamics model and 2) inverse kinematics for serial manipulators. ","Sangli Teng (University of Michigan, Ann Arbor)*; Ashkan Jasour (NASA JPL); Ram Vasudevan (University of Michigan); Maani Ghaffari Jadidi (Univ. of Michigan)",sanglit@umich.edu; jasour@csail.mit.edu; ramv@umich.edu; maanigj@umich.edu,In person,,BEST STUDENT,Robot Planning,Control and Dynamics,"Ram, Michigan +JPL",,,exact polynomizal optimization for kinodynamic planning of rigid bodies
59,3,244,Time Optimal Ergodic Search,"Robots with the ability to balance time against the thoroughness of search have the potential to provide time-critical assistance in applications such as search and rescue. Current advances in ergodic coverage-based search methods have enabled robots to completely explore and search an area in a fixed amount of time. However, optimizing time against the quality of autonomous ergodic search has yet to be demonstrated. In this paper, we investigate solutions to the time-optimal ergodic search problem for fast and adaptive robotic search and exploration. We pose the problem as a minimum time problem with an ergodic inequality constraint whose upper bound regulates and balances the granularity of search against time. Solutions to the problem are presented analytically using Pontryagin's conditions of optimality and demonstrated numerically through a direct transcription optimization approach. We show the efficacy of the approach in generating time-optimal ergodic search trajectories in simulation and with drone experiments in a cluttered environment. Obstacle avoidance is shown to be readily integrated into our formulation, and we perform ablation studies that investigate parameter dependence on optimized time and trajectory sensitivity for search. ",Dayi E Dong (Yale University); Henry P Berger (Yale University); Ian Abraham (Yale University)*,dayiethan@gmail.com; henry.berger@yale.edu; ian.abraham@yale.edu,In person,,BEST PAPER,Control and Dynamics,Robot Planning,"Ian Abraham, Yale",,,Minimum time ergodic search
60,4,317,Motion Planning (In)feasibility Detection using a Prior Roadmap via Path and Cut Search,"Motion planning seeks a collision-free path in a configuration space (C-space), representing all possible robot configurations in the environment. As it is challenging to construct a C-space explicitly for a high-dimensional robot, we generally build a graph structure called a roadmap, a discrete approximation of a complex continuous C-space, to reason about connectivity. Checking collision-free connectivity in the roadmap requires expensive edge-evaluation computations, and thus, reducing the number of evaluations has become a significant research objective. However, in practice, we often face infeasible problems:  those in which there is no collision-free path in the roadmap between the start and the goal locations. Existing studies often overlook the possibility of infeasibility, becoming highly inefficient by performing many edge evaluations. 

In this work, we address this oversight in scenarios where a prior roadmap is available; that is, the edges of the roadmap contain the probability of being a collision-free edge learned from past experience. To this end, we propose an algorithm called iterative path and cut finding (IPC) that iteratively searches for a path and a cut in a prior roadmap to detect infeasibility while reducing expensive edge evaluations as much as possible. We further improve the efficiency of IPC by introducing a second algorithm, iterative decomposition and path and cut finding (IDPC), that leverages the fact that cut-finding algorithms partition the roadmap into smaller subgraphs. We analyze the theoretical properties of IPC and IDPC, such as completeness and computational complexity, and evaluate their performance in terms of completion time and the number of edge evaluations in large-scale simulations.",Yoonchang Sung (The University of Texas at Austin)*; Peter Stone (University of Texas at Austin and Sony AI),yooncs8@cs.utexas.edu; pstone@cs.utexas.edu,In person,,,Robot Planning,Robot Learning,"Peter Stone, UT Austin",,,detecting infeasibility in roadmap-based motion planning
61,5,319,Sequence-Based Plan Feasibility Prediction for Efficient Task and Motion Planning,"We present a learning-enabled Task and Motion Planning (TAMP) algorithm for solving mobile manipulation problems in environments with many articulated and movable obstacles. Our idea is to bias the search procedure of a traditional TAMP planner with a learned plan feasibility predictor. The core of our algorithm is PIGINet, a novel Transformer-based learning method that takes in a task plan, the goal, and the initial state, and predicts the probability of finding motion trajectories associated with the task plan. We integrate PIGINet within a TAMP planner that generates a diverse set of high-level task plans, sorts them by their predicted likelihood of feasibility, and refines them in that order. We evaluate the runtime of our TAMP algorithm on seven families of kitchen rearrangement problems, comparing its performance to that of non-learning baselines. Our experiments show that PIGINet substantially improves planning efficiency, cutting down runtime by 80% on problems with small state spaces and 10%-50% on larger ones, after being trained on only 150-600 problems. Finally, it also achieves zero-shot generalization to problems with unseen object categories thanks to its visual encoding of objects. ",Zhutian  Yang (MIT)*; Caelan R Garrett (NVIDIA Research); Tomas Lozano-Perez (MIT); Leslie Kaelbling (MIT); Dieter Fox (NVIDIA),ztyang@mit.edu; cgarrett@nvidia.com; tlp@csail.mit.edu; lpk@csail.mit.edu; dieterf@nvidia.com,In person,,,Robot Planning,Cognitive Modeling & Knowledge Representation for Robots; Robot Learning,"Tomas, Leslie, MIT + Fox, NVIDIA",,,Speeding up manipulationTAMP via learning
62,6,357,Reachability-based Trajectory Design with Neural Implicit Safety Constraints,"Generating safe motion plans in real-time is a key requirement for deploying robot manipulators to assist humans in collaborative settings.
In particular, robots must satisfy strict safety requirements to avoid damaging itself or harming nearby humans.
This is particularly challenging if the robot must also operate in real-time to quickly adjust to changes in its environment.
This paper addresses these challenges by proposing Reachability-based Signed Distance Functions (RDFs) as a neural implicit representation for robot safety.
RDF, trained using supervised learning, accurately predicts the distance between the swept volume of a robot arm and an obstacle.
RDF's inference and gradient computations are fast and scale linearly with the dimension of the system; these features enables its use within a novel real-time trajectory planning framework as a continuous-time collision-avoidance constraint.
The planning method here is compared to state-of-the-art methods and is demonstrated to successfully solve challenging motion planning tasks for high-dimensional systems under a limited planning time horizon.",Jonathan B Michaux (University of Michigan)*; Yong Seok Kwon (University of Michigan); Qingyi Chen (University of Michigan); Ram Vasudevan (University of Michigan),jmichaux@umich.edu; kwonys@umich.edu; chenqy@umich.edu; ramv@umich.edu,In person,,,Control and Dynamics,Grasping & Manipulation; Robot Planning,"Ram, Michigan",,,Reachability SDFs for continuous collision avoidance
63,,199,Progressive Learning for Physics-informed Neural Motion Planning,"Motion planning (MP) is one of the core robotics problems requiring fast methods for finding a collision-free robot motion path connecting the given start and goal states. Neural motion planners (NMPs) demonstrate fast computational speed in finding path solutions but require a huge amount of expert trajectories for learning, thus adding a significant training computational load. In contrast, recent advancements have also led to a physics-informed NMP approach that directly solves the Eikonal equation for motion planning and does not require expert demonstrations for learning. However, experiments show that the physics-informed NMP approach performs poorly in complex environments and lacks scalability in multiple scenarios and high-dimensional real robot settings. To overcome these limitations, this paper presents a novel and tractable Eikonal equation formulation and introduces a new progressive learning strategy to train neural networks without expert data in complex, cluttered, multiple high-dimensional robot motion planning scenarios. The results demonstrate that our method outperforms state-of-the-art traditional MP, data-driven NMP, and physics-informed NMP methods by a significant margin in terms of computational planning speed, path quality, and success rates. We also show that our approach scales to multiple complex, cluttered scenarios and the real robot set up in a narrow passage environment. The proposed method's videos and code implementations are available at https://github.com/ruiqini/P-NTFields.",Ruiqi Ni (Purdue University)*; Ahmed H Qureshi (Purdue University),ni117@purdue.edu; ahqureshi@purdue.edu,In person,,,Robot Learning,Robot Planning,"Qureshi, Purdue",,,neural motion planning
64,,80,iPlanner: Imperative Path Planning,"The problem of path planning has been studied for years. Classic planning pipelines, including perception, mapping, and path searching, can result in latency and compounding errors between modules. While recent studies have demonstrated the effectiveness of end-to-end learning methods in achieving high planning efficiency, these methods often struggle to match the generalization abilities of classic approaches in handling different environments. Moreover, end-to-end training of policies often requires a large number of labeled data or training iterations to reach convergence. In this paper, we present a novel Imperative Learning (IL) approach. This approach leverages a differentiable cost map to provide implicit supervision during policy training, eliminating the need for demonstrations or labeled trajectories. Furthermore, the policy training adopts a Bi-Level Optimization (BLO) process, which combines network update and metric-based trajectory optimization, to generate a smooth and collision-free path toward the goal based on a single depth measurement. The proposed method allows task-level costs of predicted trajectories to be backpropagated through all components to update the network through direct gradient descent. In our experiments, the method demonstrates around 4x faster planning than the classic approach and robustness against localization noise. Additionally, the IL approach enables the planner to generalize to various unseen environments, resulting in an overall 26-87% improvement in SPL performance compared to baseline learning methods.",Fan Yang (ETH Zurich)*; Chen Wang (State University of New York at Buffalo); Cesar Cadena (ETH Zurich); Marco Hutter (ETHZ),fanyang1@ethz.ch; cwx@buffalo.edu; cesarc@ethz.ch; mahutter@ethz.ch,In person,,,Robot Planning,Autonomous Vehicle Navigation & Mobile Robots; Robot Learning,"Marco Hutter, ETH",,,differentiable optimization planner from sensing data
,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,
,,,,9. Robot State Estimation,,,,,,,,,,,
65,1,316,Efficient volumetric mapping of multi-scale environments using wavelet-based compression,"Volumetric maps are widely used in robotics due to their desirable properties in applications such as path planning, exploration, and manipulation. Constant advances in mapping technologies are needed to keep up with the improvements in sensor technology, generating increasingly vast amounts of precise measurements. Handling this data in a computationally and memory-efficient manner is paramount to representing the environment at the desired scales and resolutions. In this work, we express the desirable properties of a volumetric mapping framework through the lens of multi-resolution analysis. This shows that wavelets are a natural foundation for hierarchical and multi-resolution volumetric mapping. Based on this insight we design an efficient mapping system that uses wavelet decomposition. The efficiency of the system enables the use of uncertainty-aware sensor models, improving the quality of the maps. Experiments on both synthetic and real-world data provide mapping accuracy and runtime performance comparisons with state-of-the-art methods on both RGB-D and 3D LiDAR data. The framework is open-sourced to allow the robotics community at large to explore this approach.","Victor Reijgwart (Autonomous Systems Lab, ETH Zurich)*; Cesar Cadena (ETH Zurich); Roland Siegwart (ETH Zürich, Autonomous Systems Lab); Lionel Ott (The University of Sydney)",victorr@ethz.ch; cesarc@ethz.ch; rsiegwart@ethz.ch; lioott@ethz.ch,In person,,,"Robot State Estimation, Localization & Mapping","Robot Perception, Sensors & Vision","Siegwart, ETH",,,Wavelets for volumetric mapping
66,2,98,ConceptFusion: Open-set multimodal 3D mapping,"Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason about a finite set of concepts, pre-defined at training time. To address this issue, we propose ConceptFusion, a scene representation that is: (i) fundamentally open-set, enabling reasoning beyond a closed set of concepts (ii) inherently multi-modal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today’s foundation models that have been pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40% margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping.",Krishna Murthy Jatavallabhula (MIT)*; Alihusein Kuwajerwala (Université de Montréal ); Qiao Gu (University of Toronto); Mohd Omama (IIIT Hyderabad); Ganesh Iyer (Amazon Lab126); Soroush Saryazdi (Concordia University); Tao Chen (MIT); Alaa Maalouf (MIT); Shuang Li (MIT); Nikhil Varma Keetha (Carnegie Mellon University); Ayush Tewari (MIT); Joshua Tenenbaum (MIT); Celso de Melo (DEVCOM Army Research Laboratory); Madhava Krishna (IIIT-Hyderabad); Liam Paull (Université de Montréal); Florian Shkurti (University of Toronto); Antonio Torralba (MIT),jkrishna@mit.edu; alihusein.kuwajerwala@umontreal.ca; q.gu@mail.utoronto.ca; mohd.omama@gmail.com; giyer2309@gmail.com; soroush.saryazdi@gmail.com; taochen@mit.edu; alaam@mit.edu; lishuang@mit.edu; keethanikhil@gmail.com; ayusht@mit.edu; jbt@mit.edu; celso.miguel.de.melo@gmail.com; mkrishna@iiit.ac.in; paulll@iro.umontreal.ca; florian@cs.toronto.edu; torralba@mit.edu,In person,,,"Robot State Estimation, Localization & Mapping","Robot Learning; Robot Perception, Sensors & Vision","MIT, Toronto, Montréal",,,Cross-modality scene representation for 3D mapping using foundation models
67,3,9,ERASOR2: Instance-Aware Robust 3D Mapping of the Static World in Dynamic Scenes,"A map of the environment is an essential component for robotic navigation. In the majority of cases, a map of the static part of the world is the basis for localization, planning, and navigation. However, dynamic objects that are presented in the scenes during mapping leave undesirable traces in the map, which can impede mobile robots from achieving successful robotic navigation. To remove the artifacts caused by dynamic objects in the map, we propose a novel instance-aware map building method. Our approach rejects dynamic points at an instance-level while preserving most static points by exploiting instance segmentation estimates. Furthermore, we propose effective ways to consider the erroneous estimates of instance segmentation, enabling our proposed method to be robust even under imprecise instance segmentation. As demonstrated in our experimental evaluation, our approach shows substantial performance increases in terms of both, the preservation of static points and rejection of dynamic points. Our code will be made available on publication.","Hyungtae Lim (KAIST); Lucas Nunes (University of Bonn); Benedikt Mersch (University of Bonn); Xieyuanli Chen ( the College of Intelligence Science and Technology, National University of Defense Technology); Jens Behley (University of Bonn); Hyun Myung (KAIST)*; Cyrill Stachniss (University of Bonn)",shapelim@kaist.ac.kr; lucas.nunes@uni-bonn.de; mersch@igg.uni-bonn.de; xieyuanli.chen@nudt.edu.cn; jens.behley@igg.uni-bonn.de; hmyung@kaist.ac.kr; cyrill.stachniss@igg.uni-bonn.de,In person,,,"Robot State Estimation, Localization & Mapping","Robot Perception, Sensors & Vision","Cyrill Stachniss, Bonn + KAIST",,,Object-aware map building for dynamics scenes
68,4,130,NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects,"We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and illustrate how it supports object SLAM for consistent spatial understanding with longterm scene changes. NeuSE is a set of latent object embeddings created from partial object observations. It serves as a compact point cloud surrogate for complete object models, encoding full shape information while transforming SE(3)-equivariantly in tandem with the object in the physical world. With NeuSE, relative frame transforms can be directly derived from inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape and pose characterization, can operate independently or in conjunction with typical SLAM systems. It directly infers SE(3) camera pose constraints that are compatible with general SLAM pose graph optimization, while also maintaining a lightweight object-centric map that adapts to real-world changes. Our approach is evaluated on synthetic and real-world sequences featuring changed objects and shows improved localization accuracy and change-aware mapping capability, when working either standalone or jointly with a common SLAM pipeline.",Jiahui Fu (Massachusetts Institute of Technology)*; Yilun Du (MIT); Kurran Singh (MIT); Joshua Tenenbaum (MIT); John Leonard (MIT),jiahuifu@mit.edu; yilundu@mit.edu; singhk@mit.edu; jbt@mit.edu; jleonard@mit.edu,In person,,,"Robot State Estimation, Localization & Mapping",Autonomous Vehicle Navigation & Mobile Robots; Robot Learning,"Tenenbaum, Leonard MIT",,,Object-aware SLAM with scene changes
69,5,31,POV-SLAM: Probabilistic Object-Aware Variational SLAM in Semi-Static Environments,"Simultaneous localization and mapping (SLAM) in slowly varying scenes is important for long-term robot task completion in GPS-denied environments. Failing to detect scene changes may lead to inaccurate maps and, ultimately, lost robots. Classical SLAM algorithms assume static scenes, and recent works take dynamics into account, but require scene changes to be observed in consecutive frames. Semi-static scenes, wherein objects appear, disappear, or move slowly over time, are often overlooked, yet are critical for long-term operation. We propose an object-aware, factor-graph SLAM framework that tracks and reconstructs semi-static object-level changes. Our novel variational expectation-maximization strategy is used to optimize factor graphs involving a Gaussian-Uniform bimodal measurement likelihood for potentially-changing objects. We evaluate our approach alongside the state-of-the-art SLAM solutions in simulation and on our novel real-world SLAM dataset captured in a warehouse over four months. Our method improves the robustness of localization in the presence of semi-static changes, providing object-level reasoning about the scene.","Jingxing Qian (University of Toronto)*; Veronica Chatrath (Technical University of Munich); James Servos (Clearpath Robotics); Aaron Mavrinac (Clearpath Robotics); Wolfram Burgard (University of Technology, Nuremberg); Steven L Waslander (University of Toronto); Angela Schoellig (Technical University of Munich)",jingxing.qian@mail.utoronto.ca; veronica.chatrath@mail.utoronto.ca; jservos@clearpath.ai; amavrinac@clearpath.ai; wolfram.burgard@utn.de; steven.waslander@robotics.utias.utoronto.ca; angela.schoellig@tum.de,In person,,,"Robot State Estimation, Localization & Mapping","Robot Perception, Sensors & Vision","Burgard, Schoelling, TU Munich + Waslander",,,SLAM for changing scenes that is object aware - factor graphs
70,6,97,InstaLoc: One-shot Global Lidar Localisation in Indoor Environments through Instance Learning,"Localization for autonomous robots in prior maps is crucial for their functionality. This paper offers a solution to this problem for indoor environments called InstaLoc, which operates on an individual lidar scan to localize it within a prior map. We draw on inspiration from how humans navigate and position themselves by recognizing the layout of distinctive objects and structures. Mimicking the human approach, InstaLoc identifies and matches object instances in the scene with those from a prior map. As far as we know, this is the first method to use panoptic segmentation directly inferring on 3D lidar scans for indoor localization. InstaLoc operates through two networks based on spatially sparse tensors to directly infer dense 3D lidar point clouds. The first network is a panoptic segmentation network that produces object instances and their semantic classes. The second smaller network produces a descriptor for each object instance. A consensus based matching algorithm then matches the instances to the prior map and estimates a six degrees of freedom (DoF) pose for the input cloud in the prior map. InstaLoc utilizes two efficient networks, requires only one to two hours of training on a mobile GPU, and runs in real-time at 1 Hz. Our method achieves between two and four times more detections when localizing, as compared to baseline methods, and achieves higher precision on these detections. ","Lintong Zhang (University of Oxford)*; Sundara Tejaswi Digumarti (Oxford Robotics Institute, University of Oxford); Georgi Tinchev (Amazon); Maurice Fallon (University of Oxford)",lintong@robots.ox.ac.uk; tejaswid@oxfordrobotics.institute; georgi@tinchev.net; mfallon@robots.ox.ac.uk,In person,,,"Robot State Estimation, Localization & Mapping",Robot Learning,"Maurice Fallon, Oxford",,,Learning-based Lidar-based Localization via panoptic segmentation
71,7,198,HDVIO: Improving Localization and Disturbance Estimation with Hybrid Dynamics VIO,"Visual-inertial odometry (VIO) is the most common approach for estimating the state of autonomous micro aerial vehicles using only onboard sensors. Existing methods improve VIO performance by including a dynamics model in the estimation pipeline. However, such methods degrade in the presence of low-fidelity vehicle models and continuous external disturbances, such as wind. Our proposed method, HDVIO, overcomes these limitations by using a hybrid dynamics model that combines a point-mass vehicle model with a learning-based component that captures complex aerodynamic effects. HDVIO estimates the external force and the full robot state by leveraging the discrepancy between the actual motion and the predicted motion of the hybrid dynamics model. Our hybrid dynamics model uses a history of thrust and IMU measurements to predict the vehicle dynamics. To demonstrate the performance of our method, we present results on both public and novel drone dynamics datasets and show real-world experiments of a quadrotor flying in strong winds up to 25 km/h. The results show that our approach improves the motion and external force estimation compared to the state-of-the-art by up to 33% and 40%, respectively. Furthermore, differently from existing methods, we show that it is possible to predict the vehicle dynamics accurately while having no explicit knowledge of its full state.","Giovanni Cioffi (University of Zurich)*; Leonard Bauersfeld (ETH Zürich); Davide Scaramuzza (University of Zurich & ETH Zurich, Switzerland)",cioffi@ifi.uzh.ch; leonard.bauersfeld@gmail.com; sdavide@ifi.uzh.ch,In person,,,"Robot State Estimation, Localization & Mapping",Autonomous Vehicle Navigation & Mobile Robots,"Scaramuzza, ETH",,,Learning-based dynamics model for visual inertial odometry/localization of aerial vehicles
72,8,362,Fast Monocular Visual-Inertial Initialization Leveraging Learned Single-View Depth,"In monocular visual-inertial navigation systems, it is
ideal to initialize as quickly and robustly as possible. State-of-the-
art initialization methods typically make linear approximations
using the image features and inertial information in order
to initialize in closed-form, and then refine the states with a
nonlinear optimization. While the standard methods typically
wait for a 2sec data window, a recent work has shown that it
is possible to initialize faster (0.5sec) by adding constraints from
a robust but only up-to-scale monocular depth network in the
nonlinear optimization. To further expedite the initialization, in
this work, we leverage the scale-less depth measurements instead
in the linear initialization step that is performed prior to the
nonlinear one, which only requires a single depth image for
the first frame. We show that the typical estimation of each
feature state independently in the closed-form solution can be
replaced by just estimating the scale and offset parameters of
the learned depth map. Interestingly, our formulation makes it
possible to construct small minimal problems in a RANSAC loop,
whereas the typical linear system’s minimal problem is quite
large and includes every feature state. Experiments show that
our method can improve the overall initialization performance on
popular public datasets (EuRoC MAV and TUM-VI) over state-
of-the-art methods. For the TUM-VI dataset, we show superior
initialization performance with only a 300ms window of data,
which is the smallest ever reported, and show that our method
can initialize more often, robustly, and accurately in different
challenging scenarios.",Nathaniel  W Merrill (University of Delaware)*; Patrick Geneva (University of Delaware); Saimouli Katragadda (University of Delaware); Chuchu Chen (University of Delaware); Guoquan  Huang (University of Delaware),nmerrill@udel.edu; pgeneva@udel.edu; saimouli@udel.edu; ccchu@udel.edu; ghuang@udel.edu,In person,,BEST STUDENT,"Robot State Estimation, Localization & Mapping",Autonomous Vehicle Navigation & Mobile Robots,"Huang, Delaware",,,Initialization for visual inertial navigation from learned depth
,,,,,,,,,,,,,,,
,,,,10. Robot Perception,,,,,,,,,,,
73,1,230,CoDEPS: Online Continual Learning for Depth Estimation and Panoptic Segmentation,"Operating a robot in the open world requires a high level of robustness with respect to previously unseen environments. Optimally, the robot is able to adapt by itself to new conditions without human supervision, e.g., automatically adjusting its perception system to changing lighting conditions. In this work, we address the task of continual learning for deep learning-based monocular depth estimation and panoptic segmentation in new environments in an online manner. We introduce CoDEPS to perform continual learning involving multiple real-world domains while mitigating catastrophic forgetting by leveraging experience replay. In particular, we propose a novel domain-mixing strategy to generate pseudo-labels to adapt panoptic segmentation. Furthermore, we explicitly address the limited storage capacity of robotic systems by leveraging sampling strategies for constructing a fixed-size replay buffer based on rare semantic class sampling and image diversity. We perform extensive evaluations of CoDEPS on various real-world datasets demonstrating that it successfully adapts to unseen environments without sacrificing performance on previous domains while achieving state-of-the-art results. The code of our work is publicly available at http://codeps.cs.uni-freiburg.de.","Niclas Vödisch (University of Freiburg); Kürsat Petek (University of Freiburg)*; Wolfram Burgard (University of Technology, Nuremberg); Abhinav Valada (University of Freiburg)",voedisch@cs.uni-freiburg.de; petek@informatik.uni-freiburg.de; wolfram.burgard@utn.de; valada@cs.uni-freiburg.de,In person,,,"Robot Perception, Sensors & Vision",Robot Learning,"Burgard, Valada, Freiburg",,,Generate pseudo-labels for learning-based monocular depth estimation and panoptic segmentation in new environments
74,2,16,CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory,"We propose CLIP-Fields, an implicit scene model that can be used for a variety of tasks, such as segmentation, instance identification, semantic search over space, and view localization. CLIP-Fields learns a mapping from spatial locations to semantic embedding vectors. Importantly, we show that this mapping can be trained with supervision coming only from web-image and web-text trained models such as CLIP, Detic, and Sentence-BERT; and thus uses no direct human supervision. When compared to baselines like Mask-RCNN, our method outperforms on few-shot instance identification or semantic segmentation on the HM3D dataset with only a fraction of the examples. Finally, we show that using CLIP-Fields as a scene memory, robots can perform semantic navigation in real-world environments. Our code and demonstration videos are available here: https://clip-fields.github.io",Nur Muhammad (Mahi) Shafiullah (New York University)*; Chris Paxton (Meta AI); Lerrel Pinto (New York University); Soumith Chintala (Facebook); Arthur Szlam (Facebook),mahi@cs.nyu.edu; cpaxton@fb.com; lerrel@cs.nyu.edu; soumith@fb.com; aszlam@fb.com,In person,,,Robot Learning,"Robot Perception, Sensors & Vision","Lerrel Pinto, NYU + Facebook",,,CLIP features for navigation
75,3,143,How To Not Train Your Dragon: Training-free Embodied Object Goal Navigation with Semantic Frontiers,"Object goal navigation is an important problem in Embodied AI that involves guiding the agent to navigate to an instance of the object category in an unknown environment---typically an indoor scene. Unfortunately, current state-of-the-art methods for this problem rely heavily on data-driven approaches, e.g., end-to-end reinforcement learning, imitation learning, and others. Moreover, such methods are typically costly to train and difficult to debug, leading to a lack of transferability and explainability. Inspired by recent successes in combining classical and learning methods, we present a modular and training-free solution, which embraces more classic approaches, to tackle the object goal navigation problem. Our method builds a structured scene representation based on the classic visual simultaneous localization and mapping (V-SLAM) framework. We then inject semantics into geometric-based frontier exploration to reason about promising areas to search for a goal object. Our structured scene representation comprises a 2D occupancy map, semantic point cloud, and spatial scene graph. Our method propagates semantics on the scene graphs based on language priors and scene statistics to introduce semantic knowledge to the geometric frontiers. With injected semantic priors, the agent can reason about the most promising frontier to explore.  The proposed pipeline shows strong experimental performance for object goal navigation on the Gibson benchmark dataset, outperforming the previous state-of-the-art. We also perform comprehensive ablation studies to identify the current bottleneck in the object navigation task.",Junting Chen (ETH Zurich); Guohao Li (King Abdullah University of Science and Technology (KAUST)); Suryansh Kumar (ETH Zurich)*; Bernard Ghanem (KAUST); Fisher Yu (ETH Zurich),sgt79911030@gmail.com; guohao.li@kaust.edu.sa; k.sur46@gmail.com; Bernard.Ghanem@kaust.edu.sa; i@yf.io,In person,,,Robot Learning,"Robot Perception, Sensors & Vision",ETH Zurich,,,Semantic scene graphs for object-based goal navigation
76,4,119,A Correct-and-Certify Approach to Self-Supervise Object Pose Estimators via Ensemble Self-Training,"Real-world robotics applications demand object pose estimation methods that work reliably across a variety of scenarios. Modern learning-based approaches require large labeled datasets and tend to perform poorly outside the training domain. Our first contribution is to develop a robust corrector module that corrects pose estimates using depth information, thus enabling existing methods to better generalize to new test domains; the corrector operates on semantic keypoints (but is also applicable to other pose estimators) and is fully differentiable. Our second contribution is an ensemble self-training approach that simultaneously trains multiple pose estimators in a self-supervised manner. Our ensemble self-training architecture uses the robust corrector to refine the output of each pose estimator; then, it evaluates the quality of the outputs using observable correctness certificates; finally, it uses the observably correct outputs for further training, without requiring external supervision. As an additional contribution, we propose small improvements to a regression-based keypoint detection architecture, to enhance its robustness to outliers; these improvements include a robust pooling scheme and a robust centroid computation. Experiments on the YCBV and TLESS datasets show the proposed ensemble self-training performs on par or better than fully supervised baselines while not requiring 3D annotations on real data. ",Jingnan Shi (MIT)*; Rajat Talak (Massachusetts Institute of Technology); Dominic Maggio (Massachusetts Institute of Technology); Luca Carlone (Massachusetts Institute of Technology),jnshi@MIT.EDU; talak@mit.edu; drmaggio@mit.edu; lcarlone@mit.edu,In person,,,"Robot Perception, Sensors & Vision","Robot State Estimation, Localization & Mapping","Luca Carlone, MIT",,,Object Pose Estimation
77,5,367,CHSEL: Producing Diverse Plausible Pose Estimates from Contact and Free Space Data,"This paper proposes a novel method for estimating the set of plausible poses of a rigid object from a set of points with volumetric information, such as whether each point is in free space or on the surface of the object. In particular, we study how pose can be estimated
from force and tactile data arising from contact. 
Using data derived from contact is challenging because it is inherently less information-dense than visual data, and thus the pose estimation problem is severely under-constrained when there are few contacts. 
Rather than attempting to estimate the true pose of the object, which is not tractable without a large number of contacts, we seek to estimate a plausible set of poses which obey the constraints imposed by the sensor data. Existing methods struggle to estimate this set because they are either designed for single pose estimates or require informative priors to be effective. Our approach to this problem, Constrained pose Hypothesis Set Elimination (CHSEL), has three key attributes: 1) It considers volumetric information, which allows us to account for known free space; 2) It uses a novel differentiable volumetric cost function to take advantage of powerful gradient-based optimization tools; and 3) It uses methods from the Quality Diversity (QD) optimization literature to produce a diverse set of high-quality poses. To our knowledge, QD methods have not been used previously for pose registration. We also show how to update our plausible pose estimates online as more data is gathered by the robot. 
Our experiments suggest that CHSEL shows large performance improvements over several baseline methods for both simulated and real-world data.",Sheng Zhong (University of Michigan)*; Dmitry Berenson (University of Michigan); Nima Fazeli (Michigan),zhsh@umich.edu; dmitryb@umich.edu; nfz@umich.edu,In person,,,"Robot Perception, Sensors & Vision","Robot State Estimation, Localization & Mapping","Dmitry Berenson, Nima Fazeli, Michigan",,,Object Pose Estimates from Force and Tactile
78,6,386,MultiSCOPE: Disambiguating In-Hand Object Poses with Proprioception and Tactile Feedback,"In this paper, we propose a method for estimating in-hand object poses using proprioception and tactile feedback from a bimanual robotic system. Our method addresses the problem of reducing pose uncertainty through a sequence of frictional contact interactions between the grasped objects. As part of our method, we propose 1) a tool segmentation routine that facilitates contact location and object pose estimation, 2) a loss that allows reasoning over solution consistency between interactions, and 3) a loss to promote converging to object poses and contact locations that explain the external force-torque experienced by each arm. We demonstrate the efficacy of our method in a task-based demonstration both in simulation and on a real-world bimanual platform and show significant improvement in object pose estimation over single interactions.",Andrea Sipos (University of Michigan)*; Nima Fazeli (Michigan),asipos@umich.edu; nfz@umich.edu,In person,,,Grasping & Manipulation,"Robot State Estimation, Localization & Mapping","Nima Fazeli, Michigan",,,In hand Object pose estimation from tactile data
79,7,292,Tactile-Filter: Interactive Tactile Perception for Part Mating,"Humans rely on touch and tactile sensing for a lot of dexterous manipulation tasks. Our tactile sensing provides us with a lot of information regarding contact formations as well as geometric information about objects during any interaction. With this motivation, vision-based tactile sensors are being widely used for various robotic perception and control tasks. In this paper, we present a method for interactive perception using vision-based tactile sensors for a part mating task, where a robot can use tactile sensors and a feedback mechanism using a particle filter to incrementally improve its estimate of objects (pegs and holes) that fit together. To do this, we first train a deep neural network that makes use of tactile images to predict the probabilistic correspondence between arbitrarily shaped objects that fit together. The trained model is used to design a particle filter which is used twofold. First, given one partial (or non-unique) observation of the hole, it incrementally improves the estimate of the correct peg by sampling more tactile observations. Second, it selects the next action for the robot to sample the next touch (and thus image) which results in maximum uncertainty reduction to minimize the number of interactions during the perception task. We evaluate our method on several part-mating tasks with novel objects using a robot equipped with a vision-based tactile sensor. We also show the efficiency of the proposed action selection method against a naive method. See supplementary video at \url{https://www.youtube.com/watch?v=jMVBg_e3gLw}.",Kei Ota (Mitsubishi Electric Corporation)*; Devesh K Jha (MERL); Hsiao-Yu Tung (MIT); Joshua Tenenbaum (MIT),dev.ohtakei@gmail.com; jha@merl.com; hytung@mit.edu; jbt@mit.edu,In person,,,Grasping & Manipulation,"Robot Perception, Sensors & Vision","Tenenbaum, MIT + MERL",,,Interactive perception from tactile data for assembly
80,8,379,Integrated Object Deformation and Contact Patch Estimation from Visuo-Tactile Feedback,"Reasoning over the interplay between object deformation and force transmission through contact is central to the manipulation of compliant objects. In this paper, we propose Neural Deforming Contact Field (NDCF), a representation that jointly models object deformations and contact patches from visuo-tactile feedback using implicit representations. Representing the object geometry and contact with the environment implicitly allows a single model to predict contact patches of varying complexity. Additionally, learning geometry and contact simultaneously allows us to enforce physical priors, such as ensuring contacts lie on the surface of the object. We propose a neural network architecture to learn a NDCF, and train it using simulated data. We then demonstrate that the learned NDCF transfers directly to the real-world without the need for fine-tuning. We benchmark our proposed approach against a baseline representing geometry and contact patches with point clouds. We find that NDCF performs better on simulated data and in transfer to the real-world. More details and video results can be found at https://www.mmintlab.com/ndcf/.",Mark J Van der Merwe (University of Michigan)*; Youngsun Wi (University of Michigan); Dmitry Berenson (University of Michigan); Nima Fazeli (Michigan),markvdm@umich.edu; yswi@umich.edu; dmitryb@umich.edu; nfz@umich.edu,In person,,,Robot Learning,"Robot Perception, Sensors & Vision","Dmitry Berenson, Nima Fazeli, Michigan",,,Learning deformable object representations from visual-tactile date
,,,,,,,,,,,,,,,
,,,,11. Control & Dynamics,,,,,,,,,,,
81,1,174,Incremental Nonlinear Dynamic Inversion based Optical Flow Control for Flying Robots: An Efficient Data-driven Approach,"This paper presents a novel approach for optical flow control of Micro Air Vehicles (MAVs). The task is challenging due to the nonlinearity of optical flow observables. Our proposed Incremental Nonlinear Dynamic Inversion (INDI) control scheme incorporates an efficient data-driven method to address the nonlinearity. It directly estimates the inverse of the time-varying control effectiveness in real-time, eliminating the need for the constant assumption and avoiding high computation in traditional INDI. This approach effectively handles fast-changing system dynamics commonly encountered in optical flow control, particularly height-dependent changes. We demonstrate the robustness and efficiency of the proposed control scheme in numerical simulations and also real-world flight tests: multiple landings of an MAV on a static and flat surface with various tracking setpoints, hovering and landings on moving and undulating surfaces. Despite being challenged with the presence of noisy optical flow estimates and the lateral and vertical movement of the landing surfaces, the MAV is able to successfully track or land on the surface with an exponential decay of both height and vertical velocity at almost the same time, as desired. ","Hann Woei Ho (Universiti Sains Malaysia, Delft University of Technology); Ye Zhou (Universiti Sains Malaysia)*",aehannwoei@usm.my; zhouye@usm.my,In person,,,Control and Dynamics,"Bioinspired Robots; Robot Perception, Sensors & Vision","Malaysia, Delft",,,Optical flow data-driven control for aerial vehicles
82,2,272,G?: A New Approach to Bounding Curvature Constrained Shortest Paths through Dubins Gates,"We consider a Curvature-constrained Shortest Path (CSP) problem on a 2D plane for a robot with minimum turning radius constraints in the presence of obstacles. We introduce a new bounding technique called Gate* (G*) that provides optimality guarantees to the CSP. Our approach relies on relaxing the obstacle avoidance constraints but allows a path to travel through some restricted sets of configurations called gates which are informed by the obstacles. We also let the path to be discontinuous when it reaches a gate. This approach allows us to pose the bounding problem as a least-cost problem in a graph where the cost of traveling an edge requires us to solve a new motion planning problem called the Dubins gate problem. In addition to the theoretical results, our numerical tests show that G* can significantly improve the lower bounds with respect to the baseline approaches, and by more than 60% in some instances.
   ",Satyanarayana  Gupta Manyam (Infoscitex corp.); Abhishek Nayak (Texas A&M University ); Sivakumar  Rathinam (Texas a & m)*,msngupta@gmail.com; nykabhishek@tamu.edu; srathinam@tamu.edu,In person,,,Robot Planning,Autonomous Vehicle Navigation & Mobile Robots; Control and Dynamics,TAMU,,,optimally solving curvature-constrained shortest paths
83,3,358,"RADIUS: Risk-Aware, Real-Time, Reachability-Based Motion Planning","Deterministic methods for motion planning guarantee safety amidst uncertainty in obstacle locations by trying to restrict the robot from operating in any possible location that an obstacle could be in. Unfortunately, this can result in overly conservative behavior. Chance-constrained optimization can be applied to improve the performance of motion planning algorithms by allowing for a user-specified amount of bounded constraint violation. However, state-of-the-art methods rely either on moment-based inequalities, which can be overly conservative, or make it difficult to satisfy assumptions about the class of probability distributions used to model uncertainty. To address these challenges, this work proposes a real-time, risk-aware reachability-based motion planning framework called RADIUS. The method first generates a reachable set of parameterized trajectories for the robot offline. At run time, RADIUS computes a closed-form over-approximation of the risk of a collision with an obstacle. This is done without restricting the probability distribution used to model uncertainty to a simple class (e.g., Gaussian). Then, RADIUS performs real-time optimization to construct a trajectory that can be followed by the robot in a manner that is certified to have a risk of collision that is less than or equal to a user-specified threshold. The proposed algorithm is compared to several state-of-the-art chance-constrained and deterministic methods in simulation, and is shown to consistently outperform them in a variety of driving scenarios. A demonstration of the proposed framework on hardware is also provided.","Challen Enninful Adu (University of Michigan)*; Jinsun Liu (University of Michigan, Ann Arbor); Lucas Lymburner (University of Michigan); Vishrut Kaushik (University of Michigan); Lena Trang (University of Michigan); Ram Vasudevan (University of Michigan)",enninful@umich.edu; jinsunl@umich.edu; llymburn@umich.edu; vishrutk@umich.edu; ltrang@umich.edu; ramv@umich.edu,In person,,,Control and Dynamics,Autonomous Vehicle Navigation & Mobile Robots; Robot Planning,"Ram, Michigan",,,Minimizing risk of collision in motion planning
84,4,177,Robust Safety under Stochastic Uncertainty with Discrete-Time Control Barrier Functions,"Robots deployed in unstructured, real-world environments operate under considerable uncertainty due to imperfect state estimates, model error, and disturbances. The goal of this paper is to develop controllers that are provably safe under uncertainties.  To this end, we leverage Control Barrier Functions (CBFs) which guarantee that a robot remains in a ``safe set'' during its operation---yet CBFs (and their guarantees) are traditionally studied in the context of continuous-time, deterministic systems with bounded uncertainties. In this work, we study the safety properties of discrete-time CBFs (DTCBFs) for systems with discrete-time dynamics and unbounded stochastic disturbances. Using tools from martingale theory, we  develop bounds for the finite-time safety of systems whose dynamics satisfy the discrete-time barrier function condition in expectation, and analyze the effect of Jensen's inequality on DTCBF-based controllers. Finally we present several examples of our method synthesizing safe control inputs for systems subject to significant process noise, including an inverted pendulum, a double integrator, and a quadruped locomoting on a narrow path. ",Ryan Cosner (Caltech)*; Preston Culbertson (Stanford University); Andrew Taylor (Caltech); Aaron Ames (Caltech),rkcosner@caltech.edu; pculbert@caltech.edu; ajtaylor@caltech.edu; ames@caltech.edu,In person,,,Control and Dynamics,,"Aaron Ames, CalTech",,,Provably safe control under uncertainty
85,5,364,Solving Stabilize-Avoid via Epigraph Form Optimal Control using Deep Reinforcement Learning,"Tasks for autonomous robotic systems commonly require stabilization to a desired region while maintaining safety specifications. However, solving this multi-objective problem is challenging when the dynamics are nonlinear and high-dimensional, as traditional methods do not scale well and are often limited to specific problem structures. To address this issue, we propose a novel approach to solve the stabilize-avoid problem via the solution of an infinite-horizon constrained optimal control problem (OCP). We transform the constrained OCP into epigraph form and obtain a two-stage optimization problem that optimizes over the policy in the inner problem and over an auxiliary variable in the outer problem. We then propose a new method for this formulation that combines an on-policy deep reinforcement learning algorithm with neural network regression. Our method yields better stability during training, avoids instabilities caused by saddle-point finding, and is not restricted to specific requirements on the problem structure compared to more traditional methods. We validate our approach on different benchmark tasks, ranging from low-dimensional toy examples to an F16 fighter jet with a 17-dimensional state space. Simulation results show that our approach consistently yields controllers that match or exceed the safety of existing methods while providing ten-fold increases in stability performance from larger regions of attraction.",Oswin So (Massachusetts Institute of Technology)*; Chuchu Fan (MIT),oswinso@mit.edu; chuchu@mit.edu,In person,,,Control and Dynamics,Robot Learning,"Chuchu, MIT",,,RL approach for stabilization control problem
86,6,123,Bridging Active Exploration and Uncertainty-Aware Deployment Using Probabilistic Ensemble Neural Network Dynamics,"In recent years, learning-based control in robotics has gained significant attention due to its capability to address complex tasks in real-world environments. With the advances in machine learning algorithms and computational capabilities, this approach is becoming increasingly important for solving challenging control problems in robotics by learning unknown or partially known robot dynamics. Active exploration, in which a robot directs itself to states that yield the highest information gain, is essential for efficient data collection and minimizing human supervision. Similarly, uncertainty-aware deployment has been a growing concern in robotic control, as uncertain actions informed by the learned model can lead to unstable motions or failure. However, active exploration and uncertainty-aware deployment have been studied independently, and there is limited literature that seamlessly integrates them. This paper presents a unified model-based reinforcement learning framework that bridges these two tasks in the robotics control domain. Our framework uses a probabilistic ensemble neural network for dynamics learning, allowing the quantification of epistemic uncertainty via Jensen-Rényi Divergence. The two opposing tasks of exploration and deployment are optimized through state-of-the-art sampling-based MPC, resulting in efficient collection of training data and successful avoidance of uncertain state-action spaces. We conduct experiments on both autonomous vehicles and wheeled robots, showing promising results for both exploration and deployment.",Taekyung Kim (Agency for Defense Development)*; Jungwi Mun (Agency for Defense Development); Junwon Seo (Agency for Defense Development); Beomsu Kim (Agency for Defense Development); Seongil Hong (Agency for Defense Development),tkkim.robot@gmail.com; moonjw1109@gmail.com; junwon.vision@gmail.com; bmsukim@gmail.com; science4729@gmail.com,In person,,,Robot Learning,Autonomous Vehicle Navigation & Mobile Robots; Control and Dynamics,Agency for Defense Development,,,RL for exploration and safe deployment with some MPC
87,7,77,Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI,"Multiphysics phenomena, the coupling effects involving different aspects of physics laws, are pervasive in the real world and can often be encountered when performing everyday household tasks. Intelligent agents which seek to assist
or replace human laborers will need to learn to cope with such phenomena in household task settings. To equip the agents with such kind of abilities, the research community needs a simulation environment, which will have the capability to serve as the testbed for the training process of these intelligent agents, to have the ability to support multiphysics coupling effects.

Though many mature simulation software for multiphysics simulation have been adopted in industrial production, such techniques have not been applied to robot learning or embodied AI research. To bridge the gap, we propose a novel simulation environment named RFUniverse. This simulator can not only compute rigid and multi-body dynamics, but also multiphysics coupling effects commonly observed in daily life, such as air-solid interaction, fluid-solid interaction, and heat transfer.

Because of the unique multiphysics capacities of this simulator, we can benchmark tasks that involve complex dynamics due to multiphysics coupling effects in a simulation environment before deploying to the real world. RFUniverse provides multiple interfaces to let the users interact with the virtual world in various ways, which is helpful and essential for learning, planning, and control. We benchmark three tasks with reinforcement learning, including food cutting, water pushing, and towel catching. We also evaluate butter pushing with a classic planning-control paradigm. This simulator offers an enhancement of physics simulation in terms of the computation of multiphysics coupling effects. The simulation environment, videos, and other supplementary materials can be viewed on the website: https: //sites.google.com/view/rfuniverse.",Haoyuan Fu (Shanghai Jiao Tong University)*; Wenqiang Xu (Shanghai Jiao Tong University); Ruolin Ye (Cornell University); Han Xue (Shanghai Jiao Tong University); Zhenjun Yu (Shanghai Jiao Tong University); Tutian Tang (Shanghai Jiao Tong University); Yutong Li (Shanghai Jiao Tong University); Wenxin Du (Shanghai Jiao Tong University); Jieyi Zhang (Shanghai Jiao Tong University); Cewu Lu (Shanghai Jiao Tong University),simon-fuhaoyuan@sjtu.edu.cn; vinjohn@sjtu.edu.cn; ry273@cornell.edu; xiaoxiaoxh@sjtu.edu.cn; jeffson-yu@sjtu.edu.cn; tttang@sjtu.edu.cn; davidliyutong@sjtu.edu.cn; mnkmYuki@sjtu.edu.cn; yi_eagle@sjtu.edu.cn; lucewu@sjtu.edu.cn,In person,DEMO,,Robot Modeling & Simulation,Robot Learning,Shanghai Jiao Tong University,,,Simulation of multi-physics coupling effects 
88,8,390,Demonstrating Arena-Web: A Web-based Development and Benchmarking Platform for Autonomous Navigation Approaches,"In recent years, mobile robot navigation approaches have become increasingly important due to various application areas ranging from healthcare to warehouse logistics. In particular, Deep Reinforcement Learning approaches have gained popularity for robot navigation but are not easily accessible to non-experts and complex to develop. In recent years, efforts have been made to make these sophisticated approaches accessible to a wider audience. In this paper, we present Arena-Web, a web-based development and evaluation suite for developing, training, and testing DRL-based navigation planners for various robotic platforms and scenarios. The interface is designed to be intuitive and engaging to appeal to non-experts and make the technology accessible to a wider audience. With Arena-Web and its interface, training and developing Deep Reinforcement Learning agents is simplified and made easy without a single line of code. The web-app is free to use and openly available under the link stated in the supplementary materials. 
",Linh Kästner (Technical University Berlin)*; Reyk Carstens (Technical University Berlin); Lena Nahrwold (Technical University Berlin); Christopher  Liebig (Technical University Berlin); Volodymyr Shcherbyna (Technical University Berlin); Subhin Lee (Technical University Berlin); Jens Lambrecht (Technical University Berlin),linhdoan@win.tu-berlin.de; reyk-carstens@web.de; lena.naworld@tu-berlin.de; c.liebig@campus.tu-berlin.de; vova.shehev@gmail.com; lee.12@campus.tu-berlin.de; lambrecht@tu-berlin.de,In person,DEMO,,Autonomous Vehicle Navigation & Mobile Robots,Human-Robot Interaction; Robot Planning,TU Berlin,,,Online suite for testing DRL-based navigation planners
,,,,,,,,,,,,,,,
,,,,12. Robot Mechanisms & Control,,,,,,,,,,,
89,1,254,"LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning","Dexterous manipulation has been a long-standing challenge in robotics. While machine learning techniques have shown some promise, results have largely been currently limited to simulation.  This can be mostly attributed to the lack of suitable hardware. In this paper, we present LEAP Hand, a low-cost dexterous and anthropomorphic hand for machine learning research. In contrast to previous hands, LEAP Hand has a novel kinematic structure that allows maximal dexterity regardless of finger pose. LEAP Hand is low-cost and can be assembled in 4 hours at a cost of 2000 USD from readily available parts.  It is capable of consistently exerting large torques over long durations of time. We show that LEAP Hand can be used to perform several manipulation tasks in the real world---from visual teleoperation to learning from passive video data and sim2real. LEAP Hand significantly outperforms its closest competitor Allegro Hand in all our experiments while being 1/8th of the cost.  We release the URDF model, 3D CAD files, tuned simulation environment, and a development platform with useful APIs on our website at https://leap-hand.github.io/ .",Kenneth Shaw (Carnegie Mellon University)*; Ananye Agarwal (Carnegie Mellon University); Deepak Pathak (Carnegie Mellon University),kshaw2@andrew.cmu.edu; ananyea@andrew.cmu.edu; dpathak@cs.cmu.edu,In person,,,Robot Learning,Grasping & Manipulation,"Deepak, CMU",,,Low cost hand for learning dexterous manipulation
90,2,155,ROSE: Rotation-based Squeezing Robotic Gripper toward Universal Handling of Objects,"Robotics hand/grippers nowadays are not limited within manufacturing lines; instead, widely utilized in cluttered environments, such as restaurants, farms, and warehouses. In such scenarios, they need to deal with high uncertainty of the grasped objects’ shapes, postures, surfaces, and material properties, which requires complex integration of sensing and decision-making process. On the other hand, integrating soft materials into the gripper’s design may tolerate the above uncertainties and reduce complexity in control. In this paper, we introduce ROSE, a novel soft gripper that can embrace the object and squeeze it by buckling a funnel-liked thin-walled soft membrane around the object by simple rotation of the base. Thanks to this design, ROSE hand can adapt to a wide range of objects that can fall within the funnel, and handle with pleasant gripping force. Regardless this, ROSE can generate a high lift force (up to 328.7 N) while significantly reducing the normal pressure on the gripped objects. In our experiment, a 198 g ROSE can be integrated into a robot arm with a single actuation, and successfully lift various types of objects, even after 400,000 trials. The embracing mechanism helps reduce the dependence of friction between the object and the membrane, as ROSE could pick up a chicken egg submerged inside an olive oil tank. We also report a feasible design for equipping the ROSE hand with tactile sensing, while appealing to the scalability of the design to fit a wide range of objects.",Son Tien Bui (Japan Advanced Institute of Science and Technology); Van Anh Ho (Japan Advanced Institute of Science and Technology)*,buitienson@gmail.com; van-ho@jaist.ac.jp,In person,,,Soft Robots,Bioinspired Robots,"Van Ho, JAIST",,,Novel soft gripper
91,3,75,An Efficient Multi-solution Solver for the Inverse Kinematics of 3-Section Constant-Curvature Robots,"Piecewise constant curvature is a popular kinematics framework for continuum robots. Computing the model parameters from the desired end pose, known as the inverse kinematics problem, is fundamental in manipulation, tracking and planning tasks. In this paper, we propose an efficient multi-solution solver to address the inverse kinematics problem of 3-section constant-curvature robots by bridging both the theoretical reduction and numerical correction. We derive analytical conditions to simplify the original problem into a one-dimensional problem. Further, the equivalence of the two problems is formalised. In addition, we introduce an approximation with bounded error so that the one dimension becomes traversable while the remaining parameters analytically solvable. With the theoretical results, the global search and numerical correction are employed to implement the solver. The experiments validate the better efficiency and higher success rate of our solver than the numerical methods when one solution is required, and demonstrate the ability of obtaining multiple solutions with optimal path planning in a space with obstacles.",Ke Qiu (Zhejiang University); Jingyu Zhang (Zhejiang University); Danying Sun (Zhejiang University); Rong Xiong (Zhejiang University); Haojian LU (Zhejiang University); Yue Wang (Zhejiang University)*,ke.qiu@zju.edu.cn; 12132011@zju.edu.cn; danyingsun@zju.edu.cn; rxiong@zju.edu.cn; luhaojian@zju.edu.cn; ywang24@zju.edu.cn,In person,,,Soft Robots,Robot Modeling & Simulation,Zhejiang,,,IK for constant curvature robot
92,4,81,Predefined-Time Convergent Motion Control for Heterogeneous Continuum Robots,"As research into continuum robots flourishes, there are more and more types of continuum robots, which require researchers to tirelessly design robot-specific motion control algorithms. Besides, the convergence time of control systems for continuum robots has received very little attention. In this paper, we propose a novel predefined-time convergent zeroing dynamics (PTCZD) model, which ensures that the associated error-monitoring function converges to zero in predefined-time. Based on the PTCZD model, we design an inverse kinematics solver and a state estimator for continuum robots, thereby obtaining a generic predefined-time convergent control method for heterogeneous continuum robots for the first time. Simulations and experiments based on cable-driven continuum robots and concentric tube continuum robots are performed to verify the efficacy, robustness and adaptability of the proposed control method. In addition, comparative studies are carried out to demonstrate its advantages against existing control methods for continuum robots.",Ning Tan (Sun Yat-sen University)*; Peng Yu (Sun Yat-sen University); Kai Huang (Sun Yat-Sen University),tann5@mail.sysu.edu.cn; yupeng6@mail2.sysu.edu.cn; huangk36@mail.sysu.edu.cn,In person,,,Soft Robots,Control and Dynamics,Sun Yat-sen University,,,"dynamics model for continuum robots giving IK, state estimation and control"
93,5,153,Adaptive Tracking Control of Dielectric Elastomer Soft Actuators with Viscoelastic Hysteresis Compensation,"This paper proposes a new adaptive control method with viscoelastic hysteresis compensation for high-precision tracking control of dielectric elastomer actuators (DEAs). A direct inverse feedforward compensator is constructed by using a modified Prandtl-Ishlinskii model for compensating hysteresis nonlinearities. The dynamics effects of DEAs and disturbances are coped with the adaptive inverse controller using filtered-x normalized least mean square algorithm. A series of real-time tracking experiments are carried out on a DEA made of commercial acrylic elastomers. The proposed control method achieves accurate tracking of various trajectories with the relative root-mean-square tracking error ranging from 1.37% to a maximum of 4.37% over the whole operating frequency range, and outperforms previously proposed methods in terms of accuracy. The excellent tracking results demonstrate the effectiveness of the developed control method for dielectric elastomer artificial muscles based soft actuators.",Yunhua Zhao (Beihang University)*; Li Wen (Beihang University),yh_zhao@buaa.edu.cn; liwen@buaa.edu.cn,In person,,,Soft Robots,Control and Dynamics; Robot Modeling & Simulation,Beihang University,,,tracking control of dielectric elastomer soft actuators 
94,6,126,Gait design for limbless obstacle aided locomotion using geometric mechanics,"Limbless robots have the potential to maneuver through cluttered environments that conventional robots cannot traverse. As illustrated in their biological counterparts such as snakes and nematodes, limbless locomotors can benefit from interactions with obstacles, yet such obstacle-aided locomotion (OAL) requires properly coordinated high-level self-deformation patterns (gait templates) as well as low-level body adaptation to environments. Most prior work on OAL utilized stereotyped traveling-wave gait templates and relied on local body deformations (e.g., passive body mechanics or decentralized controller parameter adaptation based on force feedback) for obstacle navigation, while gait template design for OAL remains less studied. In this paper, we explore novel gait templates for OAL based on tools derived from geometric mechanics (GM), which thus far has been limited to homogeneous environments. Here, we expand the scope of GM to obstacle-rich environments. Specifically, we establish a model that maps the presence of an obstacle to directional constraints in optimization. In doing so, we identify novel gait templates suitable for sparsely and densely distributed obstacle-rich environments respectively. Open-loop robophysical experiments verify the effectiveness of our identified OAL gaits in obstacle-rich environments. We posit that when such OAL gait templates are augmented with appropriate sensing and feedback controls, limbless locomotors will gain robust function in obstacle rich environments.
","Baxi Chong (Georgia Institute of Technology	)*; Tianyu Wang (Goergia Institute of Technology); Daniel Irvine (Goergia Institute of Technology	); Velin Kojouharov  (Georgia Tech); Bo Lin (Georgia Institute of Technology); Howie Choset (Carnegie Mellon University); Daniel Goldman (Georgia Institute of Technology); Grigoriy Blekherman (Goergia Tech)",bchong9@gatech.edu; twang479@gatech.edu; dirvine7@gatech.edu; velinkojouharov@gatech.edu; bo.lin@math.gatech.edu; choset@andrew.cmu.edu; daniel.goldman@physics.gatech.edu; greg@math.gatech.edu,In person,,,Bioinspired Robots,Mechanisms & Design,"Choset, CMU + Georgia Tech",,,gaits for limbless locomotion
95,7,111,Reconfigurable Robot Control Using Flexible Coupling Mechanisms,"Reconfigurable robot swarms are capable of connecting with each other to form complex structures. Current mechanical or magnetic connection mechanisms can be complicated to manufacture, consume high power, have a limited load-bearing capacity, or can only form rigid structures. In this paper, we present our low-cost soft anchor design that enables flexible coupling and decoupling between robots. Our asymmetric anchor requires minimal force to be pushed into the opening of another robot while having a strong pulling force so that the connection between robots can be secured. To maintain this flexible coupling mechanism as an assembled structure, we present our Model Predictive Control (MPC) frameworks with polygon constraints to model the geometric relationship between robots. We conducted experiments on the soft anchor to obtain its force profile, which informed the three-bar linkage model of the anchor in the simulations. We show that the proposed mechanism and MPC frameworks enable the robots to couple, decouple, and perform various behaviors in both the simulation environment and hardware platform. Our code is available at https://github.com/ZoomLabCMU/puzzlebot_anchor.",Sha Yi (Carnegie Mellon University)*; Katia Sycara (CMU); Zeynep Temel (CMU),shayi@cs.cmu.edu; sycara@andrew.cmu.edu; ztemel@andrew.cmu.edu,In person,,,Multi-Robot & Networked Systems,Control and Dynamics; Mechanisms & Design,"Sycara, CMU",,,MPC for coupling modular robots using soft elements
96,8,366,Co-optimization of Morphology and Behavior of Modular Robots via Hierarchical Deep Reinforcement Learning,"Modular robots hold the promise of changing their shape and even dimension to adapt to various tasks and environments. To realize this superiority, it is essential to find the appropriate morphology and its corresponding behavior simultaneously to ensure optimality of the reconfiguration. However, achieving co-optimization is challenging because robotic configuration and motion are interactive and coupled with each other, as well as their optimization processes. To this end, we proposed a co-optimization framework based on hierarchical Deep Reinforcement Learning (DRL), consisting of a configuration model and a motion model based on the Twin Delayed Deep Deterministic policy gradient algorithm (TD3). The two network models update asynchronously with a shared reward to ensure co-optimality. We conduct simulations and experiments with the Webots platform to validate the proposed framework, and the preliminary results show that it yields high quality optimization schemes and thus allows modular robots to be more adaptive to dynamic and multi-task scenarios.",Jieqiang Sun (Jilin University); Meibao Yao (Jilin University)*; Xueming Xiao (Changchun University Of Science And Technology); zhibing xie (jilin university); Bo Zheng (Shanghai Aerospace Control Technology Institute),jieqsun457911@163.com; meibaoyao@jlu.edu.cn; alexcapshow@gmail.com; 1092673859@qq.com; zhengbohit@gmail.com,In person,,,Robot Learning,"Robot Perception, Sensors & Vision; Robot Planning",Jilin University,,,Co-optimization of configuration and motion for modular robots
,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,
,,,,13. Autonomous Vehicles & Field Robotics,,,,,,,,,,,
97,1,32,Active Velocity Estimation using Light Curtains via Self-Supervised Multi-Armed Bandits,"To navigate in an environment safely and autonomously, robots must accurately estimate where obstacles are and how they move. Instead of using expensive traditional 3D sensors, we explore the use of a much cheaper, faster, and higher resolution alternative: programmable light curtains. Light curtains are a controllable depth sensor that sense only along a surface that the user selects. We adapt a probabilistic method based on particle filters and occupancy grids to explicitly estimate the position and velocity of 3D points in the scene using partial measurements made by light curtains. The central challenge is to decide where to place the light curtain to accurately perform this task. We propose multiple curtain placement strategies guided by maximizing information gain and verifying predicted object locations. Then, we combine these strategies using an online learning framework. We propose a novel self-supervised reward function that evaluates the accuracy of current velocity estimates using future light curtain placements. We use a multi-armed bandit framework to intelligently switch between placement policies in real time, outperforming fixed policies. We develop a full-stack navigation system that uses position and velocity estimates from light curtains for downstream tasks such as localization, mapping, path-planning, and obstacle avoidance. This work paves the way for controllable light curtains to accurately, efficiently, and purposefully perceive and navigate complex and dynamic environments.","Siddharth  Ancha (Massachusetts Institute of Technology)*; Gaurav Pathak (Adobe); Ji Zhang (CMU); Srinivasa Narasimhan (Carnegie Mellon University, USA); David Held (CMU)",sancha@mit.edu; gpathak@adobe.com; zhangji@andrew.cmu.edu; srinivas@andrew.cmu.edu; dheld@andrew.cmu.edu,HYBRID ,,,"Robot Perception, Sensors & Vision",Autonomous Vehicle Navigation & Mobile Robots,"David Held, CMU",,,controllable depth sensor for mobile robot navigation
98,2,13,Self-Supervised Lidar Place Recognition in Overhead Imagery Using Unpaired Data,"As much as place recognition is crucial for navigation, mapping and collecting training ground truth, namely sensor data pairs across different locations, are costly and time-consuming.
This paper tackles these by learning lidar place recognition on public overhead imagery and in a self-supervised fashion, with no need for paired lidar and overhead imagery data.
We learn the cross-modal data comparison between lidar and overhead imagery with a multi-step framework.
First, images are transformed into synthetic lidar data and a latent projection is learned.
Next, we discover pseudo pairs of lidar and satellite data from unpaired and asynchronous sequences, and use them for training a final embedding space projection in a cross-modality place recognition framework.
We train and test our approach on real data from various environments and show performances approaching a supervised method using paired data.",Tim Y. Tang (University of Oxford)*; Daniele De Martini (University of Oxford); Paul M Newman (University of Oxford),ttang@robots.ox.ac.uk; daniele@robots.ox.ac.uk; pnewman@robots.ox.ac.uk,HYBRID,,,"Robot State Estimation, Localization & Mapping",Autonomous Vehicle Navigation & Mobile Robots; Robot Learning,"Newman, Oxford",,,learning place recognition from overhead imagery
99,3,25,Metric-Free Exploration for Topological Mapping by Task and Motion Imitation in Feature Space,"We propose DeepExplorer, a simple and lightweight metric-free exploration method for topological mapping of unknown environments. It performs task and motion planning (TAMP) entirely in image feature space. The task planner is a recurrent network using the latest image observation sequence to hallucinate a feature as the next-best exploration goal. The motion planner then utilizes the current and the hallucinated features to generate an action taking the agent towards that goal.  Our novel feature hallucination enables imitation learning with deep supervision to jointly train the two planners more efficiently than baseline methods. During exploration, we iteratively call the two planners to predict the next action, and the topological map is built by constantly appending the latest image observation and action to the map and using visual place recognition (VPR) for loop closing. 
The resulting topological map efficiently represents an environment's connectivity and traversability, so it can be used for tasks such as visual navigation. We show DeepExplorer's exploration efficiency and strong sim2sim generalization capability on large-scale simulation datasets like Gibson and MP3D. Its effectiveness is further validated via the image-goal navigation performance on the resulting topological map. We further show its strong zero-shot sim2real generalization capability in real-world experiments. The source code is available at https://ai4ce.github.io/DeepExplorer/.",Yuhang He (University of Oxford); Irving Fang (New York University); Yiming Li (New York University); Rushi Bhavesh Shah (New York University Tandon School of Engineering); Chen Feng (New York University)*,yuhanghe01@gmail.com; irvingf7@berkeley.edu; yimingli9702@gmail.com; rs7236@nyu.edu; cfeng@nyu.edu,In person,,,Robot Learning,"Robot Perception, Sensors & Vision; Robot State Estimation, Localization & Mapping",NYU,,,"Exploration for topological mapping, visual TAMP"
100,4,112,Task-Aware Risk Estimation of Perception Failures for Autonomous Vehicles,"Safety and performance are key enablers for autonomous driving: on the one hand we want our autonomous vehicles (AVs) to be safe, while at the same time their performance (e.g., comfort or progression) is key to adoption. To effectively walk the tightrope between safety and performance, AVs need to be risk-averse, but not entirely risk-avoidant. To facilitate safe-yet-performant driving, in this paper, we develop a task-aware risk estimator that assesses the risk a perception failure poses to the AV’s motion plan. If the failure has no bearing on the safety of the AV’s motion plan, then regardless of how egregious the perception failure is, our task-aware risk estimator considers the failure to have a low risk; on the other hand, if a seemingly benign perception failure severely impacts the motion plan, then our estimator considers it to have a high risk. In this paper, we propose a task-aware risk estimator to decide whether a safety maneuver needs to be triggered. To estimate the task-aware risk, first, we leverage the perception failure — detected by a perception monitor— to synthesize an alternative plausible model for the vehicle’s surroundings. The risk due to the perception failure is then formalized as the “relative” risk to the AV’s motion plan between the perceived and the alternative plausible scenario. We employ a statistical tool called copula, which models tail dependencies between distributions, to estimate this risk. The theoretical properties of the copula allow us to compute probably approximately correct (PAC) estimates of the risk. We evaluate our task-aware risk estimator using NuPlan and compare it with established baselines, showing that the proposed risk estimator achieves the best F1-score (doubling the score of the best baseline) and exhibits a good balance between recall and precision, i.e., a good balance of safety and performance.",Pasquale Antonante (Massachusetts Institute of Technology)*; Sushant Veer (NVIDIA Research); Karen Leung (NVIDIA); Xinshuo Weng (NVIDIA Research); Luca Carlone (Massachusetts Institute of Technology); Marco Pavone (NVIDIA),antonap@mit.edu; sveer@nvidia.com; kaleung@nvidia.com; xweng@nvidia.com; lcarlone@mit.edu; mpavone@nvidia.com,In person,,,"Robot Perception, Sensors & Vision",Robot Planning,"Luca Carlone, MIT + Marco Pavone, NVIDIA",,,Risk estimator for perception failures in autonomous driving
101,5,50,CCIL: Context-conditioned imitation learning for urban driving,"Imitation learning holds great promise for addressing the complex task of autonomous urban driving, as experienced human drivers can navigate highly challenging scenarios with ease. While behavior cloning is a widely used imitation learning approach in autonomous driving due to its exemption from risky online interactions, it suffers from the covariate shift issue. To address this limitation, we propose a context-conditioned imitation learning approach that employs a policy to map the context state into the ego vehicle's future trajectory, rather than relying on the traditional formulation of both ego and context states to predict the ego action. Additionally, to reduce the implicit ego information in the coordinate system, we design an ego-perturbed goal-oriented coordinate system. The origin of this coordinate system is the ego vehicle's position plus a zero mean Gaussian perturbation, and the x-axis direction points towards its goal position. Our experiments on the real-world large-scale Lyft and nuPlan datasets show that our method significantly outperforms state-of-the-art approaches. ",Ke Guo (The University of Hong Kong)*; Wei Jing (Alibaba Group); Junbo Chen (Alibaba Damo Acadamy); Jia Pan (The University of Hong Kong),u3006612@connect.hku.hk; 21wjing@gmail.com; junbo.chenjb@taobao.com; jpan@cs.hku.hk,In person,,,Autonomous Vehicle Navigation & Mobile Robots,Robot Planning,"Jia Pan, Hong Kong",,,imitation learning for autonomous urban driving
102,6,171,Efficient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors,"When autonomous vehicles are deployed on public roads, they will encounter countless and diverse driving situations. Many manually designed driving policies are difficult to scale to the real world. Fortunately, reinforcement learning has shown great success in many tasks by automatic trial and error. However, when it comes to autonomous driving in interactive dense traffic, RL agents either fail to learn reasonable performance or necessitate a large amount of data. Our insight is that when humans learn to drive, they will 1) make decisions over the high-level skill space instead of the low-level control space and 2) leverage expert prior knowledge rather than learning from scratch. Inspired by this, we propose ASAP-RL, an efficient reinforcement learning algorithm for autonomous driving that simultaneously leverages motion skills and expert priors. We first parameterized motion skills, which are diverse enough to cover various complex driving scenarios and situations. A skill parameter inverse recovery method is proposed to convert expert demonstrations from control space to skill space. A simple but effective double initialization technique is proposed to leverage expert priors while bypassing the issue of expert suboptimality and early performance degradation. We validate our proposed method on interactive dense-traffic driving tasks given simple and sparse rewards. Experimental results show that our method can lead to higher learning efficiency and better driving performance relative to previous methods that exploit skills and priors differently. Code is open-sourced to facilitate further research.",Letian Wang (University of Toronto); Jie Liu (Beihang University); Hao Shao (Tsinghua University); Wenshuo Wang (McGill University); Ruobing Chen (Sensetime Group); Yu Liu (SenseTime Group LTD)*; Steven L Waslander (University of Toronto),letianwang0@gmail.com; ljie@buaa.edu.cn; shaoh19@mails.tsinghua.edu.cn; wwsbit@gmail.com; leonharddt@alumni.sjtu.edu.cn; liuyuisanai@gmail.com; steven.waslander@robotics.utias.utoronto.ca,In person,,,Autonomous Vehicle Navigation & Mobile Robots,Robot Learning; Robot Planning,"Tsinghua, Waslander, Toronto",,,RL for autonomous driving
103,7,146,"TerrainNet: Visual Modeling of Complex Terrain for High-speed, Off-road Navigation","Effective use of camera-based vision systems is essential for robust performance in autonomous off-road driving, particularly in the high-speed regime. Despite success in structured, on-road settings, current end-to-end approaches for scene prediction have yet to be successfully adapted for complex outdoor terrain. To this end, we present TerrainNet, a vision-based terrain perception system for semantic and geometric terrain prediction for aggressive, off-road navigation. The approach relies on several key insights and practical considerations for achieving reliable terrain modeling. The network includes a multi-headed output representation to capture fine- and coarse-grained terrain features necessary for estimating traversability. Accurate depth estimation is achieved using self-supervised depth completion with multi-view RGB and stereo inputs. Requirements for real-time performance and fast inference speeds are met using efficient, learned image feature projections. Furthermore, the model is trained on a large-scale, real-world off-road dataset collected across a variety of diverse outdoor environments. We show how TerrainNet can also be used for costmap prediction and provide a detailed framework for integration into a planning module. We demonstrate the performance of TerrainNet through extensive comparison to current state-of-the-art baselines for camera-only scene prediction. Finally, we showcase the effectiveness of integrating TerrainNet within a complete autonomous-driving stack by conducting a real-world vehicle test in a challenging off-road scenario. ",Xiangyun Meng (University of Washington)*; Nathan Hatch (University of Washington); Alexander Lambert (University of Washington); Anqi Li (University of Washington); Nolan Wagener (Georgia Tech); Matthew Schmittle (Unviersity of Washington); JoonHo Lee (University of Washington); Wentao Yuan (University of Washington); Zoey Chen (UW); Sameul Deng (University of Washington); Greg Okopal (University of Washington); Dieter Fox (NVIDIA Research / University of Washington); Byron Boots (University of Washington); Amirreza Shaban (University of Washington),xiangyun@cs.washington.edu; nhatch2@cs.washington.edu; lambert6@cs.washington.edu; anqil4@cs.washington.edu; nolan.wagener@gatech.edu; schmttle@cs.washington.edu; joonl4@uw.edu; wentaoy@cs.washington.edu; qiuyuc@cs.washington.edu; sdeng128@uw.edu; okopal@uw.edu; fox@cs.washington.edu; bboots@cs.washington.edu; ashaban@cs.washington.edu,In person,,,"Robot Perception, Sensors & Vision",Autonomous Vehicle Navigation & Mobile Robots; Field Robotics,"Byron Boots, Washington + Fox, NVIDIA",,,vision-based terrain perception system for off-road navigation
104,8,340,"Autonomous Navigation, Mapping and Exploration with Gaussian Processes","Navigating and exploring an unknown environment is a challenging task for autonomous robots, especially in complex and unstructured environments. We propose a new framework that can simultaneously accomplish multiple objectives that are essential to robot autonomy including identifying free space for navigation, building a metric-topological representation for mapping, and ensuring good spatial coverage for unknown space exploration. Different from existing work that model these critical objectives separately, we show that navigation, mapping, and exploration can be derived with the same foundation modeled with a sparse variant of Gaussian Process. Specifically, in our framework the robot navigates by following frontiers computed from a local Gaussian Process perception model, and along the way builds a map in a metric-topological form where nodes are adaptively selected from important perception frontiers. The topology expands towards unexplored areas by assessing a low-cost global uncertainty map also computed from a sparse Gaussian Process. Through evaluations in various cluttered and unstructured environments, we validate that the proposed framework can explore unknown environments faster and with a traveled distance less than the start-of-art frontier exploration approaches. Through field demonstration, we have begun to lay the groundwork for field robots to explore challenging environments such as forests that humans have yet to set foot in.","Mahmoud ALI (Indiana University)*; Hassan Jardali (Indiana University); Nicholas Roy (MIT); Lantao Liu (Indiana University, Intelligent Systems Engineering)",alimaa@iu.edu; hjardali@iu.edu; nickroy@csail.mit.edu; lantao@iu.edu,In person,,,Autonomous Vehicle Navigation & Mobile Robots,Field Robotics,"Lantau Liu, Indiana + Nick Roy, MIT",,,"navigation, mapping and exploration for field robotics as a single Gaussian Process approach"
,,,,,,,,,,,,,,,
,,,,14. Multi-Robot and Aerial Systems,,,,,,,,,,,
105,1,382,Graph Attention Multi-Agent Fleet Autonomy for Advanced Air Mobility,"Autonomous mobility is emerging as a new disruptive mode of urban transportation for moving cargo and passengers. 
However, designing scalable autonomous fleet coordination schemes to accommodate fast-growing mobility systems is challenging primarily due to the increasing heterogeneity of the fleets, time-varying demand patterns, service area expansions, and communication limitations. We introduce the concept of partially observable advanced air mobility games to coordinate a fleet of aerial vehicles by accounting for the heterogeneity of the interacting agents and the self-interested nature inherent to commercial mobility fleets. To model the complex interactions among the agents and the observation uncertainty in the mobility networks, we propose a novel heterogeneous graph attention encoder-decoder (HetGAT Enc-Dec) neural network-based stochastic policy. We train the policy by leveraging deep multi-agent reinforcement learning, allowing decentralized decision-making for the agents using their local observations. Through extensive experimentation, we show that the learned policy generalizes to various fleet compositions, demand patterns, and observation topologies. Further, fleets operating under the HetGAT Enc-Dec policy outperform other state-of-the-art graph neural network policies by achieving the highest fleet reward and fulfillment ratios in on-demand mobility networks.",Malintha Fernando (Indiana University)*; Ransalu Senanayake (Stanford University); Heeyoul Choi (Indiana University); Martin Swany (Indiana University),ccfernan@iu.edu; ransalu@stanford.edu; henchoi@iu.edu; swany@indiana.edu,In person,,,Multi-Robot & Networked Systems,Human-Robot Interaction,Indiana + Stanford,,,Deep multi-agent RL policy for on demand mobiliy
106,2,161,Path Planning for Multiple Tethered Robots Using Topological Braids,"Path planning for multiple tethered robots is a challenging problem due to the complex interactions among the cables and the possibility of severe entanglements. Previous works on this problem either consider idealistic cable models or provide no guarantee for entanglement-free paths. In this work, we present a new approach to address this problem using the theory of braids. By establishing a topological equivalence between the physical cables and the space-time trajectories of the robots, and identifying particular braid patterns that emerge from the entangled trajectories, we obtain the key finding that all complex entanglements stem from a finite number of interaction patterns between 2 or 3 robots. Hence, non-entanglement can be guaranteed by avoiding these interaction patterns in the trajectories of the robots. Based on this finding, we present a graph search algorithm using the permutation grid to efficiently search for a feasible topology of paths and reject braid patterns that result in an entanglement. 
We demonstrate that the proposed algorithm can achieve 100% goal-reaching capability without entanglement for up to 10 drones with a slack cable model in a high-fidelity simulation platform.
The practicality of the proposed approach is verified using three small tethered UAVs in indoor flight experiments.","Muqing Cao (Nanyang Technological University); Kun Cao (Nanyang Technological University); Shenghai Yuan (Nanyang Technological University); Kangcheng Liu (Nanyang Technological University	); Yan Loi Wong (National University of Singapore); Lihua Xie (Nanyang Technological University)*",caom0006@e.ntu.edu.sg; kun001@e.ntu.edu.sg; syuan003@e.ntu.edu.sg; kangcheng.liu@ntu.edu.sg; matwyl@nus.edu.sg; elhxie@ntu.edu.sg,In person,,,Robot Planning,Multi-Robot & Networked Systems,"Lihua Xe, Nanyang Technological University",,,topology for path planning of tethered robots (aerial)
107,3,116,A Sampling-Based Approach for Heterogeneous Coalition Scheduling with Temporal Uncertainty,"Scheduling algorithms for real-world heterogeneous multi-robot teams must be able to reason about temporal uncertainty in the world model in order to create plans that are tolerant to the risk of unexpected delays. To this end, we present a novel sampling-based risk-aware approach for solving Heterogeneous Coalition Scheduling with Temporal Uncertainty (HCSTU) problems, which does not require any assumptions regarding the specific underlying cause of the temporal uncertainty or the specific duration distributions. Our approach computes a schedule which obeys the temporal constraints of a small number of heuristically-selected sample scenarios by solving a Mixed-Integer Linear Program, along with an upper bound on the schedule execution time. Then, it uses a hypothesis testing method, the Sequential Probability Ratio Test, to provide a probabilistic guarantee that the upper bound on the execution time will be respected for a user-specified risk tolerance. With extensive experiments, we demonstrate that our approach empirically respects the risk tolerance, and generates solutions of comparable or better quality than state-of-the-art approaches while being an order of magnitude faster to compute on average. Finally, we demonstrate how robust schedules generated by our approach can be incorporated as solutions to subproblems within the broader Simultaneous Task Allocation and Planning with Spatiotemporal Constraints problem to both guide and expedite the search for solutions of higher quality and lower risk.",Andrew Messing (Georgia Institute of Technology); Jacopo Banfi (MIT)*; Martina Stadler (Massachusetts Institute of Technology); Ethan Stump (US Army Research Laboratory); Harish Ravichandar (Georgia Institute of Technology); Nicholas Roy (MIT); Seth Hutchinson (Georgia Tech),amessing@gatech.edu; jbanfi@mit.edu; mstadler@mit.edu; ethan.a.stump2.civ@mail.mil; harish.ravichandar@cc.gatech.edu; nickroy@csail.mit.edu; seth@gatech.edu,In person,,,Multi-Robot & Networked Systems,Robot Planning,"Hutchinson, GT + Nick Roy, MIT",,,task allocation and planning for heterogeneous robot teams
108,4,334,Concurrent Constrained Optimization of Unknown Rewards for Multi-Robot Task Allocation,"Task allocation can enable effective coordination of multi-robot teams to accomplish tasks that are intractable for individual robots. However, existing approaches to task allocation often assume that task requirements or reward functions are known and explicitly specified by the user. In this work, we consider the challenge of forming effective coalitions for a given heterogeneous multi-robot team when task reward functions are unknown. To this end, we first formulate a new class of problems, dubbed COncurrent Constrained Online optimization of Allocation (COCOA). The COCOA problem requires online optimization of coalitions such that the unknown rewards of all the tasks are simultaneously maximized using a given multi-robot team with constrained resources. To address the COCOA problem, we introduce an online optimization algorithm, named Concurrent Multi-Task Adaptive Bandits (CMTAB), that leverages and builds upon continuum-armed bandit algorithms. Experiments involving detailed numerical simulations and a simulated emergency response task reveal that CMTAB can effectively trade-off exploration and exploitation to simultaneously and efficiently optimize the unknown task rewards while respecting the team's resource constraints.",Sukriti Singh (Georgia Institute of Technology)*; Anusha Srikanthan (University of Pennsylvania); Vivek Mallampati (Georgia Institute of Technology); Harish Ravichandar (Georgia Institute of Technology),sukriti@gatech.edu; sanusha@seas.upenn.edu; vmallampati@gatech.edu; harish.ravichandar@cc.gatech.edu,In person,,,Multi-Robot & Networked Systems,Robot Learning,UPenn + Georgia Tech,,,task allocation and planning for heterogeneous robot teams
109,5,326,Bandit Submodular Maximization for Multi-Robot Coordination in Unpredictable and Partially Observable Environments,"We study the problem of multi-agent coordination in unpredictable and partially observable environments, that is, environments whose future evolution is unknown a priori and that can only be partially observed. We are motivated by the future of autonomy that involves multiple robots coordinating actions in dynamic, unstructured, and partially observable environments to complete complex tasks such as target tracking, environmental mapping, and area monitoring. Such tasks are often modeled as submodular maximization coordination problems due to the information overlap among the robots. We introduce the first submodular coordination algorithm with bandit feedback and bounded tracking regret —bandit feedback is the robots’ ability to compute in hindsight only the effect of their chosen actions, instead of all the alternative actions that they could have chosen instead, due to the partial observability; and tracking regret is the algorithm’s suboptimality with respect to the optimal time-varying actions that fully know the future a priori. The bound gracefully degrades with the environments’ capacity to change adversarially, quantifying how often the robots should re-select actions to learn to coordinate as if they fully knew the future a priori. The algorithm generalizes the seminal Sequential Greedy algorithm by Fisher et al. to the bandit setting, by leveraging submodularity and algorithms for the problem of tracking the best action. We validate our algorithm in simulated scenarios of multi-target tracking.",Zirui Xu (University of Michigan)*; Xiaofeng Lin (University of Michigan); Vasileios Tzoumas (University of Michigan),ziruixu@umich.edu; linxiaof@umich.edu; vtzoumas@umich.edu,In person,,,Multi-Robot & Networked Systems,Robot Planning,"Tzoumas, Michigan",,,multi-target tracking under uncertainty
110,6,389,Distributed Hierarchical Distribution Control for Very-Large-Scale Clustered Multi-Agent Systems,"As the scale and complexity of multi-agent robotic systems are subject to a continuous increase, this paper considers a class of systems labeled as Very-Large-Scale Multi-Agent Systems (VLMAS) with dimensionality that can scale up to the order of millions of agents. In particular, we consider the problem of steering the state distributions of all agents of a VLMAS to prescribed target distributions while satisfying probabilistic safety guarantees. Based on the key assumption that such systems often admit a multi-level hierarchical clustered structure - where the agents are organized into cliques of different levels -  we associate the control of such cliques with the control of distributions, and introduce the Distributed Hierarchical Distribution Control (DHDC) framework. The proposed approach consists of two sub-frameworks. The first one, Distributed Hierarchical Distribution Estimation (DHDE), is a bottom-up hierarchical decentralized algorithm which links the initial and target configurations of the cliques of all levels with suitable Gaussian distributions. The second part, Distributed Hierarchical Distribution Steering (DHDS), is a top-down hierarchical distributed method that steers the distributions of all cliques and agents from the initial to the targets ones assigned by DHDE. Simulation results that scale up to two million agents demonstrate the effectiveness and scalability of the proposed framework. The increased computational efficiency and safety performance of DHDC against related methods is also illustrated. The results of this work indicate the importance of hierarchical distribution control approaches towards achieving safe and scalable solutions for the control of VLMAS.",Augustinos D Saravanos (Georgia Institute of Technology)*; Yihui Li (Georgia Institute of Technology); Evangelos Theodorou (Georgia Institute of Technology),asaravanos3@gatech.edu; yli3039@gatech.edu; evangelos.theodorou@gatech.edu,In person,,,Multi-Robot & Networked Systems,Autonomous Vehicle Navigation & Mobile Robots; Control and Dynamics,"Evangelos Theodorou, Georgia Tech",,,controlling very large scale of robot teams
111,7,56,Decentralization and Acceleration Enables Large-Scale Bundle Adjustment,"Scaling to arbitrarily large bundle adjustment problems requires data and compute to be distributed across multiple devices. Centralized methods in prior works are only able to solve small or medium size problems due to overhead in computation and communication. In this paper, we present a fully decentralized method that alleviates computation and communication bottlenecks to solve arbitrarily large bundle adjustment problems. We achieve this by reformulating the reprojection error and deriving a novel surrogate function that decouples optimization variables from different devices. This function makes it possible to use majorization minimization techniques and reduces bundle adjustment to independent optimization subproblems that can be solved in parallel. We further apply Nesterov's acceleration and adaptive restart to improve convergence while maintaining its theoretical guarantees. Despite limited peer-to-peer communication, our method has provable convergence to first-order critical points under mild conditions. On extensive benchmarks with public datasets, our method converges much faster than decentralized baselines with similar memory usage and communication load. Compared to centralized baselines using a single device, our method, while being decentralized, yields more accurate solutions with significant speedups of up to 953.7x over Ceres and 174.6x over DeepLM. Code: https://github.com/facebookresearch/DABA.",Taosha Fan (Meta AI)*; Joseph Ortiz (Imperial College London); Ming Hsiao (Facebook); Maurizio Monge (Facebook); Jing Dong (Facebook); Todd Murphey (Northwestern Univ.); Mustafa Mukadam (Meta AI / FAIR),taosha.fan@gmail.com; j.ortiz@imperial.ac.uk; mhsaio@fb.com; maurimo@fb.com; jingdong@fb.com; t-murphey@northwestern.edu; mukadam.mh@gmail.com,In person,,,"Robot State Estimation, Localization & Mapping",Multi-Robot & Networked Systems,"Todd Murphey, NWU - Mustafa, Facebook",,,decentralized bundle adjustment
112,8,282,Active Collaborative Localization in Heterogeneous Robot Teams,"Accurate and robust state estimation is critical for autonomous navigation of robot teams. This task is especially challenging for large groups of size, weight, and power (SWAP) constrained aerial robots operating in perceptually-degraded GPS-denied environments. We can, however, actively increase the amount of perceptual information available to such robots by augmenting them with a small number of more expensive, but less resource-constrained, agents. Specifically, the latter can serve as sources of perceptual information themselves. In this paper, we study the problem of optimally positioning (and potentially navigating) a small number of more capable agents to enhance the perceptual environment for their lightweight, inexpensive, teammates that only need to rely on cameras and IMUs. We propose a numerically robust, computationally efficient approach to solve this problem via nonlinear optimization. Our method outperforms the standard approach based on the greedy algorithm, while matching the accuracy of a heuristic evolutionary scheme for global optimization at a fraction of its running time. Ultimately, we validate our solution in both photorealistic simulations and real-world experiments. In these experiments, we use lidar-based autonomous ground vehicles as the more capable agents, and vision-based aerial robots as their SWAP-constrained teammates. Our method is able to reduce drift in visual-inertial odometry by as much as 90%, and it outperforms random positioning of lidar-equipped agents by a significant margin. Furthermore, our method can be generalized to different types of robot teams with heterogeneous perception capabilities. It has a wide range of applications, such as surveying and mapping challenging, dynamic, environments, and enabling resilience to large-scale perturbations that can be caused by earthquakes or storms.",Igor Spasojevic (University of Pennsylvania)*; Xu Liu (University of Pennsylvania); Alejandro Ribeiro (University of Pennsylvania); George J. Pappas (University of Pennsylvania); Vijay Kumar (University of Pennsylvania),igorspas@seas.upenn.edu; liuxu@seas.upenn.edu; aribeiro@seas.upenn.edu; pappasg@seas.upenn.edu; kumar@seas.upenn.edu,MAYBE HYBRID,,,Robot Planning,"Multi-Robot & Networked Systems; Robot State Estimation, Localization & Mapping","Pappas, Kumar, UPenn",,,placing more capable robots in teams with less capable ones for better localization (aerial)
