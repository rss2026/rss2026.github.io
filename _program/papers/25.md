---
layout: paper
title: "Robot Learning with Super-Linear Scaling"
invisible: true
prev_id: "24"
next_id: "26"
---
<div class="paper-authors">
  <div class="paper-author-box">
    <div class="paper-author-name">Marcel Torne Villasevil, Arhan Jain, Jiayi Yuan, Vidyaaranya Macha, Lars Lien Ankile, Anthony Simeonov, Pulkit Agrawal, Abhishek Gupta</div>
    <div class="paper-author-uni"></div>
  </div>
</div>

<div class="paper-pdf">
  <div>
    <a href="https://www.roboticsproceedings.org/rss21/p025.pdf" title="Download PDF" target="_blank">
      <img src="{{ site.baseurl }}/images/paper_link_cardinal_red.png" alt="Paper PDF" width="33" height="40" />
    </a>
  </div>
</div>

### Paper ID 25
{: style="margin-top: 10px; text-align: center;" }

### [Session 3. Scaling Robot Learning]({{ site.baseurl }}/program/papersession?session=3.%20Scaling%20Robot%20Learning)
{: style="text-align: center;" }

#### Poster Session (Day 1): Saturday, June 21, 6:30-8:00 PM
{: style="margin-top: 10px; color: #555555; text-align: center;" }

<b style="color: black;">Abstract: </b>Scaling robot learning requires data collection pipelines that scale favorably with human effort. In this work, we propose *Crowdsourcing and Amortizing Human Effort for Real-to-Sim-to-Real* (**CASHER**), a pipeline for scaling up data collection and learning in simulation where the performance scales superlinearly with human effort. The key idea is to crowdsource digital twins of real-world scenes using 3D reconstruction and collect large-scale data in simulation, rather than the real-world. Data collection in simulation is initially driven by RL, bootstrapped with human demonstrations. As the training of a generalist policy progresses across environments, its generalization capabilities can be used to replace human effort with model-generated demonstrations. This results in a pipeline where behavioral data is collected in simulation with continually reducing human effort. We show that **CASHER** demonstrates zero-shot and few-shot scaling laws on three real-world tasks across diverse scenarios. We show that **CASHER** enables fine-tuning of pre-trained policies to a target scenario using a video scan without any additional human effort.
{: style="color:gray; font-size: 120%; text-align: justified;" }

<div class="paper-menu">
  <div class="paper-menu-inner">
    <a href="{{ site.baseurl }}/program/papers/24/" title="Previous Paper">
            <div class="paper-menu-icon">
                <i class="fas fa-arrow-left"></i><br>
                <span class="paper-menu-label">Back</span>
            </div>
        </a>
    <a href="{{ site.baseurl }}/program/papers" title="All Papers">
      <div class="paper-menu-icon">
        <i class="fas fa-list"></i><br>
        <span class="paper-menu-label">Papers</span>
      </div>
    </a>
    <a href="{{ site.baseurl }}/program/papers/26/" title="Next Paper">
            <div class="paper-menu-icon">
                <i class="fas fa-arrow-right"></i><br>
                <span class="paper-menu-label">Next</span>
            </div>
        </a>
  </div>
</div>
