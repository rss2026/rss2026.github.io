---
layout: paper
title: "RAPID: Robust and Agile Planner Using Inverse Reinforcement Learning for Vision-Based Drone Navigation"
invisible: true
prev_id: "141"
next_id: "143"
---
<div class="paper-authors">
  <div class="paper-author-box">
    <div class="paper-author-name">Minwoo Kim, Geunsik Bae, Jinwoo Lee, Woojae Shin, Changseung Kim, Myongyol Choi, Heejung Shin, Hyongdong Oh</div>
    <div class="paper-author-uni"></div>
  </div>
</div>

<div class="paper-pdf">
  <div>
    <a href="https://www.roboticsproceedings.org/rss21/p142.pdf" title="Download PDF" target="_blank">
      <img src="{{ site.baseurl }}/images/paper_link_cardinal_red.png" alt="Paper PDF" width="33" height="40" />
    </a>
  </div>
</div>

### Paper ID 142
{: style="margin-top: 10px; text-align: center;" }

### [Session 15. Navigation]({{ site.baseurl }}/program/papersession?session=15.%20Navigation)
{: style="text-align: center;" }

#### Poster Session (Day 4): Tuesday, June 24, 12:30-2:00 PM
{: style="margin-top: 10px; color: #555555; text-align: center;" }

<b style="color: black;">Abstract: </b>This paper introduces a learning-based visual planner for agile drone flight in cluttered environments. The proposed planner generates collision-free waypoints in milliseconds, enabling drones to perform agile maneuvers in complex environments without building separate perception, mapping, and planning modules. Learning-based methods, such as behavior cloning (BC) and reinforcement learning (RL), demonstrate promising performance in visual navigation but still face inherent limitations. BC is susceptible to compounding errors due to limited expert imitation, while RL struggles with reward function design and sample inefficiency. To address these limitations, this paper proposes an inverse reinforcement learning (IRL)based framework for high-speed visual navigation. By leveraging IRL, it is possible to reduce the number of interactions with simulation environments and improve capability to deal with high-dimensional spaces (i.e., visual information) while preserving the robustness of RL policies. A motion primitivebased path planning algorithm collects an expert dataset with privileged map data from diverse environments (e.g., narrow gaps, cubes, spheres, trees), ensuring comprehensive scenario coverage. By leveraging both the acquired expert and learner dataset gathered from the agentâ€™s interactions with the simulation environments, a robust reward function and policy are learned across diverse states. While the proposed method is trained in a simulation environment only, it can be directly applied to real-world scenarios without additional training or tuning. The performance of the proposed method is validated in both simulation and real-world environments, including forests and various structures. The trained policy achieves an average speed of 7 m/s and a maximum speed of 8.8 m/s in real flight experiments. To the best of our knowledge, this is the first work to successfully apply an IRL framework for high-speed visual navigation of drones. The experimental videos can be found at https://youtu.be/ZfV6ij0qZMI.
{: style="color:gray; font-size: 120%; text-align: justified;" }

<div class="paper-menu">
  <div class="paper-menu-inner">
    <a href="{{ site.baseurl }}/program/papers/141/" title="Previous Paper">
            <div class="paper-menu-icon">
                <i class="fas fa-arrow-left"></i><br>
                <span class="paper-menu-label">Back</span>
            </div>
        </a>
    <a href="{{ site.baseurl }}/program/papers" title="All Papers">
      <div class="paper-menu-icon">
        <i class="fas fa-list"></i><br>
        <span class="paper-menu-label">Papers</span>
      </div>
    </a>
    <a href="{{ site.baseurl }}/program/papers/143/" title="Next Paper">
            <div class="paper-menu-icon">
                <i class="fas fa-arrow-right"></i><br>
                <span class="paper-menu-label">Next</span>
            </div>
        </a>
  </div>
</div>
