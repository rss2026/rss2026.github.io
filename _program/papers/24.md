---
layout: paper
title: "Coherence-based Approximate Derivatives via Web of Affine Spaces Optimization"
invisible: true
prev_id: "23"
next_id: "25"
---
<div class="paper-authors">
  <div class="paper-author-box">
    <div class="paper-author-name">Daniel Rakita, Chen Liang, Qian Wang</div>
    <div class="paper-author-uni"></div>
  </div>
</div>

<div class="paper-pdf">
  <div>
    <a href="https://www.roboticsproceedings.org/rss21/p024.pdf" title="Download PDF" target="_blank">
      <img src="{{ site.baseurl }}/images/paper_link_cardinal_red.png" alt="Paper PDF" width="33" height="40" />
    </a>
  </div>
</div>

### Paper ID 24
{: style="margin-top: 10px; text-align: center;" }

### [Session 3. Scaling Robot Learning]({{ site.baseurl }}/program/papersession?session=3.%20Scaling%20Robot%20Learning)
{: style="text-align: center;" }

#### Poster Session (Day 1): Saturday, June 21, 6:30-8:00 PM
{: style="margin-top: 10px; color: #555555; text-align: center;" }

<b style="color: black;">Abstract: </b>Computing derivatives is a crucial subroutine in computer science and related fields as it provides a local characterization of a function's steepest directions of ascent or descent.  In this work, we recognize that derivatives are often not computed in isolation; conversely, it is quite common to compute a sequence of derivatives, each one somewhat related to the last.  Thus, we propose accelerating derivative computation by reusing information from previous, related calculationsâ€”a general strategy known as coherence.  We introduce the first instantiation of this strategy through a novel approach called the Web of Affine Spaces (WASP) Optimization.  This approach provides an accurate approximation of a function's derivative object (i.e. gradient, Jacobian matrix, etc.) at the current input within a sequence.  Each derivative within the sequence only requires a small number of forward passes through the function (typically two), regardless of the number of function inputs and outputs.  We demonstrate the efficacy of our approach through several numerical experiments, comparing it with alternative derivative computation methods on benchmark functions.  We show that our method significantly improves the performance of derivative computation on small to medium-sized functions, i.e., functions with approximately fewer than 500 combined inputs and outputs.  Furthermore, we show that this method can be effectively applied in a robotics optimization context. We conclude with a discussion of the limitations and implications of our work.  Open-source code, visual explanations, and videos are located at the paper website: https://apollo-lab-yale.github.io/25-RSS-WASP-website/
{: style="color:gray; font-size: 120%; text-align: justified;" }

<div class="paper-menu">
  <div class="paper-menu-inner">
    <a href="{{ site.baseurl }}/program/papers/23/" title="Previous Paper">
            <div class="paper-menu-icon">
                <i class="fas fa-arrow-left"></i><br>
                <span class="paper-menu-label">Back</span>
            </div>
        </a>
    <a href="{{ site.baseurl }}/program/papers" title="All Papers">
      <div class="paper-menu-icon">
        <i class="fas fa-list"></i><br>
        <span class="paper-menu-label">Papers</span>
      </div>
    </a>
    <a href="{{ site.baseurl }}/program/papers/25/" title="Next Paper">
            <div class="paper-menu-icon">
                <i class="fas fa-arrow-right"></i><br>
                <span class="paper-menu-label">Next</span>
            </div>
        </a>
  </div>
</div>
