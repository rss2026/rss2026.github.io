---
layout: paper
title: "Leveling the Playing Field: Carefully Comparing Classical and Learned Controllers for Quadrotor Trajectory Tracking"
invisible: true
prev_id: "115"
next_id: "117"
---
<div class="paper-authors">
  <div class="paper-author-box">
    <div class="paper-author-name">Pratik Kunapuli, Jake Welde, Dinesh Jayaraman, Vijay Kumar</div>
    <div class="paper-author-uni"></div>
  </div>
</div>

<div class="paper-pdf">
  <div>
    <a href="https://www.roboticsproceedings.org/rss21/p116.pdf" title="Download PDF" target="_blank">
      <img src="{{ site.baseurl }}/images/paper_link_cardinal_red.png" alt="Paper PDF" width="33" height="40" />
    </a>
  </div>
</div>

### Paper ID 116
{: style="margin-top: 10px; text-align: center;" }

### [Session 12. Control and Dynamics]({{ site.baseurl }}/program/papersession?session=12.%20Control%20and%20Dynamics)
{: style="text-align: center;" }

#### Poster Session (Day 3): Monday, June 23, 6:30-8:00 PM
{: style="margin-top: 10px; color: #555555; text-align: center;" }

<b style="color: black;">Abstract: </b>Learning-based control approaches like reinforcement learning (RL) have recently produced a slew of impressive results for tasks like quadrotor trajectory tracking and drone racing. Naturally, it is common to demonstrate the advantages of these new controllers against established methods like analytical controllers. We observe, however, that reliably comparing the performance of these very different classes of controllers is more complicated than might appear at first sight. As a case study, we take up the problem of agile tracking of an end-effector for a quadrotor with a fixed-arm. We develop a set of best practices for synthesizing the best RL and Geometric controllers for benchmarking. In the process, we fix widely prevalent RL-favoring biases in prior studies that provide asymmetric access to: (1) the task definition in the form of objective functions, (2) datasets for parameter optimization, and (3) “feed-forward” controller inputs revealing the desired future trajectory. The resulting contributions are threefold: first, our improved robust experimental protocol reveals that the gaps between the two controller classes are much smaller than expected from previously published findings. Geometric control performs on par or better than RL in most practical settings, while RL fares better in transient performance at the expense of steady-state error. Second, our improvements to the experimental protocol for comparing learned and classical controller synthesis approaches are critical: each of the above asymmetries can yield misleading conclusions, and we show evidence that suggests that they indeed have in prior quadrotor studies. Finally, we open-source implementations of Geometric and RL controllers for these aerial vehicles implementing best practices for future development. Code, videos, and more can be found on the project website: https://pratikkunapuli.github.io/rl-vs-gc/
{: style="color:gray; font-size: 120%; text-align: justified;" }

<div class="paper-menu">
  <div class="paper-menu-inner">
    <a href="{{ site.baseurl }}/program/papers/115/" title="Previous Paper">
            <div class="paper-menu-icon">
                <i class="fas fa-arrow-left"></i><br>
                <span class="paper-menu-label">Back</span>
            </div>
        </a>
    <a href="{{ site.baseurl }}/program/papers" title="All Papers">
      <div class="paper-menu-icon">
        <i class="fas fa-list"></i><br>
        <span class="paper-menu-label">Papers</span>
      </div>
    </a>
    <a href="{{ site.baseurl }}/program/papers/117/" title="Next Paper">
            <div class="paper-menu-icon">
                <i class="fas fa-arrow-right"></i><br>
                <span class="paper-menu-label">Next</span>
            </div>
        </a>
  </div>
</div>
