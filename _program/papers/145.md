---
layout: paper
title: "Prompting with the Future: Open-World Model Predictive Control with Interactive Digital Twins"
invisible: true
prev_id: "144"
next_id: "146"
---
<div class="paper-authors">
  <div class="paper-author-box">
    <div class="paper-author-name">Chuanruo Ning, Kuan Fang, Wei-Chiu Ma</div>
    <div class="paper-author-uni"></div>
  </div>
</div>

<div class="paper-pdf">
  <div>
    <a href="https://www.roboticsproceedings.org/rss21/p145.pdf" title="Download PDF" target="_blank">
      <img src="{{ site.baseurl }}/images/paper_link_cardinal_red.png" alt="Paper PDF" width="33" height="40" />
    </a>
  </div>
</div>

### Paper ID 145
{: style="margin-top: 10px; text-align: center;" }

### [Session 16. Manipulation III]({{ site.baseurl }}/program/papersession?session=16.%20Manipulation%20III)
{: style="text-align: center;" }

#### Poster Session (Day 4): Tuesday, June 24, 12:30-2:00 PM
{: style="margin-top: 10px; color: #555555; text-align: center;" }

<b style="color: black;">Abstract: </b>Open-world robotic manipulation requires robots to perform novel tasks described by free-form language in unstructured settings. While visionâ€“language models (VLMs) offer strong high-level semantic reasoning, they lack the fine-grained physical insight needed for precise low-level control. To address this gap, we introduce Prompting with the Future (PWTF), a model-predictive control framework that augments VLM-based policies with explicit physics modeling. PWTF builds an interactive digital twin of the workspace from a quick handheld video scan, enabling prediction of future states under candidate action sequences. Instead of asking the VLM to predict actions or results by reasoning dynamics, the framework simulates diverse possible outcomes, renders them as visual prompts with adaptively selected camera viewpoints that expose the most informative physical context. A sampling-based planner then selects the action sequence that the VLM rates as best aligned with the task objective. We validate PWTF on eight real-world manipulation tasks involving contact-rich interaction, object reorientation, and clutter removal, demonstrating significantly higher success rates than state-of-the-art VLM-based control methods. Through ablation studies, we further analyze the performance and demonstrate that explicitly modeling physics, while still leveraging VLM semantic strengths, is essential for robust manipulation.
{: style="color:gray; font-size: 120%; text-align: justified;" }

<div class="paper-menu">
  <div class="paper-menu-inner">
    <a href="{{ site.baseurl }}/program/papers/144/" title="Previous Paper">
            <div class="paper-menu-icon">
                <i class="fas fa-arrow-left"></i><br>
                <span class="paper-menu-label">Back</span>
            </div>
        </a>
    <a href="{{ site.baseurl }}/program/papers" title="All Papers">
      <div class="paper-menu-icon">
        <i class="fas fa-list"></i><br>
        <span class="paper-menu-label">Papers</span>
      </div>
    </a>
    <a href="{{ site.baseurl }}/program/papers/146/" title="Next Paper">
            <div class="paper-menu-icon">
                <i class="fas fa-arrow-right"></i><br>
                <span class="paper-menu-label">Next</span>
            </div>
        </a>
  </div>
</div>
