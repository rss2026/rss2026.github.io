---
layout: paper
title: "Solving Stabilize-Avoid via Epigraph Form Optimal Control using Deep Reinforcement Learning"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Oswin So</div>
    <div class="paper-author-uni">Massachusetts Institute of Technology</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Chuchu Fan</div>
    <div class="paper-author-uni">Massachusetts Institute of Technology</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p085.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 85
{: style="margin-top: 10px; text-align: center;"}

### [Session 11. Control & Dynamics]({{ site.baseurl }}/program/papersession?session=11.%20Control%20%26%20Dynamics&c1=Nadia%20Figueroa&c2=Nima%20Fazeli&c1a=University%20of%20Pennsylvania&c2a=University%20of%20Michigan)
{: style="text-align: center;"}

#### Poster Session Thursday, July 13
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 21
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>Tasks for autonomous robotic systems commonly require stabilization to a desired region while maintaining safety specifications. However, solving this multi-objective problem is challenging when the dynamics are nonlinear and high-dimensional, as traditional methods do not scale well and are often limited to specific problem structures. To address this issue, we propose a novel approach to solve the stabilize-avoid problem via the solution of an infinite-horizon constrained optimal control problem (OCP). We transform the constrained OCP into epigraph form and obtain a two-stage optimization problem that optimizes over the policy in the inner problem and over an auxiliary variable in the outer problem. We then propose a new method for this formulation that combines an on-policy deep reinforcement learning algorithm with neural network regression. Our method yields better stability during training, avoids instabilities caused by saddle-point finding, and is not restricted to specific requirements on the problem structure compared to more traditional methods. We validate our approach on different benchmark tasks, ranging from low-dimensional toy examples to an F16 fighter jet with a 17-dimensional state space. Simulation results show that our approach consistently yields controllers that match or exceed the safety of existing methods while providing ten-fold increases in stability performance from larger regions of attraction.
{: style="color:gray; font-size: 120%; text-align: justified;"}


### Links
- [Supplementary materials](http://www.roboticsproceedings.org/rss19/p085_sup.zip)

<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/084/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/086/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
