---
layout: paper
title: "Hindsight States: Blending Sim & Real Task Elements for Efficient Reinforcement Learning"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Simon Guist</div>
    <div class="paper-author-uni">Max Planck Institute for Intelligent Systems</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Jan Schneider</div>
    <div class="paper-author-uni">Max Planck Institute for Intelligent Systems</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Vincent Berenz</div>
    <div class="paper-author-uni">Max Planck Institute for Intelligent Systems</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Alexander Dittrich</div>
    <div class="paper-author-uni">Max Planck Institute for Intelligent Systems</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Bernhard Schölkopf</div>
    <div class="paper-author-uni">Max Planck Institute for Intelligent Systems</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Dieter Büchler</div>
    <div class="paper-author-uni">Max Planck Institute for Intelligent Systems</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p038.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 38
{: style="margin-top: 10px; text-align: center;"}

### [Session 5. Simulation and Sim2Real]({{ site.baseurl }}/program/papersession?session=5.%20Simulation%20and%20Sim2Real&c1=Gregory%20Chirikjian&c2=Hao%20Su&c1a=National%20University%20of%20Singapore&c2a=UC%20San%20Diego)
{: style="text-align: center;"}

#### Poster Session Wednesday, July 12
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 6
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>Reinforcement learning has shown great potential in solving complex tasks when large amounts of data can be generated with little effort. In robotics, one approach to generate training data builds on simulations or models. However, for many tasks, such as with complex soft robots, devising such models is substantially more challenging. Recent successes in soft robotics indicate that employing complex robots can lead to performance boosts. Here, we leverage the imbalance in complexity of the dynamics to learn more sample-efficiently. We (i) abstract the task into distinct components, (ii) off-load the simple dynamics parts into the simulation, and (iii) multiply these virtual parts to generate more data in hindsight. Our new method, Hindsight States (HiS), uses this data and selects the most useful transitions for training. It can be used with an arbitrary off-policy algorithm. 
We validate our method on several challenging simulated tasks and demonstrate that it improves learning both on its own and when combined with an existing hindsight algorithm, Hindsight Experience Replay (HER). Finally, we evaluate HiS on a physical system and show that it boosts performance on a complex table tennis task with a muscular robot.
{: style="color:gray; font-size: 120%; text-align: justified;"}


### Links
- [Supplementary materials](http://www.roboticsproceedings.org/rss19/p038_sup.zip)

<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/037/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/039/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
