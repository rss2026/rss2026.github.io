---
layout: paper
title: "Active Velocity Estimation using Light Curtains via Self-Supervised Multi-Armed Bandits"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Siddharth  Ancha</div>
    <div class="paper-author-uni">Massachusetts Institute of Technology</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Gaurav Pathak</div>
    <div class="paper-author-uni">Adobe</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Ji Zhang</div>
    <div class="paper-author-uni">Carnegie Mellon University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Srinivasa Narasimhan</div>
    <div class="paper-author-uni">Carnegie Mellon University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">David Held</div>
    <div class="paper-author-uni">Carnegie Mellon University</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p097.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 97
{: style="margin-top: 10px; text-align: center;"}

### [Session 13. Autonomous Vehicles & Field Robotics]({{ site.baseurl }}/program/papersession?session=13.%20Autonomous%20Vehicles%20%26%20Field%20Robotics&c1=Shoudong%20Huang&c2=Jen%20Jen%20Chung&c1a=University%20of%20Technology%20Sydney&c2a=Univ.%20of.%20Queensland)
{: style="text-align: center;"}

#### Poster Session Friday, July 14
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 1
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>To navigate in an environment safely and autonomously, robots must accurately estimate where obstacles are and how they move. Instead of using expensive traditional 3D sensors, we explore the use of a much cheaper, faster, and higher resolution alternative: programmable light curtains. Light curtains are a controllable depth sensor that sense only along a surface that the user selects. We adapt a probabilistic method based on particle filters and occupancy grids to explicitly estimate the position and velocity of 3D points in the scene using partial measurements made by light curtains. The central challenge is to decide where to place the light curtain to accurately perform this task. We propose multiple curtain placement strategies guided by maximizing information gain and verifying predicted object locations. Then, we combine these strategies using an online learning framework. We propose a novel self-supervised reward function that evaluates the accuracy of current velocity estimates using future light curtain placements. We use a multi-armed bandit framework to intelligently switch between placement policies in real time, outperforming fixed policies. We develop a full-stack navigation system that uses position and velocity estimates from light curtains for downstream tasks such as localization, mapping, path-planning, and obstacle avoidance. This work paves the way for controllable light curtains to accurately, efficiently, and purposefully perceive and navigate complex and dynamic environments.
{: style="color:gray; font-size: 120%; text-align: justified;"}


<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/096/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/098/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
