---
layout: paper
title: "ConceptFusion: Open-set multimodal 3D mapping"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Krishna Murthy Jatavallabhula</div>
    <div class="paper-author-uni">Massachusetts Institute of Technology</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Alihusein Kuwajerwala</div>
    <div class="paper-author-uni">Université de Montréal</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Qiao Gu</div>
    <div class="paper-author-uni">University of Toronto</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Mohd Omama</div>
    <div class="paper-author-uni">IIIT Hyderabad</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Ganesh Iyer</div>
    <div class="paper-author-uni">Amazon</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Soroush Saryazdi</div>
    <div class="paper-author-uni">Concordia University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Tao Chen</div>
    <div class="paper-author-uni">Massachusetts Institute of Technology</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Alaa Maalouf</div>
    <div class="paper-author-uni">Massachusetts Institute of Technology</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Shuang Li</div>
    <div class="paper-author-uni">Massachusetts Institute of Technology</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Nikhil Varma Keetha</div>
    <div class="paper-author-uni">Carnegie Mellon University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Ayush Tewari</div>
    <div class="paper-author-uni">Massachusetts Institute of Technology</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Joshua Tenenbaum</div>
    <div class="paper-author-uni">Massachusetts Institute of Technology</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Celso de Melo</div>
    <div class="paper-author-uni">Army Research Laboratory</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Madhava Krishna</div>
    <div class="paper-author-uni">IIIT Hyderabad</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Liam Paull</div>
    <div class="paper-author-uni">Université de Montréal</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Florian Shkurti</div>
    <div class="paper-author-uni">University of Toronto</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Antonio Torralba</div>
    <div class="paper-author-uni">Massachusetts Institute of Technology</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p066.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 66
{: style="margin-top: 10px; text-align: center;"}

### [Session 9. Robot State Estimation]({{ site.baseurl }}/program/papersession?session=9.%20Robot%20State%20Estimation&c1=Luca%20Carlone&c2=Ayoung%20Kim&c1a=MIT&c2a=Seoul%20National%20University)
{: style="text-align: center;"}

#### Poster Session Thursday, July 13
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 2
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason about a finite set of concepts, pre-defined at training time. To address this issue, we propose ConceptFusion, a scene representation that is: (i) fundamentally open-set, enabling reasoning beyond a closed set of concepts (ii) inherently multi-modal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today’s foundation models that have been pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40% margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping.
{: style="color:gray; font-size: 120%; text-align: justified;"}


### Links
- [Supplementary materials](http://www.roboticsproceedings.org/rss19/p066_sup.zip)

<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/065/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/067/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
