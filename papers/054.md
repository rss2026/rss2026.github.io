---
layout: paper
title: "Fast Traversability Estimation for Wild Visual Navigation"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Jonas Frey</div>
    <div class="paper-author-uni">ETH Zürich</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Matias Mattamala</div>
    <div class="paper-author-uni">University of Oxford</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Nived Chebrolu</div>
    <div class="paper-author-uni">University of Oxford</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Cesar Cadena</div>
    <div class="paper-author-uni">ETH Zürich</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Maurice Fallon</div>
    <div class="paper-author-uni">University of Oxford</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Marco Hutter</div>
    <div class="paper-author-uni">ETH Zürich</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p054.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 54
{: style="margin-top: 10px; text-align: center;"}

### [Session 7. Mobile Manipulation and Locomotion]({{ site.baseurl }}/program/papersession?session=7.%20Mobile%20Manipulation%20and%20Locomotion&c1=Hae-Won%20Park&c2=Tirthankar%20Bandyopadhyay&c1a=KAIST&c2a=CSIRO)
{: style="text-align: center;"}

#### Poster Session Wednesday, July 12
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 22
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>Natural environments such as forests and grasslands are challenging for robotic navigation because of the false perception of rigid obstacles from high grass, twigs, or bushes. In this work, we propose Wild Visual Navigation (WVN), an online self-supervised learning system for traversability estimation which uses only vision. The system is able to continuously adapt from a short human demonstration in the field. It leverages high-dimensional features from self-supervised visual transformer models, with an online scheme for supervision generation that runs in real-time on the robot. We demonstrate the advantages of our approach with experiments and ablation studies in challenging environments in forests, parks, and grasslands. Our system is able to bootstrap the traversable terrain segmentation in less than 5 min of in-field training time, enabling the robot to navigate in complex outdoor terrains - negotiating obstacles in high grass as well as a 1.4 km footpath following. While our experiments were executed with a quadruped robot, ANYmal, the approach presented can generalize to any ground robot. Project page: https://bit.ly/3M6nMHH
{: style="color:gray; font-size: 120%; text-align: justified;"}


<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/053/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/055/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
