---
layout: paper
title: "Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Maximilian Du</div>
    <div class="paper-author-uni">Stanford University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Suraj Nair</div>
    <div class="paper-author-uni">Stanford University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Dorsa Sadigh</div>
    <div class="paper-author-uni">Stanford University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Chelsea Finn</div>
    <div class="paper-author-uni">Stanford University</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p011.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 11
{: style="margin-top: 10px; text-align: center;"}

### [Session 2. Manipulation from Demonstrations and Teleoperation]({{ site.baseurl }}/program/papersession?session=2.%20Manipulation%20from%20Demonstrations%20and%20Teleoperation&c1=Florian%20Shkurti&c2=Dongheui%20Lee&c1a=University%20of%20Toronto&c2a=TU%20Wien)
{: style="text-align: center;"}

#### Poster Session Tuesday, July 11
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 11
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>Enabling robots to learn novel visuomotor skills in a data-efficient manner remains an unsolved problem with myriad challenges. A popular paradigm for tackling this problem is through leveraging large unlabeled datasets that have many behaviors in them and then adapting a policy to a specific task using a small amount of task-specific human supervision (i.e. interventions or demonstrations). However, how best to leverage the narrow task-specific supervision and balance it with offline data remains an open question. Our key insight in this work is that task-specific data not only provides new data for an agent to train on but can also inform the type of prior data the agent should use for learning. Concretely, we propose a simple approach that uses a small amount of downstream expert interventions or demonstrations to selectively query relevant behaviors from an offline, unlabeled dataset (including many sub-optimal behaviors). The agent is then jointly trained on the expert and queried data. We observe that our method learns to query only the relevant transitions to the task, filtering out sub-optimal or task-irrelevant data. By doing so, it is able to learn more effectively from the mix of task-specific and offline data compared to naively mixing the data or only using the task-specific data. 
Furthermore, we find that our simple querying approach outperforms more complex goal-conditioned methods by 20% across simulated and real robotic manipulation tasks from images. 
{: style="color:gray; font-size: 120%; text-align: justified;"}


<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/010/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/012/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
