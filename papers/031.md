---
layout: paper
title: "StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Weiyu Liu</div>
    <div class="paper-author-uni">Georgia Tech</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Yilun Du</div>
    <div class="paper-author-uni">Massachusetts Institute of Technology</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Tucker Hermans</div>
    <div class="paper-author-uni">University of Utah</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Sonia Chernova</div>
    <div class="paper-author-uni">Georgia Tech</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Chris Paxton</div>
    <div class="paper-author-uni">Meta AI</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p031.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 31
{: style="margin-top: 10px; text-align: center;"}

### [Session 4. Large Data and Vision-Language Models for Robotics]({{ site.baseurl }}/program/papersession?session=4.%20Large%20Data%20and%20Vision-Language%20Models%20for%20Robotics&c1=Lerrel%20Pinto&c2=Yuke%20Zhu&c1a=NYU&c2a=UT-Austin)
{: style="text-align: center;"}

#### Poster Session Tuesday, July 11
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 31
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>Robots operating in human environments must be able to rearrange objects into semantically-meaningful configurations, even if these objects are previously unseen. In this work, we focus on the problem of building physically-valid structures without step-by-step instructions. We propose StructDiffusion, which combines a diffusion model and an object-centric transformer to construct structures given partial-view point clouds and high-level language goals, such as "set the table". Our method can perform multiple challenging language-conditioned multi-step 3D planning tasks using one model. StructDiffusion even improves the success rate of assembling physically-valid structures out of unseen objects by on average 16% over an existing multi-modal transformer model trained on specific structures. We show experiments on held-out objects in both simulation and on real-world rearrangement tasks. Importantly, we show how integrating both a diffusion model and a collision-discriminator model allows for improved generalization over other methods when rearranging previously-unseen objects.
{: style="color:gray; font-size: 120%; text-align: justified;"}


<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/030/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/032/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
