---
layout: paper
title: "Co-optimization of Morphology and Behavior of Modular Robots via Hierarchical Deep Reinforcement Learning"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Jieqiang Sun</div>
    <div class="paper-author-uni">Jilin University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Meibao Yao</div>
    <div class="paper-author-uni">Jilin University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Xueming Xiao</div>
    <div class="paper-author-uni">Changchun University Of Science And Technology</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Zhibing Xie</div>
    <div class="paper-author-uni">Jilin University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Bo Zheng</div>
    <div class="paper-author-uni">Shanghai Aerospace Control Technology Institute</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p096.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 96
{: style="margin-top: 10px; text-align: center;"}

### [Session 12. Robot Mechanisms & Control]({{ site.baseurl }}/program/papersession?session=12.%20Robot%20Mechanisms%20%26%20Control&c1=Ankur%20Mehta&c2=Jiachen%20Zhang&c1a=UCLA&c2a=CU%20Hong%20Kong)
{: style="text-align: center;"}

#### Poster Session Thursday, July 13
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 32
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>Modular robots hold the promise of changing their shape and even dimension to adapt to various tasks and environments. To realize this superiority, it is essential to find the appropriate morphology and its corresponding behavior simultaneously to ensure optimality of the reconfiguration. However, achieving co-optimization is challenging because robotic configuration and motion are interactive and coupled with each other, as well as their optimization processes. To this end, we proposed a co-optimization framework based on hierarchical Deep Reinforcement Learning (DRL), consisting of a configuration model and a motion model based on the Twin Delayed Deep Deterministic policy gradient algorithm (TD3). The two network models update asynchronously with a shared reward to ensure co-optimality. We conduct simulations and experiments with the Webots platform to validate the proposed framework, and the preliminary results show that it yields high quality optimization schemes and thus allows modular robots to be more adaptive to dynamic and multi-task scenarios.
{: style="color:gray; font-size: 120%; text-align: justified;"}


<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/095/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/097/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
