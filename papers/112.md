---
layout: paper
title: "Active Collaborative Localization in Heterogeneous Robot Teams"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Igor Spasojevic</div>
    <div class="paper-author-uni">University of Pennsylvania</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Xu Liu</div>
    <div class="paper-author-uni">University of Pennsylvania</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Alejandro Ribeiro</div>
    <div class="paper-author-uni">University of Pennsylvania</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">George J. Pappas</div>
    <div class="paper-author-uni">University of Pennsylvania</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Vijay Kumar</div>
    <div class="paper-author-uni">University of Pennsylvania</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p112.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 112
{: style="margin-top: 10px; text-align: center;"}

### [Session 14. Multi-Robot and Aerial Systems]({{ site.baseurl }}/program/papersession?session=14.%20Multi-Robot%20and%20Aerial%20Systems&c1=Han-Lim%20Choi&c2=Lantao%20Liu&c1a=KAIST&c2a=Indiana%20University)
{: style="text-align: center;"}

#### Poster Session Friday, July 14
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 16
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>Accurate and robust state estimation is critical for autonomous navigation of robot teams. This task is especially challenging for large groups of size, weight, and power (SWAP) constrained aerial robots operating in perceptually-degraded GPS-denied environments. We can, however, actively increase the amount of perceptual information available to such robots by augmenting them with a small number of more expensive, but less resource-constrained, agents. Specifically, the latter can serve as sources of perceptual information themselves. In this paper, we study the problem of optimally positioning (and potentially navigating) a small number of more capable agents to enhance the perceptual environment for their lightweight, inexpensive, teammates that only need to rely on cameras and IMUs. We propose a numerically robust, computationally efficient approach to solve this problem via nonlinear optimization. Our method outperforms the standard approach based on the greedy algorithm, while matching the accuracy of a heuristic evolutionary scheme for global optimization at a fraction of its running time. Ultimately, we validate our solution in both photorealistic simulations and real-world experiments. In these experiments, we use lidar-based autonomous ground vehicles as the more capable agents, and vision-based aerial robots as their SWAP-constrained teammates. Our method is able to reduce drift in visual-inertial odometry by as much as 90%, and it outperforms random positioning of lidar-equipped agents by a significant margin. Furthermore, our method can be generalized to different types of robot teams with heterogeneous perception capabilities. It has a wide range of applications, such as surveying and mapping challenging, dynamic, environments, and enabling resilience to large-scale perturbations that can be caused by earthquakes or storms.
{: style="color:gray; font-size: 120%; text-align: justified;"}


<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/111/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<img src="{{ site.baseurl }}/images/blank_icon.png" alt="End of Program" title="End of Program"/> 

</div>
