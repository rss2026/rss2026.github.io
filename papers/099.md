---
layout: paper
title: "Metric-Free Exploration for Topological Mapping by Task and Motion Imitation in Feature Space"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Yuhang He</div>
    <div class="paper-author-uni">University of Oxford</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Irving Fang</div>
    <div class="paper-author-uni">New York University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Yiming Li</div>
    <div class="paper-author-uni">New York University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Rushi Bhavesh Shah</div>
    <div class="paper-author-uni">New York University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Chen Feng</div>
    <div class="paper-author-uni">New York University</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p099.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 99
{: style="margin-top: 10px; text-align: center;"}

### [Session 13. Autonomous Vehicles & Field Robotics]({{ site.baseurl }}/program/papersession?session=13.%20Autonomous%20Vehicles%20%26%20Field%20Robotics&c1=Shoudong%20Huang&c2=Jen%20Jen%20Chung&c1a=University%20of%20Technology%20Sydney&c2a=Univ.%20of.%20Queensland)
{: style="text-align: center;"}

#### Poster Session Friday, July 14
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 3
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>We propose DeepExplorer, a simple and lightweight metric-free exploration method for topological mapping of unknown environments. It performs task and motion planning (TAMP) entirely in image feature space. The task planner is a recurrent network using the latest image observation sequence to hallucinate a feature as the next-best exploration goal. The motion planner then utilizes the current and the hallucinated features to generate an action taking the agent towards that goal.  Our novel feature hallucination enables imitation learning with deep supervision to jointly train the two planners more efficiently than baseline methods. During exploration, we iteratively call the two planners to predict the next action, and the topological map is built by constantly appending the latest image observation and action to the map and using visual place recognition (VPR) for loop closing. 
The resulting topological map efficiently represents an environment's connectivity and traversability, so it can be used for tasks such as visual navigation. We show DeepExplorer's exploration efficiency and strong sim2sim generalization capability on large-scale simulation datasets like Gibson and MP3D. Its effectiveness is further validated via the image-goal navigation performance on the resulting topological map. We further show its strong zero-shot sim2real generalization capability in real-world experiments. The source code is available at https://ai4ce.github.io/DeepExplorer/.
{: style="color:gray; font-size: 120%; text-align: justified;"}


<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/098/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/100/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
