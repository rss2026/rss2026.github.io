---
layout: paper
title: "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Ted Xiao</div>
    <div class="paper-author-uni">Google Inc</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Harris Chan</div>
    <div class="paper-author-uni">University of Toronto</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Pierre Sermanet</div>
    <div class="paper-author-uni">Google Inc</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Ayzaan Wahid</div>
    <div class="paper-author-uni">Google Inc</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Anthony Brohan</div>
    <div class="paper-author-uni">Google Research</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Karol Hausman</div>
    <div class="paper-author-uni">Google Brain</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Sergey Levine</div>
    <div class="paper-author-uni">Google Inc</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Jonathan Tompson</div>
    <div class="paper-author-uni">Google Inc</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p029.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 29
{: style="margin-top: 10px; text-align: center;"}

### [Session 4. Large Data and Vision-Language Models for Robotics]({{ site.baseurl }}/program/papersession?session=4.%20Large%20Data%20and%20Vision-Language%20Models%20for%20Robotics&c1=Lerrel%20Pinto&c2=Yuke%20Zhu&c1a=NYU&c2a=UT-Austin)
{: style="text-align: center;"}

#### Poster Session Tuesday, July 11
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 29
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>Robotic manipulation policies that follow natural language instructions are typically trained from corpora of robot-language data that were either collected with specific tasks in mind or expensively relabeled by humans with varied language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? For example, if the original annotations contained simple task descriptions such as "pick up the apple", a pretrained VLM-based labeler could significantly expand the number of semantic concepts available in the data and introduce spatial concepts such as "the apple on the right side of the table" or alternative phrasings such as "the red colored fruit". To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabeled demonstration data and then train language-conditioned policies on the augmented datasets. This method enables cheaper acquisition of useful language descriptions compared to expensive human labels, allowing for more efficient label coverage of large-scale datasets. We apply DIAL to a challenging real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations do not contain crowd-sourced language annotations. Through a large-scale study of over 1,300 real world evaluations, we find that DIAL enables imitation learning policies to acquire new capabilities and generalize to 60 novel instructions unseen in the original dataset.
{: style="color:gray; font-size: 120%; text-align: justified;"}


<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/028/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/030/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
