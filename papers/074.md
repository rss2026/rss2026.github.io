---
layout: paper
title: "CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Nur Muhammad 	❨Mahi❩ Shafiullah</div>
    <div class="paper-author-uni">New York University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Chris Paxton</div>
    <div class="paper-author-uni">Meta AI</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Lerrel Pinto</div>
    <div class="paper-author-uni">New York University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Soumith Chintala</div>
    <div class="paper-author-uni">Meta</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Arthur Szlam</div>
    <div class="paper-author-uni">Meta</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p074.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 74
{: style="margin-top: 10px; text-align: center;"}

### [Session 10. Robot Perception]({{ site.baseurl }}/program/papersession?session=10.%20Robot%20Perception&c1=Christoffer%20Heckman&c2=David%20Rosen&c1a=University%20of%20Colorado%20Boulder&c2a=Northeastern%20University)
{: style="text-align: center;"}

#### Poster Session Thursday, July 13
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 10
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>We propose CLIP-Fields, an implicit scene model that can be used for a variety of tasks, such as segmentation, instance identification, semantic search over space, and view localization. CLIP-Fields learns a mapping from spatial locations to semantic embedding vectors. Importantly, we show that this mapping can be trained with supervision coming only from web-image and web-text trained models such as CLIP, Detic, and Sentence-BERT; and thus uses no direct human supervision. When compared to baselines like Mask-RCNN, our method outperforms on few-shot instance identification or semantic segmentation on the HM3D dataset with only a fraction of the examples. Finally, we show that using CLIP-Fields as a scene memory, robots can perform semantic navigation in real-world environments. Our code and demonstration videos are available here: https://clip-fields.github.io
{: style="color:gray; font-size: 120%; text-align: justified;"}


<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/073/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/075/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
