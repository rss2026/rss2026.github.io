---
layout: paper
title: "Autonomous Justification for Enabling Explainable Decision Support in Human-Robot Teaming"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Matthew Luebbers</div>
    <div class="paper-author-uni">University of Colorado Boulder</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Aaquib Tabrez</div>
    <div class="paper-author-uni">University of Colorado Boulder</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Kyler Ruvane</div>
    <div class="paper-author-uni">University of Colorado</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Bradley Hayes</div>
    <div class="paper-author-uni">University of Colorado Boulder</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p002.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 2
{: style="margin-top: 10px; text-align: center;"}

### [Session 1. Human-Centered Robotics]({{ site.baseurl }}/program/papersession?session=1.%20Human-Centered%20Robotics&c1=Dorsa%20Sadigh&c2=Tapomayukh%20Bhattacharjee&c1a=Stanford%20University&c2a=Cornell%20University)
{: style="text-align: center;"}

#### Poster Session Tuesday, July 11
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 2
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>Justification is an important facet of policy explanation, a process for describing the behavior of an autonomous system. In human-robot collaboration, an autonomous agent can attempt to justify distinctly important decisions by offering explanations as to why those decisions are right or reasonable, leveraging a snapshot of its internal reasoning to do so. Without sufficient insight into a robot's decision-making process, it becomes challenging for users to trust or comply with those important decisions, especially when they are viewed as confusing or contrary to the user's expectations (e.g., when decisions change as new information is introduced to the agent's decision-making process). In this work we characterize the benefits of justification within the context of decision-support during human-robot teaming (i.e., agents giving recommendations to human teammates). We introduce a formal framework using value of information theory to strategically time justifications during periods of misaligned expectations for greater effect. We also characterize four different types of counterfactual justification derived from established explainable AI literature and evaluate them against each other in a human-subjects study involving a collaborative, partially observable search task. Based on our findings, we present takeaways on the effective use of different types of justifications in human-robot teaming scenarios, to improve user compliance and decision-making by strategically influencing human teammate thinking patterns. Finally, we present an augmented reality system incorporating these findings into a real-world decision-support system for human-robot teaming.
{: style="color:gray; font-size: 120%; text-align: justified;"}


<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/001/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/003/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
