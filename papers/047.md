---
layout: paper
title: "Dynamic-Resolution Model Learning for Object Pile Manipulation"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Yixuan Wang</div>
    <div class="paper-author-uni">University of Illinois at Urbana-Champaign</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Yunzhu Li</div>
    <div class="paper-author-uni">Stanford University & University of Illinois at Urbana-Champaign</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Katherine Driggs-Campbell</div>
    <div class="paper-author-uni">University of Illinois at Urbana-Champaign</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Li Fei-Fei</div>
    <div class="paper-author-uni">Stanford University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Jiajun Wu</div>
    <div class="paper-author-uni">Stanford University</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p047.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 47
{: style="margin-top: 10px; text-align: center;"}

### [Session 6. Grasping and Manipulation]({{ site.baseurl }}/program/papersession?session=6.%20Grasping%20and%20Manipulation&c1=Abhishek%20Gupta&c2=Juxi%20Leitner&c1a=University%20of%20Washington&c2a=Monash%20University)
{: style="text-align: center;"}

#### Poster Session Wednesday, July 12
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 15
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>Dynamics models learned from visual observations have shown to be effective in various robotic manipulation tasks. One of the key questions for learning such dynamics models is what scene representation to use. Prior works typically assume representation at a fixed dimension or resolution, which may be inefficient for simple tasks and ineffective for more complicated tasks. In this work, we investigate how to learn dynamic and adaptive representations at different levels of abstraction to achieve the optimal trade-off between efficiency and effectiveness. Specifically, we construct dynamic-resolution particle representations of the environment and learn a unified dynamics model using graph neural networks (GNNs) that allows continuous selection of the abstraction level. During test time, the agent can adaptively determine the optimal resolution at each model-predictive control (MPC) step. We evaluate our method in object pile manipulation, a task we commonly encounter in cooking, agriculture, manufacturing, and pharmaceutical applications. Through comprehensive evaluations both in the simulation and the real world, we show that our method achieves significantly better performance than state-of-the-art fixed-resolution baselines at the gathering, sorting, and redistribution of granular object piles made with various instances like coffee beans, almonds, corn, etc.
{: style="color:gray; font-size: 120%; text-align: justified;"}


### Links
- [Supplementary materials](http://www.roboticsproceedings.org/rss19/p047_sup.zip)

<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/046/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/048/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
