---
layout: paper
title: "Sampling-based Exploration for Reinforcement Learning of Dexterous Manipulation"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Gagan Khandate</div>
    <div class="paper-author-uni">Columbia University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Siqi Shang</div>
    <div class="paper-author-uni">Columbia University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Eric T Chang</div>
    <div class="paper-author-uni">Columbia University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Tristan L Saidi</div>
    <div class="paper-author-uni">Columbia University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Johnson Adams</div>
    <div class="paper-author-uni">Columbia University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Matei Ciocarlie</div>
    <div class="paper-author-uni">Columbia University</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p020.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 20
{: style="margin-top: 10px; text-align: center;"}

### [Session 3. Self-supervision and RL for Manipulation]({{ site.baseurl }}/program/papersession?session=3.%20Self-supervision%20and%20RL%20for%20Manipulation&c1=Joseph%20Lim&c2=Jens%20Kober&c1a=KAIST&c2a=TU%20Delft)
{: style="text-align: center;"}

#### Poster Session Tuesday, July 11
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 20
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>In this paper, we present a novel method for achieving dexterous manipulation of complex objects, while simultaneously securing the object without the use of passive support surfaces. We posit that a key difficulty for training such policies in a Reinforcement Learning framework is the difficulty of exploring the problem state space, as the accessible regions of this space form a complex structure along manifolds of a high-dimensional space. To address this challenge, we use two versions of the non-holonomic Rapidly-Exploring Random Trees algorithm; one version is more general, but requires explicit use of the environmentâ€™s transition function, while the second version uses manipulation-specific kinematic constraints to attain better sample efficiency. In both cases, we use states found via sampling-based exploration to generate reset distributions that enable training control policies under full dynamic constraints via model-free Reinforcement Learning. We show that these policies are effective at manipulation problems of higher difficulty than previously shown, and also transfer effectively to real robots.
{: style="color:gray; font-size: 120%; text-align: justified;"}


<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/019/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/021/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
