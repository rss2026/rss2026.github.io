---
layout: paper
title: "Scaling Robot Learning with Semantically Imagined Experience"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Tianhe Yu</div>
    <div class="paper-author-uni">Google Brain</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Ted Xiao</div>
    <div class="paper-author-uni">Google Inc</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Jonathan Tompson</div>
    <div class="paper-author-uni">Google Inc</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Austin Stone</div>
    <div class="paper-author-uni">Google Inc</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Su Wang</div>
    <div class="paper-author-uni">Google Inc</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Anthony Brohan</div>
    <div class="paper-author-uni">Google Research</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Jaspiar Singh</div>
    <div class="paper-author-uni">Google Inc</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Clayton Tan</div>
    <div class="paper-author-uni">Google Inc</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Dee M</div>
    <div class="paper-author-uni">Google Inc</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Jodilyn Peralta</div>
    <div class="paper-author-uni">Google Inc</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Karol Hausman</div>
    <div class="paper-author-uni">Google Brain</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Brian Ichter</div>
    <div class="paper-author-uni">Google Brain</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Fei Xia</div>
    <div class="paper-author-uni">Google Inc</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p027.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 27
{: style="margin-top: 10px; text-align: center;"}

### [Session 4. Large Data and Vision-Language Models for Robotics]({{ site.baseurl }}/program/papersession?session=4.%20Large%20Data%20and%20Vision-Language%20Models%20for%20Robotics&c1=Lerrel%20Pinto&c2=Yuke%20Zhu&c1a=NYU&c2a=UT-Austin)
{: style="text-align: center;"}

#### Poster Session Tuesday, July 11
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 27
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>Recent advances in robot learning have shown promise in enabling robots to perform a variety of manipulation tasks and generalize to novel scenarios. 
One of the key contributing factors to this progress is the scale of robot data used to train the models. To obtain large-scale datasets, prior approaches have relied on either demonstrations requiring high human involvement or engineering-heavy autonomous data collection schemes, both of which being challenging in scaling up the space of new tasks and skills needed for building generalist robots. 
To mitigate this issue, we propose to take an alternative route and leverage text-to-image foundation models widely used in computer vision and natural language processing to obtain meaningful data for robot learning without requiring additional robot data. Specifically, we make use of the state of the art text-to-image diffusion models and perform aggressive data augmentation on top of our existing robotic manipulation datasets via inpainting of various unseen objects for manipulation, backgrounds, and distractors with pure text guidance. Through extensive real-world experiments, we show that manipulation policies trained on the augmented data are able to solve completely unseen tasks with new objects and can behave more robustly w.r.t. novel distractors. In addition, we also find that we can improve the robustness and generalization of high-level robot learning tasks such as success detection through training with the diffusion-based data augmentation.
{: style="color:gray; font-size: 120%; text-align: justified;"}


<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/026/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/028/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
