---
layout: paper
title: "Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Siddhant Haldar</div>
    <div class="paper-author-uni">New York University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Jyothish Pari</div>
    <div class="paper-author-uni">New York University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Anant Rai</div>
    <div class="paper-author-uni">New York University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Lerrel Pinto</div>
    <div class="paper-author-uni">New York University</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p009.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 9
{: style="margin-top: 10px; text-align: center;"}

### Nominated for Best Student Paper
{: style="margin-top: 10px; font-weight: bold; color: #555555; text-align: center;"}

### [Session 2. Manipulation from Demonstrations and Teleoperation]({{ site.baseurl }}/program/papersession?session=2.%20Manipulation%20from%20Demonstrations%20and%20Teleoperation&c1=Florian%20Shkurti&c2=Dongheui%20Lee&c1a=University%20of%20Toronto&c2a=TU%20Wien)
{: style="text-align: center;"}

#### Poster Session Tuesday, July 11
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 9
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>While imitation learning provides us with an efficient toolkit to train robots, learning skills that are robust to environment variations remains a significant challenge. Current approaches address this challenge by relying either on large amounts of demonstrations that span environment variations or on handcrafted reward functions that require state estimates. Both directions are not scalable to fast imitation. In this work, we present Fast Imitation of Skills from Humans (FISH), a new imitation learning approach that can learn robust visual skills with less than a minute of human demonstrations. Given a weak base-policy trained by offline imitation of demonstrations, FISH computes rewards that correspond to the “match” between the robot’s behavior and the demonstrations. These rewards are then used to adaptively update a residual policy that adds on to the base-policy. Across all tasks, FISH requires at most twenty minutes of interactive learning to imitate demonstrations on object configurations that were not seen in the demonstrations. Importantly, FISH is constructed to be versatile, which allows it to be used across robot morphologies (e.g. xArm, Allegro, Stretch) and camera configurations (e.g. third-person, eye-in-hand). Our experimental evaluations on 9 different tasks show that FISH achieves an average success rate of 93%, which is around 3.8× higher than prior state-of-the-art methods.
{: style="color:gray; font-size: 120%; text-align: justified;"}


<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/008/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/010/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
