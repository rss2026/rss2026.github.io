---
layout: paper
title: "Self-Supervised Visuo-Tactile Pretraining to Locate and Follow Garment Features"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Justin Kerr</div>
    <div class="paper-author-uni">University of California, Berkeley</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Huang Huang</div>
    <div class="paper-author-uni">University of California, Berkeley</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Albert Wilcox</div>
    <div class="paper-author-uni">University of California, Berkeley</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Ryan I Hoque</div>
    <div class="paper-author-uni">University of California, Berkeley</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Jeffrey Ichnowski</div>
    <div class="paper-author-uni">Carnegie Mellon University</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Roberto Calandra</div>
    <div class="paper-author-uni">Technical University Dresden</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Ken Goldberg</div>
    <div class="paper-author-uni">University of California, Berkeley</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p018.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 18
{: style="margin-top: 10px; text-align: center;"}

### [Session 3. Self-supervision and RL for Manipulation]({{ site.baseurl }}/program/papersession?session=3.%20Self-supervision%20and%20RL%20for%20Manipulation&c1=Joseph%20Lim&c2=Jens%20Kober&c1a=KAIST&c2a=TU%20Delft)
{: style="text-align: center;"}

#### Poster Session Tuesday, July 11
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 18
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>Humans make extensive use of vision and touch as complementary senses, with vision providing global information about the scene and touch measuring local information during manipulation without suffering from occlusions. While prior work demonstrates the efficacy of tactile sensing for precise manipulation of deformables, they typically rely on supervised, human-labeled datasets. We propose Self-Supervised Visuo-Tactile Pretraining (SSVTP), a framework for learning multi-task visuo-tactile representations in a self-supervised manner through cross-modal supervision. We design a mechanism that enables a robot to autonomously collect precisely spatially-aligned visual and tactile image pairs, then train visual and tactile encoders to embed these pairs into a shared latent space using cross- modal contrastive loss. We apply this latent space to downstream perception and control of deformable garments on flat surfaces, and evaluate the flexibility of the learned representations without fine-tuning on 5 tasks: feature classification, contact localization, anomaly detection, feature search from a visual query (e.g., garment feature localization under occlusion), and edge following along cloth edges. The pretrained representations achieve a 73-100% success rate on these 5 tasks.
{: style="color:gray; font-size: 120%; text-align: justified;"}


<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/017/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/019/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
