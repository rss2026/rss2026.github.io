---
layout: paper
title: "How To Not Train Your Dragon: Training-free Embodied Object Goal Navigation with Semantic Frontiers"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Junting Chen</div>
    <div class="paper-author-uni">ETH Zürich</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Guohao Li</div>
    <div class="paper-author-uni">KAUST</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Suryansh Kumar</div>
    <div class="paper-author-uni">ETH Zürich</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Bernard Ghanem</div>
    <div class="paper-author-uni">KAUST</div>
</div>
<div class="paper-author-box">
    <div class="paper-author-name">Fisher Yu</div>
    <div class="paper-author-uni">ETH Zürich</div>
</div>

</div><div class="paper-pdf">
<div> <a href="http://www.roboticsproceedings.org/rss19/p075.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
</div>

### Paper ID 75
{: style="margin-top: 10px; text-align: center;"}

### [Session 10. Robot Perception]({{ site.baseurl }}/program/papersession?session=10.%20Robot%20Perception&c1=Christoffer%20Heckman&c2=David%20Rosen&c1a=University%20of%20Colorado%20Boulder&c2a=Northeastern%20University)
{: style="text-align: center;"}

#### Poster Session Thursday, July 13
{: style="margin-top: 10px; color: #555555; text-align: center;"}

#### Poster 11
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>Object goal navigation is an important problem in Embodied AI that involves guiding the agent to navigate to an instance of the object category in an unknown environment---typically an indoor scene. Unfortunately, current state-of-the-art methods for this problem rely heavily on data-driven approaches, e.g., end-to-end reinforcement learning, imitation learning, and others. Moreover, such methods are typically costly to train and difficult to debug, leading to a lack of transferability and explainability. Inspired by recent successes in combining classical and learning methods, we present a modular and training-free solution, which embraces more classic approaches, to tackle the object goal navigation problem. Our method builds a structured scene representation based on the classic visual simultaneous localization and mapping (V-SLAM) framework. We then inject semantics into geometric-based frontier exploration to reason about promising areas to search for a goal object. Our structured scene representation comprises a 2D occupancy map, semantic point cloud, and spatial scene graph. Our method propagates semantics on the scene graphs based on language priors and scene statistics to introduce semantic knowledge to the geometric frontiers. With injected semantic priors, the agent can reason about the most promising frontier to explore.  The proposed pipeline shows strong experimental performance for object goal navigation on the Gibson benchmark dataset, outperforming the previous state-of-the-art. We also perform comprehensive ablation studies to identify the current bottleneck in the object navigation task.
{: style="color:gray; font-size: 120%; text-align: justified;"}


### Links
- [Supplementary materials](http://www.roboticsproceedings.org/rss19/p075_sup.zip)

<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/074/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<a href="{{ site.baseurl }}/program/papers/076/"> <img src="{{ site.baseurl }}/images/next_paper_icon.png" alt="Next Paper" title="Next Paper"/> </a>

</div>
